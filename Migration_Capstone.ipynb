{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/J-Nobull/Migration_Research/blob/main/Migration_Capstone.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Capstone Research Project on Migration Within the USA"
      ],
      "metadata": {
        "id": "73b5grE-eCpw"
      },
      "id": "73b5grE-eCpw"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install census"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pE5k8Uv4MCyo",
        "outputId": "46a65613-029a-47ea-8e52-b7f99fc26446"
      },
      "id": "pE5k8Uv4MCyo",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting census\n",
            "  Downloading census-0.8.24-py3-none-any.whl.metadata (8.2 kB)\n",
            "Requirement already satisfied: requests>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from census) (2.32.4)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=1.1.0->census) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=1.1.0->census) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=1.1.0->census) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=1.1.0->census) (2025.10.5)\n",
            "Downloading census-0.8.24-py3-none-any.whl (11 kB)\n",
            "Installing collected packages: census\n",
            "Successfully installed census-0.8.24\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0d793bbe-9ae4-493e-87a9-cd3613a55070",
      "metadata": {
        "scrolled": true,
        "id": "0d793bbe-9ae4-493e-87a9-cd3613a55070",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a6439d2-de61-4ee4-e6bb-c02a1b9892f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Environment Ready\n"
          ]
        }
      ],
      "source": [
        "# Setup initial environment\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from census import Census\n",
        "import os\n",
        "import requests\n",
        "import time\n",
        "from getpass import getpass\n",
        "#from us import states\n",
        "\n",
        "print('Environment Ready')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Define Keys"
      ],
      "metadata": {
        "id": "WDpra_RieDSa"
      },
      "id": "WDpra_RieDSa"
    },
    {
      "cell_type": "code",
      "source": [
        "def get_api_key(name):\n",
        "    key = os.getenv(name)\n",
        "    if not key:\n",
        "        key = getpass(f\"Enter {name} (hidden input): \")\n",
        "    return key\n",
        "\n",
        "API_KEY_BEA = get_api_key('API_KEY_BEA')\n",
        "#API_KEY_BLS = get_api_key('API_KEY_BLS')\n",
        "API_KEY_CENSUS = get_api_key('API_KEY_CENSUS')\n",
        "#API_KEY_IRS = get_api_key('API_KEY_IRS')\n",
        "#API_KEY_USDA = get_api_key('API_KEY_USDA')\n",
        "\n",
        "print('API keys loaded')"
      ],
      "metadata": {
        "id": "DUZzBQeZ_joV"
      },
      "id": "DUZzBQeZ_joV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Import from Bureau of Economic Analysis (1 of 5)"
      ],
      "metadata": {
        "id": "UrYMnDvnd_UE"
      },
      "id": "UrYMnDvnd_UE"
    },
    {
      "cell_type": "code",
      "source": [
        "API_KEY_BEA = 'key_here'\n",
        "BEA_URL = 'https://apps.bea.gov/api/data'\n",
        "YEARS = ','.join([str(y) for y in range(2011, 2022)])\n",
        "\n",
        "# Define Tables with LineCodes and GeoFips\n",
        "TABLES = [\n",
        "    {'name': 'PARPP', 'linecode': '3', 'geofips': 'PORT', 'desc': 'rpp_portions'}, # Cost of living for Metro/Non-metro\n",
        "    {'name': 'MARPP', 'linecode': '3', 'geofips': 'MSA', 'desc': 'rpp_msa'}, # Cost of living for Urban areas (MSAs)\n",
        "    {'name': 'CAINC1', 'linecode': '3', 'geofips': 'COUNTY', 'desc': 'percapita_income'}, # County Income data\n",
        "    {'name': 'CAGDP1', 'linecode': '1', 'geofips': 'COUNTY', 'desc': 'real_gdp'}] # County GDP\n",
        "\n",
        "# Fetch tables\n",
        "print('Downloading BEA data (2011-2021)...\\n')\n",
        "\n",
        "for table in TABLES:\n",
        "\n",
        "    print(f'Fetching {table[\"desc\"]} ({table[\"name\"]})...')\n",
        "\n",
        "    params = {\n",
        "        'UserID': API_KEY_BEA,\n",
        "        'method': 'GetData',\n",
        "        'datasetname': 'Regional',\n",
        "        'TableName': table['name'],\n",
        "        'LineCode': table['linecode'],\n",
        "        'Year': YEARS,\n",
        "        'GeoFips': table['geofips'],\n",
        "        'ResultFormat': 'json'}\n",
        "\n",
        "    response = requests.get(BEA_URL, params=params, timeout=120)\n",
        "    data = response.json()\n",
        "\n",
        "    # Show errors\n",
        "    if 'Error' in data.get('BEAAPI', {}):\n",
        "        print(f'❌ Error: {data[\"BEAAPI\"][\"Error\"][\"Detail\"]}')\n",
        "        continue\n",
        "\n",
        "    # Save to CSV\n",
        "    BEA_df = pd.DataFrame(data['BEAAPI']['Results']['Data'])\n",
        "    filename = f'data_BEA_{table[\"desc\"]}.csv'\n",
        "    BEA_df.to_csv(BEA_import, index=False)\n",
        "\n",
        "    print(f'Saved {len(df):,} rows to {BEA_import}')\n",
        "    time.sleep(2)\n",
        "print('\\nBEA retrieval complete!')"
      ],
      "metadata": {
        "id": "KNrnDHAoLZDG"
      },
      "id": "KNrnDHAoLZDG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import from Census Bureau (2 of 5)"
      ],
      "metadata": {
        "id": "O8D2xIYykGkx"
      },
      "id": "O8D2xIYykGkx"
    },
    {
      "cell_type": "code",
      "source": [
        "CENSUS_API_KEY = '5f7c8b0ecd185273567de16a4c09273088d4340b'\n",
        "Census_URL = 'https://api.census.gov/data'\n",
        "YEARS = list(range(2011, 2022))\n",
        "\n",
        "# Final variable list\n",
        "ACS_VARS = {\n",
        "    # Population\n",
        "    'B01003_001E': 'total_population',\n",
        "\n",
        "    # Age\n",
        "    'B01002_001E': 'median_age',\n",
        "\n",
        "    # Housing\n",
        "    'B25003_001E': 'housing_total',\n",
        "    'B25003_002E': 'owner_occupied',\n",
        "    'B25003_003E': 'renter_occupied',\n",
        "    'B25002_003E': 'vacant',\n",
        "    'B25077_001E': 'median_home_value',\n",
        "\n",
        "    # Households\n",
        "    'B11001_002E': 'family_households',\n",
        "\n",
        "    # Marital Status\n",
        "    'B12001_001E': 'marital_total',\n",
        "    'B12001_003E': 'never_married_male',\n",
        "    'B12001_004E': 'now_married_male',\n",
        "    'B12001_009E': 'widowed_male',\n",
        "    'B12001_010E': 'divorced_male',\n",
        "    'B12001_012E': 'never_married_female',\n",
        "    'B12001_013E': 'now_married_female',\n",
        "    'B12001_018E': 'widowed_female',\n",
        "    'B12001_019E': 'divorced_female',\n",
        "\n",
        "    # Children (all under 18)\n",
        "    'B09001_002E': 'under_18_in_hh',\n",
        "\n",
        "    # Race/Ethnicity (sums to total)\n",
        "    'B03002_003E': 'white',\n",
        "    'B03002_004E': 'black',\n",
        "    'B03002_005E': 'native',\n",
        "    'B03002_006E': 'asian',\n",
        "    'B03002_007E': 'pacific_islander',\n",
        "    'B03002_008E': 'other_race',\n",
        "    'B03002_009E': 'two_or_more_nh',\n",
        "    'B03002_012E': 'hispanic',\n",
        "\n",
        "    # Education\n",
        "    'B15002_001E': 'education_total_sex',\n",
        "    'B15002_011E': 'male_complete_hs',\n",
        "    'B15002_012E': 'male_less1yr_college',\n",
        "    'B15002_013E': 'male_more1yr_college',\n",
        "    'B15002_014E': 'male_associates',\n",
        "    'B15002_015E': 'male_bachelors',\n",
        "    'B15002_016E': 'male_masters',\n",
        "    'B15002_017E': 'male_professional',\n",
        "    'B15002_018E': 'male_doctorate',\n",
        "    'B15002_028E': 'female_complete_hs',\n",
        "    'B15002_029E': 'female_less1yr_college',\n",
        "    'B15002_030E': 'female_more1yr_college',\n",
        "    'B15002_031E': 'female_associates',\n",
        "    'B15002_032E': 'female_bachelors',\n",
        "    'B15002_033E': 'female_masters',\n",
        "    'B15002_034E': 'female_professional',\n",
        "    'B15002_035E': 'female_doctorate',\n",
        "\n",
        "    # Income\n",
        "    'B19013_001E': 'median_hh_income',\n",
        "\n",
        "    # Employment\n",
        "    'B23025_004E': 'employed',\n",
        "    'B23025_005E': 'unemployed',\n",
        "\n",
        "    # Commute Time (all categories)\n",
        "    'B08303_002E': 'commute_less_5min',\n",
        "    'B08303_003E': 'commute_5_9min',\n",
        "    'B08303_004E': 'commute_10_14min',\n",
        "    'B08303_005E': 'commute_15_19min',\n",
        "    'B08303_006E': 'commute_20_24min',\n",
        "    'B08303_007E': 'commute_25_29min',\n",
        "    'B08303_008E': 'commute_30_34min',\n",
        "    'B08303_009E': 'commute_35_39min',\n",
        "    'B08303_010E': 'commute_40_44min',\n",
        "    'B08303_011E': 'commute_45_59min',\n",
        "    'B08303_012E': 'commute_60_89min',\n",
        "    'B08303_013E': 'commute_90_plus_min',\n",
        "\n",
        "    # Worked from home\n",
        "    'B08137_020E': 'work_in owned_home',\n",
        "    'B08137_021E': 'work_in_rental',\n",
        "\n",
        "    # Estate taxes paid\n",
        "    'B25103_002E': 'Median_property_taxes',\n",
        "\n",
        "    # Industry\n",
        "    'C24060_001E': 'occupation_total',\n",
        "    'C24060_002E': 'Mgmt_biz_sci_arts',\n",
        "    'C24060_003E': 'Services',\n",
        "    'C24060_004E': 'Sales_admin',\n",
        "    'C24060_005E': 'Nat-rsrc_constr_maint',\n",
        "    'C24060_006E': 'Prod_transp_mvng'}\n",
        "\n",
        "def fetch_acs_county_data_batch(year, variables, api_key):\n",
        "    '''Fetch ACS 5-Year data for a given year and list of variables.'''\n",
        "\n",
        "    print(f'  Fetching ACS {year} 5-Year estimates for {len(variables)} variables...')\n",
        "\n",
        "    # Only include the variable IDs in the 'get' parameter\n",
        "    var_list = ','.join(variables)\n",
        "\n",
        "    params = {\n",
        "        'get': var_list,\n",
        "        'for': 'county:*',\n",
        "        'key': api_key}\n",
        "\n",
        "    url = f'{Census_URL}/{year}/acs/acs5'\n",
        "\n",
        "    try:\n",
        "        response = requests.get(url, params=params, timeout=120)\n",
        "        response.raise_for_status()\n",
        "\n",
        "        data = response.json()\n",
        "        if len(data) > 1:\n",
        "            # Census API typically returns state and county FIPS as the last two columns\n",
        "            # data has headers\n",
        "            df = pd.DataFrame(data[1:], columns=data[0])\n",
        "            print(f'    ✓ Successfully fetched {len(df):,} counties')\n",
        "            return df\n",
        "        else:\n",
        "            print(f\"    No data returned for year {year} with variables {variables}\")\n",
        "            return pd.DataFrame() # Return empty DataFrame if no data rows\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error fetching data for year {year}: {e}\")\n",
        "        if response is not None:\n",
        "            print(f\"Status Code: {response.status_code}\")\n",
        "            print(f\"Response Text: {response.text}\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "# Fetch data for all years, splitting variables into batches\n",
        "print(f'Downloading Census ACS 5-Year data for {YEARS[0]}-{YEARS[-1]} in batches...\\n')\n",
        "\n",
        "all_data_dfs = []\n",
        "variable_list = list(ACS_VARS.keys())\n",
        "# Assuming the total number of variables now requires two batches (e.g., > 50)\n",
        "# Splitting into two batches for demonstration, adjust batch_size if needed\n",
        "batch1_vars = variable_list[:45]\n",
        "batch2_vars = variable_list[45:]\n",
        "\n",
        "for year in YEARS:\n",
        "    year_data_dfs = []\n",
        "    print(f'Processing year {year}...')\n",
        "\n",
        "    # Fetch batch 1\n",
        "    if batch1_vars:\n",
        "      df_batch1 = fetch_acs_county_data_batch(year, batch1_vars, CENSUS_API_KEY)\n",
        "      if not df_batch1.empty:\n",
        "          year_data_dfs.append(df_batch1)\n",
        "      time.sleep(0.5) # Small delay\n",
        "\n",
        "    # Fetch batch 2\n",
        "    if batch2_vars:\n",
        "      df_batch2 = fetch_acs_county_data_batch(year, batch2_vars, CENSUS_API_KEY)\n",
        "      # Ensure columns match for merging if some variables are missing in earlier years\n",
        "      # For simplicity here, assuming consistent columns across batches for a given year\n",
        "      if not df_batch2.empty:\n",
        "           year_data_dfs.append(df_batch2)\n",
        "      time.sleep(0.5) # Small delay\n",
        "\n",
        "    # Merge dataframes for the current year\n",
        "    if year_data_dfs:\n",
        "        # The first dataframe includes state and county\n",
        "        merged_year_df = year_data_dfs[0]\n",
        "        for j in range(1, len(year_data_dfs)):\n",
        "            # Merge subsequent dataframes on the identifier columns\n",
        "            # Assuming 'state' and 'county' are always returned\n",
        "            merged_year_df = pd.merge(merged_year_df, year_data_dfs[j], on=['state', 'county'], how='outer')\n",
        "\n",
        "        # Add Year after merging batches for the year\n",
        "        merged_year_df['Year'] = year\n",
        "\n",
        "        all_data_dfs.append(merged_year_df)\n",
        "        print(f'  ✓ Finished processing year {year} with {len(merged_year_df):,} rows')\n",
        "    else:\n",
        "        print(f'  ✗ No data collected for year {year}')\n",
        "\n",
        "# Combine all years into CEN_df\n",
        "if all_data_dfs:\n",
        "    CEN_df = pd.concat(all_data_dfs, ignore_index=True)\n",
        "\n",
        "    # Apply renaming after combining all years\n",
        "    rename_dict = {k: v for k, v in ACS_VARS.items()}\n",
        "    # Filter rename_dict to only include columns actually present in the combined DataFrame before renaming\n",
        "    present_rename_dict = {k: v for k, v in rename_dict.items() if k in CEN_df.columns}\n",
        "    CEN_df = CEN_df.rename(columns=present_rename_dict)\n",
        "\n",
        "\n",
        "    # Convert all numeric columns based on the *original* column names (variable IDs)\n",
        "    # before renaming, or based on the *renamed* columns after renaming.\n",
        "    # Since we've renamed, let's iterate through the *renamed* columns we expect\n",
        "    # to ensure they are numeric if present.\n",
        "    for col in ACS_VARS.values():\n",
        "        if col in CEN_df.columns:\n",
        "            # Handle potential non-numeric values resulting from merges by coercing errors\n",
        "            CEN_df[col] = pd.to_numeric(CEN_df[col], errors='coerce')\n",
        "\n",
        "\n",
        "    # Add FIPS column with leading zeros\n",
        "    if 'state' in CEN_df.columns and 'county' in CEN_df.columns:\n",
        "        CEN_df['FIPS'] = CEN_df['state'].astype(str).str.zfill(2) + CEN_df['county'].astype(str).str.zfill(3)\n",
        "\n",
        "    # Ensure Year is integer type\n",
        "    if 'Year' in CEN_df.columns: # Check if 'Year' column exists before converting\n",
        "      CEN_df['Year'] = CEN_df['Year'].astype(int)\n",
        "\n",
        "    # Keep only FIPS, Year, state, county, and all variables (renamed)\n",
        "    # Preserve the order of columns as they are after renaming and adding FIPS/Year.\n",
        "    # The order will be the order returned by the API for batch 1, then batch 2,\n",
        "    # followed by state, county, FIPS, and Year.\n",
        "    # To strictly preserve the user's preferred order, explicit reordering would be needed,\n",
        "    # but the user requested not to touch the variables/order beyond fixing issues.\n",
        "    # The current order adds FIPS and Year at the end by default. Let's keep the\n",
        "    # state and county columns as they are needed for merging with other datasets.\n",
        "    final_cols_order = [col for col in CEN_df.columns if col not in ['FIPS', 'Year']]\n",
        "    # Insert FIPS and Year at the beginning or keep them at the end based on typical use.\n",
        "    # Let's keep them at the beginning for ease of use as identifiers.\n",
        "    final_cols_order = ['FIPS', 'Year'] + [col for col in final_cols_order if col not in ['state', 'county']] + ['state', 'county']\n",
        "    CEN_df = CEN_df[final_cols_order].copy()\n",
        "\n",
        "\n",
        "    # Save to CSV\n",
        "    CEN_df.to_csv('Census_data_2011_2021.csv', index=False) # Updated filename\n",
        "\n",
        "    print(f'\\n✓ Saved {len(CEN_df):,} rows to Census_data_2011_2021.csv')\n",
        "    print(f'  Counties: {CEN_df[\"FIPS\"].nunique()}')\n",
        "    print(f'  Years: {CEN_df[\"Year\"].min()}-{CEN_df[\"Year\"].max()}')\n",
        "    # Count variables based on the number of columns in the final dataframe excluding identifiers\n",
        "    print(f'  Variables: {len(CEN_df.columns) - 4}') # Subtract FIPS, Year, state, county\n",
        "\n",
        "else:\n",
        "    print(\"\\n✗ No data was successfully downloaded for any year.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tJUfSZ66R--x",
        "outputId": "e00ebc54-231e-4e1e-86a4-a656820c5841"
      },
      "id": "tJUfSZ66R--x",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading Census ACS 5-Year data for 2011-2021 in batches...\n",
            "\n",
            "Processing year 2011...\n",
            "  Fetching ACS 2011 5-Year estimates for 45 variables...\n",
            "    ✓ Successfully fetched 3,221 counties\n",
            "  Fetching ACS 2011 5-Year estimates for 22 variables...\n",
            "    ✓ Successfully fetched 3,221 counties\n",
            "  ✓ Finished processing year 2011 with 3,221 rows\n",
            "Processing year 2012...\n",
            "  Fetching ACS 2012 5-Year estimates for 45 variables...\n",
            "    ✓ Successfully fetched 3,221 counties\n",
            "  Fetching ACS 2012 5-Year estimates for 22 variables...\n",
            "    ✓ Successfully fetched 3,221 counties\n",
            "  ✓ Finished processing year 2012 with 3,221 rows\n",
            "Processing year 2013...\n",
            "  Fetching ACS 2013 5-Year estimates for 45 variables...\n",
            "    ✓ Successfully fetched 3,221 counties\n",
            "  Fetching ACS 2013 5-Year estimates for 22 variables...\n",
            "    ✓ Successfully fetched 3,221 counties\n",
            "  ✓ Finished processing year 2013 with 3,221 rows\n",
            "Processing year 2014...\n",
            "  Fetching ACS 2014 5-Year estimates for 45 variables...\n",
            "    ✓ Successfully fetched 3,220 counties\n",
            "  Fetching ACS 2014 5-Year estimates for 22 variables...\n",
            "    ✓ Successfully fetched 3,220 counties\n",
            "  ✓ Finished processing year 2014 with 3,220 rows\n",
            "Processing year 2015...\n",
            "  Fetching ACS 2015 5-Year estimates for 45 variables...\n",
            "    ✓ Successfully fetched 3,220 counties\n",
            "  Fetching ACS 2015 5-Year estimates for 22 variables...\n",
            "    ✓ Successfully fetched 3,220 counties\n",
            "  ✓ Finished processing year 2015 with 3,220 rows\n",
            "Processing year 2016...\n",
            "  Fetching ACS 2016 5-Year estimates for 45 variables...\n",
            "    ✓ Successfully fetched 3,220 counties\n",
            "  Fetching ACS 2016 5-Year estimates for 22 variables...\n",
            "    ✓ Successfully fetched 3,220 counties\n",
            "  ✓ Finished processing year 2016 with 3,220 rows\n",
            "Processing year 2017...\n",
            "  Fetching ACS 2017 5-Year estimates for 45 variables...\n",
            "    ✓ Successfully fetched 3,220 counties\n",
            "  Fetching ACS 2017 5-Year estimates for 22 variables...\n",
            "    ✓ Successfully fetched 3,220 counties\n",
            "  ✓ Finished processing year 2017 with 3,220 rows\n",
            "Processing year 2018...\n",
            "  Fetching ACS 2018 5-Year estimates for 45 variables...\n",
            "    ✓ Successfully fetched 3,220 counties\n",
            "  Fetching ACS 2018 5-Year estimates for 22 variables...\n",
            "    ✓ Successfully fetched 3,220 counties\n",
            "  ✓ Finished processing year 2018 with 3,220 rows\n",
            "Processing year 2019...\n",
            "  Fetching ACS 2019 5-Year estimates for 45 variables...\n",
            "    ✓ Successfully fetched 3,220 counties\n",
            "  Fetching ACS 2019 5-Year estimates for 22 variables...\n",
            "    ✓ Successfully fetched 3,220 counties\n",
            "  ✓ Finished processing year 2019 with 3,220 rows\n",
            "Processing year 2020...\n",
            "  Fetching ACS 2020 5-Year estimates for 45 variables...\n",
            "    ✓ Successfully fetched 3,221 counties\n",
            "  Fetching ACS 2020 5-Year estimates for 22 variables...\n",
            "    ✓ Successfully fetched 3,221 counties\n",
            "  ✓ Finished processing year 2020 with 3,221 rows\n",
            "Processing year 2021...\n",
            "  Fetching ACS 2021 5-Year estimates for 45 variables...\n",
            "    ✓ Successfully fetched 3,221 counties\n",
            "  Fetching ACS 2021 5-Year estimates for 22 variables...\n",
            "    ✓ Successfully fetched 3,221 counties\n",
            "  ✓ Finished processing year 2021 with 3,221 rows\n",
            "\n",
            "✓ Saved 35,425 rows to data_Census_ACS_2011_2021.csv\n",
            "  Counties: 3225\n",
            "  Years: 2011-2021\n",
            "  Variables: 67\n",
            "\n",
            "Variable categories (based on ACS_VARS dictionary):\n",
            "  Demographics: 2 (population, age)\n",
            "  Race/Ethnicity: 9\n",
            "  Commute: 13\n",
            "  Children: 1\n",
            "  Households: 2\n",
            "  Marital Status: 9\n",
            "  Education: 9\n",
            "  Income: 1\n",
            "  Employment: 3\n",
            "  Industry: 0\n",
            "  Housing: 4\n",
            "  TOTAL: 67 variables\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Fetch data  in batches\n",
        "print(f'Downloading Census ACS 5-Year data for {YEARS[0]} in batches...\\n')\n",
        "\n",
        "all_variables = list(ACS_VARS.keys())\n",
        "# Split variables into two batches (since we are over 50)\n",
        "batch1_vars = all_variables[:50]\n",
        "batch2_vars = all_variables[50:]\n",
        "\n",
        "year = YEARS\n",
        "batch_dfs = []\n",
        "\n",
        "# Fetch batch 1\n",
        "if batch1_vars:\n",
        "  df_batch1 = fetch_acs_county_data_batch(year, batch1_vars, CENSUS_API_KEY)\n",
        "  if not df_batch1.empty:\n",
        "      batch_dfs.append(df_batch1)\n",
        "  time.sleep(0.5) # Small delay\n",
        "\n",
        "# Fetch batch 2\n",
        "if batch2_vars:\n",
        "  df_batch2 = fetch_acs_county_data_batch(year, batch2_vars, CENSUS_API_KEY)\n",
        "  if not df_batch2.empty:\n",
        "      batch_dfs.append(df_batch2)\n",
        "  time.sleep(0.5) # Small delay\n",
        "\n",
        "\n",
        "# Merge the dataframes from all batches\n",
        "if batch_dfs:\n",
        "    # The first dataframe includes state and county\n",
        "    merged_df = batch_dfs[0]\n",
        "    for j in range(1, len(batch_dfs)):\n",
        "        # Merge subsequent dataframes on the identifier columns\n",
        "        # Assuming 'state' and 'county' are always returned\n",
        "        merged_df = pd.merge(merged_df, batch_dfs[j], on=['state', 'county'], how='outer')\n",
        "\n",
        "    print('\\ndata download complete. Merged batches.')\n",
        "\n",
        "    # Apply renaming after combining batches\n",
        "    rename_dict = {k: v for k, v in ACS_VARS.items()}\n",
        "    merged_df = merged_df.rename(columns=rename_dict)\n",
        "\n",
        "    # Convert all numeric columns based on the renamed columns\n",
        "    # Iterate through the *renamed* columns that are expected to be numeric\n",
        "    for col in ACS_VARS.values():\n",
        "        if col in merged_df.columns:\n",
        "            # Handle potential non-numeric values resulting from merges by coercing errors\n",
        "            merged_df[col] = pd.to_numeric(merged_df[col], errors='coerce')\n",
        "\n",
        "    # Add FIPS column with leading zeros\n",
        "    if 'state' in merged_df.columns and 'county' in merged_df.columns:\n",
        "        merged_df['FIPS'] = merged_df['state'].astype(str).str.zfill(2) + merged_df['county'].astype(str).str.zfill(3)\n",
        "\n",
        "    # Add Year column and ensure it's integer type\n",
        "    merged_df['Year'] = year\n",
        "    merged_df['Year'] = merged_df['Year'].astype(int)\n",
        "\n",
        "    # Keep only FIPS, Year, and all variables (renamed), ensuring columns exist\n",
        "    # Preserve the order of variables as they were returned by the API + FIPS and Year\n",
        "    # The order will be FIPS, Year, followed by the variables in the order they were returned by the API\n",
        "    # and then renamed.\n",
        "    ordered_cols = ['FIPS', 'Year'] + [col for col in merged_df.columns if col not in ['FIPS', 'Year', 'state', 'county']]\n",
        "    census_df = merged_df[ordered_cols].copy()\n",
        "\n",
        "\n",
        "    # Save to CSV\n",
        "    census_df.to_csv('Census_import.csv', index=False) # Updated filename\n",
        "\n",
        "    print(f'\\n✓ Saved {len(census_df):,} rows to Census_import.csv')\n",
        "    print(f'  Counties: {census_df[\"FIPS\"].nunique()}')\n",
        "    print(f'  Year: {census_df[\"Year\"].iloc[0]}')\n",
        "    print(f'  Variables: {len([col for col in ACS_VARS.values() if col in census_df.columns])}') # Count variables actually in the DF\n",
        "\n",
        "else:\n",
        "    print(\"\\n Data download failed.\")\n"
      ],
      "metadata": {
        "id": "iN055jK9M3By"
      },
      "id": "iN055jK9M3By",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KqECawr_LcDg"
      },
      "id": "KqECawr_LcDg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UjtJ9_8Z7v8V",
        "outputId": "5cd10694-23c7-4dbd-fac7-c7cd3c5b3faf"
      },
      "id": "UjtJ9_8Z7v8V",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading Census ACS 5-Year data for 2011 in batches...\n",
            "\n",
            "  Fetching ACS 2011 5-Year estimates for 50 variables...\n",
            "    ✓ Successfully fetched 3,221 counties\n",
            "  Fetching ACS 2011 5-Year estimates for 5 variables...\n",
            "    ✓ Successfully fetched 3,221 counties\n",
            "\n",
            "2011 data download complete. Merged batches.\n",
            "\n",
            "✓ Saved 3,221 rows to Census_2011.csv\n",
            "  Counties: 3221\n",
            "  Year: 2011\n",
            "  Variables: 55\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def fetch_acs_county_data(year, variables, api_key):\n",
        "\n",
        "    print(f' Fetching ACS {year} 5-Year estimates for {len(variables)} variables...')\n",
        "\n",
        "    # Only include the variable IDs in the 'get' parameter\n",
        "    var_list = ','.join(variables)\n",
        "\n",
        "    params = {\n",
        "        'get': var_list,\n",
        "        'for': 'county:*',\n",
        "        'key': api_key\n",
        "    }\n",
        "\n",
        "    url = f'{Census_URL}/{year}/acs/acs5'\n",
        "\n",
        "    try:\n",
        "        response = requests.get(url, params=params, timeout=120)\n",
        "        response.raise_for_status()\n",
        "\n",
        "        data = response.json()\n",
        "        if len(data) > 1:\n",
        "            # Census API typically returns state and county FIPS as the last two columns\n",
        "            # even if not explicitly requested in 'get'.\n",
        "            # The first row is headers, subsequent rows are data\n",
        "            df = pd.DataFrame(data[1:], columns=data[0])\n",
        "            print(f'    ✓ Successfully fetched {len(df):,} counties')\n",
        "            return df\n",
        "        else:\n",
        "            print(f\"    No data returned for year {year} with variables {variables}\")\n",
        "            return pd.DataFrame() # Return empty DataFrame if no data rows\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"  Error fetching data for year {year}: {e}\")\n",
        "        if response is not None:\n",
        "            print(f\"  Status Code: {response.status_code}\")\n",
        "            print(f\"  Response Text: {response.text}\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "print(f'Downloading Census ACS 5-Year data for {YEARS[0]}-{YEARS[-1]}...\\n')\n",
        "\n",
        "all_data_dfs = []\n",
        "variable_list = list(ACS_VARS.keys())\n",
        "\n",
        "for year in YEARS:\n",
        "    print(f'Processing year {year}...')\n",
        "    df = fetch_acs_county_data(year, variable_list, CENSUS_API_KEY) # Pass all variables\n",
        "    if not df.empty:\n",
        "        # Add the Year column to the DataFrame for the current year\n",
        "        df['Year'] = year # Add year here\n",
        "        all_data_dfs.append(df)\n",
        "    time.sleep(1) # Small delay between year requests\n",
        "\n",
        "# Combine all years into CEN_df\n",
        "if all_data_dfs:\n",
        "    CEN_df = pd.concat(all_data_dfs, ignore_index=True)\n",
        "\n",
        "    # Apply renaming after combining all years\n",
        "    rename_dict = {k: v for k, v in ACS_VARS.items()}\n",
        "    CEN_df = CEN_df.rename(columns=rename_dict)\n",
        "\n",
        "    # Convert all numeric columns based on the renamed columns\n",
        "    for col in ACS_VARS.values():\n",
        "        if col in CEN_df.columns:\n",
        "            # Handle potential non-numeric values resulting from merges by coercing errors\n",
        "            CEN_df[col] = pd.to_numeric(CEN_df[col], errors='coerce')\n",
        "\n",
        "    # Add FIPS column with leading zeros\n",
        "    if 'state' in CEN_df.columns and 'county' in CEN_df.columns:\n",
        "        CEN_df['FIPS'] = CEN_df['state'].astype(str).str.zfill(2) + CEN_df['county'].astype(str).str.zfill(3)\n",
        "\n",
        "    # Ensure Year is integer type\n",
        "    if 'Year' in CEN_df.columns: # Check if 'Year' column exists before converting\n",
        "      CEN_df['Year'] = CEN_df['Year'].astype(int)\n",
        "\n",
        "    # Keep only FIPS, Year, and all variables (renamed), ensuring columns exist and preserving original order\n",
        "    # We will keep the original order of columns returned by the API for each year's batch,\n",
        "    # plus the 'FIPS' and 'Year' columns.\n",
        "    # The order will be the order of columns in the first downloaded dataframe,\n",
        "    # with 'FIPS' and 'Year' potentially added at the end unless explicitly reordered later.\n",
        "    # To preserve the user's desired final order, explicit column reordering would be needed,\n",
        "    # but the user requested not to change the variable order.\n",
        "    # The current approach adds FIPS and Year at the end.\n",
        "\n",
        "\n",
        "    # Save to CSV\n",
        "    CEN_df.to_csv('Census_data_2012_2021.csv', index=False) # Updated filename\n",
        "\n",
        "    print(f'\\n✓ Saved {len(CEN_df):,} rows to data_Census_ACS_2011_2021.csv')\n",
        "    print(f'  Counties: {CEN_df[\"FIPS\"].nunique()}')\n",
        "    print(f'  Years: {CEN_df[\"Year\"].min()}-{CEN_df[\"Year\"].max()}')\n",
        "    print(f'  Variables: {len(ACS_VARS)}')\n",
        "\n",
        "else:\n",
        "    print(\"\\n✗ No data was successfully downloaded for any year.\")"
      ],
      "metadata": {
        "id": "c9_TiqFFGT5J"
      },
      "id": "c9_TiqFFGT5J",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get API key: https://api.census.gov/data/key_signup.html\n",
        "key = Census('5f7c8b0ecd185273567de16a4c09273088d4340b')"
      ],
      "metadata": {
        "id": "t1GjV13ndXCL"
      },
      "id": "t1GjV13ndXCL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import and clean data files  \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "orO2NpGwlpjE"
      },
      "id": "orO2NpGwlpjE"
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the Census library and initialize the Census object\n",
        "from census import Census\n",
        "import pandas as pd\n",
        "import requests # Import requests to check variable availability\n",
        "\n",
        "# Get API key from the defined variable (assuming API_KEY_CENSUS or similar is available)\n",
        "# Using the key defined in cell t1GjV13ndXCL directly as it's present in the notebook\n",
        "c = Census('5f7c8b0ecd185273567de16a4c09273088d4340b')\n",
        "\n",
        "# Base URL for Census API\n",
        "BASE_URL = 'https://api.census.gov/data'\n",
        "\n",
        "\n",
        "# Function to check if a variable is available for a given year and dataset\n",
        "def is_variable_available(year, dataset, variable_id):\n",
        "    variables_url = f'{BASE_URL}/{year}/{dataset}/variables.json'\n",
        "    try:\n",
        "        variables_response = requests.get(variables_url, timeout=120)\n",
        "        variables_response.raise_for_status()\n",
        "        available_vars_data = variables_response.json()\n",
        "        return variable_id in available_vars_data.get('variables', {})\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error checking variable availability for year {year}: {e}\")\n",
        "        return False\n",
        "\n",
        "\n",
        "# Function to get property tax for one year\n",
        "def get_property_tax(year):\n",
        "    variable_id = 'B25103_002E' # Median property tax\n",
        "    dataset = 'acs/acs5' # Dataset name for ACS 5-Year estimates\n",
        "\n",
        "    # Check if the variable is available for this year\n",
        "    if not is_variable_available(year, dataset, variable_id):\n",
        "        print(f\"  Variable {variable_id} not available for year {year}. Skipping.\")\n",
        "        return pd.DataFrame() # Return empty DataFrame if variable is not available\n",
        "\n",
        "    print(f\"  Fetching property tax data for year {year}...\")\n",
        "    data = c.acs5.state_county(\n",
        "        fields=('NAME', variable_id),  # Median property tax\n",
        "        state_fips='*',\n",
        "        county_fips='*',\n",
        "        year=year\n",
        "    )\n",
        "\n",
        "    if data: # Check if data was returned\n",
        "        df = pd.DataFrame(data)\n",
        "        df['Year'] = year\n",
        "        # Ensure state and county FIPS are treated as strings with leading zeros before concatenation\n",
        "        df['County_FIPS'] = df['state'].astype(str).str.zfill(2) + df['county'].astype(str).str.zfill(3)\n",
        "        df = df.rename(columns={variable_id: 'Median_Property_Tax'})\n",
        "        # Select and reorder columns to match desired output\n",
        "        return df[['County_FIPS', 'NAME', 'Year', 'Median_Property_Tax']]\n",
        "    else:\n",
        "        print(f\"  No data returned from API for year {year} even though variable was listed as available.\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "\n",
        "# Get data for all years\n",
        "print('Downloading property tax data (2011-2021)...')\n",
        "dfs = [get_property_tax(year) for year in range(2011, 2022)]\n",
        "\n",
        "# Filter out empty DataFrames before concatenation\n",
        "dfs = [df for df in dfs if not df.empty]\n",
        "\n",
        "property_tax_data = pd.concat(dfs, ignore_index=True)\n",
        "\n",
        "# Save\n",
        "property_tax_data.to_csv('property_taxes_2011_2021.csv', index=False)\n",
        "\n",
        "# Confirm\n",
        "print('\\nProperty Tax Data Head:')\n",
        "print(property_tax_data.head())\n",
        "print('\\nProperty Tax Data Tail:')\n",
        "print(property_tax_data.tail())\n",
        "print(f'\\nTotal rows saved: {len(property_tax_data)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BAFXqwBXeRMh",
        "outputId": "bb3a1fac-9e76-42fb-b557-2acc3b7f2ee2"
      },
      "id": "BAFXqwBXeRMh",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading property tax data (2011-2021)...\n",
            "  Fetching property tax data for year 2011...\n",
            "  Fetching property tax data for year 2012...\n",
            "  Fetching property tax data for year 2013...\n",
            "  Fetching property tax data for year 2014...\n",
            "  Fetching property tax data for year 2015...\n",
            "  Fetching property tax data for year 2016...\n",
            "  Fetching property tax data for year 2017...\n",
            "  Fetching property tax data for year 2018...\n",
            "  Fetching property tax data for year 2019...\n",
            "  Fetching property tax data for year 2020...\n",
            "  Fetching property tax data for year 2021...\n",
            "\n",
            "Property Tax Data Head:\n",
            "  County_FIPS                               NAME  Year  Median_Property_Tax\n",
            "0       37043        Clay County, North Carolina  2011                667.0\n",
            "1       37051  Cumberland County, North Carolina  2011               1443.0\n",
            "2       37081    Guilford County, North Carolina  2011               1644.0\n",
            "3       37099     Jackson County, North Carolina  2011                708.0\n",
            "4       37139  Pasquotank County, North Carolina  2011               1266.0\n",
            "\n",
            "Property Tax Data Tail:\n",
            "      County_FIPS                              NAME  Year  Median_Property_Tax\n",
            "35420       72145  Vega Baja Municipio, Puerto Rico  2021                631.0\n",
            "35421       72147    Vieques Municipio, Puerto Rico  2021         -666666666.0\n",
            "35422       72149   Villalba Municipio, Puerto Rico  2021                494.0\n",
            "35423       72151    Yabucoa Municipio, Puerto Rico  2021         -666666666.0\n",
            "35424       72153      Yauco Municipio, Puerto Rico  2021                919.0\n",
            "\n",
            "Total rows saved: 35425\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import DP1 data (1 of 3)\n",
        "Demographic and Housing Characteristic (DHC) data from U.S. Census Bureau Decennial Survey at the County level. Includes 50 states, Puerto Rico, and D.C. *(Note on Alaska: See accompanying 'Alaska County' amalgamation file on github for method used to match census area to state senate district. DHC datafile combines 30 Alaskan census areas into 14 'County_fips' created for this analysis)*. Will drop Puerto Rico (rows 3144-3223). Also dropped Kalawao County, Hawaii: 82 residents, none of them voted, dropping will align it with MEDSL file when Kalawao county_fips (15005) is cleaned from MEDSL data.  \n",
        "https://data.census.gov/table?q=DP1&g=010XX00US$0500000&y=2020\n"
      ],
      "metadata": {
        "id": "A0WjxuVNguoR"
      },
      "id": "A0WjxuVNguoR"
    },
    {
      "cell_type": "code",
      "source": [
        "# Import first dataset\n",
        "DHC_import = pd.read_csv(\n",
        "    'DECENNIALDP2020.DP1-AKfix.csv', header=1)\n",
        "\n",
        "# Inspect\n",
        "print(DHC_import.head())\n",
        "print(DHC_import.info())"
      ],
      "metadata": {
        "collapsed": true,
        "id": "uK6_WZDCsQWd"
      },
      "id": "uK6_WZDCsQWd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Clean DHC data\n",
        "All 322 features will need:  \n",
        "to be renamed (for clarity) or  \n",
        "to be dropped (for redundency)\n",
        "Project will prioritize 'percent' variables, scale is improved over 'count'."
      ],
      "metadata": {
        "id": "3KLiwCkVhkBn"
      },
      "id": "3KLiwCkVhkBn"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d4d05e06-0624-410a-b39f-80c8041ddc9a",
      "metadata": {
        "scrolled": true,
        "id": "d4d05e06-0624-410a-b39f-80c8041ddc9a",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# Create new working dataframe\n",
        "DHC_clean = DHC_import.copy()\n",
        "\n",
        "# Remove Puerto Rico, rows where GEOID is US72000 or greater\n",
        "DHC_clean = DHC_clean[~DHC_clean['Geography'].str.startswith('0500000US72')]\n",
        "\n",
        "# Rename Geography and Geographic Area Name\n",
        "DHC_clean = DHC_clean.rename(columns={\n",
        "    'Geography': 'GEOID',\n",
        "    'Geographic Area Name': 'County'})\n",
        "DHC_clean['GEOID'] = DHC_clean['GEOID'].str[-5:]\n",
        "\n",
        "# Confirm\n",
        "print(DHC_clean.info())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Continue to CLEAN age data  \n",
        "[previously looked at 10 and 15 year groups (generation) during original analysis of Pennsylvania only data (before expanding to nationwide study)]"
      ],
      "metadata": {
        "id": "ypgkqg04h-kE"
      },
      "id": "ypgkqg04h-kE"
    },
    {
      "cell_type": "code",
      "source": [
        "# Rename age variables to keep, drop remaining\n",
        "DHC_clean = DHC_clean.rename(columns={\n",
        "    'Count!!SEX AND AGE!!Total population': 'Pop_total',\n",
        "    'Count!!SEX AND AGE!!Total population!!Under 5 years': 'Total_U5',\n",
        "    'Count!!SEX AND AGE!!Total population!!5 to 9 years': 'Total_5_9',\n",
        "    'Count!!SEX AND AGE!!Total population!!10 to 14 years': 'Total_10_14',\n",
        "    'Count!!SEX AND AGE!!Total population!!15 to 19 years': 'Total_15_19',\n",
        "    'Count!!SEX AND AGE!!Male population': 'Male_total',\n",
        "    'Count!!SEX AND AGE!!Male population!!Under 5 years': 'Male_U5',\n",
        "    'Count!!SEX AND AGE!!Male population!!5 to 9 years': 'Male_5_9',\n",
        "    'Count!!SEX AND AGE!!Male population!!10 to 14 years': 'Male_10_14',\n",
        "    'Count!!SEX AND AGE!!Male population!!15 to 19 years': 'Male_15_19',\n",
        "    'Count!!SEX AND AGE!!Female population': 'Female_total',\n",
        "    'Count!!SEX AND AGE!!Female population!!Under 5 years': 'Female_U5',\n",
        "    'Count!!SEX AND AGE!!Female population!!5 to 9 years': 'Female_5_9',\n",
        "    'Count!!SEX AND AGE!!Female population!!10 to 14 years': 'Female_10_14',\n",
        "    'Count!!SEX AND AGE!!Female population!!15 to 19 years': 'Female_15_19',\n",
        "    'Count!!SEX AND AGE!!Total population!!Selected Age Categories!!18 years and over': 'Total_18+',\n",
        "    'Count!!SEX AND AGE!!Male population!!Selected Age Categories!!18 years and over': 'Male_18+',\n",
        "    'Count!!SEX AND AGE!!Female population!!Selected Age Categories!!18 years and over': 'Female_18+'})\n",
        "\n",
        "columns_age_drop = [\n",
        "    'Count!!SEX AND AGE!!Total population!!20 to 24 years',\n",
        "    'Count!!SEX AND AGE!!Total population!!25 to 29 years',\n",
        "    'Count!!SEX AND AGE!!Total population!!30 to 34 years',\n",
        "    'Count!!SEX AND AGE!!Total population!!35 to 39 years',\n",
        "    'Count!!SEX AND AGE!!Total population!!40 to 44 years',\n",
        "    'Count!!SEX AND AGE!!Total population!!45 to 49 years',\n",
        "    'Count!!SEX AND AGE!!Total population!!50 to 54 years',\n",
        "    'Count!!SEX AND AGE!!Total population!!55 to 59 years',\n",
        "    'Count!!SEX AND AGE!!Total population!!60 to 64 years',\n",
        "    'Count!!SEX AND AGE!!Total population!!65 to 69 years',\n",
        "    'Count!!SEX AND AGE!!Total population!!70 to 74 years',\n",
        "    'Count!!SEX AND AGE!!Total population!!75 to 79 years',\n",
        "    'Count!!SEX AND AGE!!Total population!!80 to 84 years',\n",
        "    'Count!!SEX AND AGE!!Total population!!85 years and over',\n",
        "    'Count!!SEX AND AGE!!Total population!!Selected Age Categories!!16 years and over',\n",
        "    'Count!!SEX AND AGE!!Total population!!Selected Age Categories!!21 years and over',\n",
        "    'Count!!SEX AND AGE!!Total population!!Selected Age Categories!!62 years and over',\n",
        "    'Count!!SEX AND AGE!!Total population!!Selected Age Categories!!65 years and over',\n",
        "    'Count!!SEX AND AGE!!Male population!!20 to 24 years',\n",
        "    'Count!!SEX AND AGE!!Male population!!25 to 29 years',\n",
        "    'Count!!SEX AND AGE!!Male population!!30 to 34 years',\n",
        "    'Count!!SEX AND AGE!!Male population!!35 to 39 years',\n",
        "    'Count!!SEX AND AGE!!Male population!!40 to 44 years',\n",
        "    'Count!!SEX AND AGE!!Male population!!45 to 49 years',\n",
        "    'Count!!SEX AND AGE!!Male population!!50 to 54 years',\n",
        "    'Count!!SEX AND AGE!!Male population!!55 to 59 years',\n",
        "    'Count!!SEX AND AGE!!Male population!!60 to 64 years',\n",
        "    'Count!!SEX AND AGE!!Male population!!65 to 69 years',\n",
        "    'Count!!SEX AND AGE!!Male population!!70 to 74 years',\n",
        "    'Count!!SEX AND AGE!!Male population!!75 to 79 years',\n",
        "    'Count!!SEX AND AGE!!Male population!!80 to 84 years',\n",
        "    'Count!!SEX AND AGE!!Male population!!85 years and over',\n",
        "    'Count!!SEX AND AGE!!Male population!!Selected Age Categories!!16 years and over',\n",
        "    'Count!!SEX AND AGE!!Male population!!Selected Age Categories!!21 years and over',\n",
        "    'Count!!SEX AND AGE!!Male population!!Selected Age Categories!!62 years and over',\n",
        "    'Count!!SEX AND AGE!!Male population!!Selected Age Categories!!65 years and over',\n",
        "    'Count!!SEX AND AGE!!Female population!!20 to 24 years',\n",
        "    'Count!!SEX AND AGE!!Female population!!25 to 29 years',\n",
        "    'Count!!SEX AND AGE!!Female population!!30 to 34 years',\n",
        "    'Count!!SEX AND AGE!!Female population!!35 to 39 years',\n",
        "    'Count!!SEX AND AGE!!Female population!!40 to 44 years',\n",
        "    'Count!!SEX AND AGE!!Female population!!45 to 49 years',\n",
        "    'Count!!SEX AND AGE!!Female population!!50 to 54 years',\n",
        "    'Count!!SEX AND AGE!!Female population!!55 to 59 years',\n",
        "    'Count!!SEX AND AGE!!Female population!!60 to 64 years',\n",
        "    'Count!!SEX AND AGE!!Female population!!65 to 69 years',\n",
        "    'Count!!SEX AND AGE!!Female population!!70 to 74 years',\n",
        "    'Count!!SEX AND AGE!!Female population!!75 to 79 years',\n",
        "    'Count!!SEX AND AGE!!Female population!!80 to 84 years',\n",
        "    'Count!!SEX AND AGE!!Female population!!85 years and over',\n",
        "    'Count!!SEX AND AGE!!Female population!!Selected Age Categories!!16 years and over',\n",
        "    'Count!!SEX AND AGE!!Female population!!Selected Age Categories!!21 years and over',\n",
        "    'Count!!SEX AND AGE!!Female population!!Selected Age Categories!!62 years and over',\n",
        "    'Count!!SEX AND AGE!!Female population!!Selected Age Categories!!65 years and over']\n",
        "\n",
        "# Drop the specified columns\n",
        "DHC_clean.drop(columns=columns_age_drop, inplace=True)"
      ],
      "metadata": {
        "id": "vnBAGViN7FgP"
      },
      "id": "vnBAGViN7FgP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Rename percent age groups\n",
        "DHC_clean = DHC_clean.rename(columns={\n",
        "    'Percent!!SEX AND AGE!!Total population!!20 to 24 years': '%TOTAL_20_24',\n",
        "    'Percent!!SEX AND AGE!!Total population!!25 to 29 years': '%TOTAL_25_29',\n",
        "    'Percent!!SEX AND AGE!!Total population!!30 to 34 years': '%TOTAL_30_34',\n",
        "    'Percent!!SEX AND AGE!!Total population!!35 to 39 years': '%TOTAL_35_39',\n",
        "    'Percent!!SEX AND AGE!!Total population!!40 to 44 years': '%TOTAL_40_44',\n",
        "    'Percent!!SEX AND AGE!!Total population!!45 to 49 years': '%TOTAL_45_49',\n",
        "    'Percent!!SEX AND AGE!!Total population!!50 to 54 years': '%TOTAL_50_54',\n",
        "    'Percent!!SEX AND AGE!!Total population!!55 to 59 years': '%TOTAL_55_59',\n",
        "    'Percent!!SEX AND AGE!!Total population!!60 to 64 years': '%TOTAL_60_64',\n",
        "    'Percent!!SEX AND AGE!!Total population!!65 to 69 years': '%TOTAL_65_69',\n",
        "    'Percent!!SEX AND AGE!!Total population!!70 to 74 years': '%TOTAL_70_74',\n",
        "    'Percent!!SEX AND AGE!!Total population!!75 to 79 years': '%TOTAL_75_79',\n",
        "    'Percent!!SEX AND AGE!!Total population!!80 to 84 years': '%TOTAL_80_84',\n",
        "    'Percent!!SEX AND AGE!!Total population!!85 years and over': '%TOTAL_85+',\n",
        "    'Percent!!SEX AND AGE!!Male population!!20 to 24 years': '%MALE_20_24',\n",
        "    'Percent!!SEX AND AGE!!Male population!!25 to 29 years': '%MALE_25_29',\n",
        "    'Percent!!SEX AND AGE!!Male population!!30 to 34 years': '%MALE_30_34',\n",
        "    'Percent!!SEX AND AGE!!Male population!!35 to 39 years': '%MALE_35_39',\n",
        "    'Percent!!SEX AND AGE!!Male population!!40 to 44 years': '%MALE_40_44',\n",
        "    'Percent!!SEX AND AGE!!Male population!!45 to 49 years': '%MALE_45_49',\n",
        "    'Percent!!SEX AND AGE!!Male population!!50 to 54 years': '%MALE_50_54',\n",
        "    'Percent!!SEX AND AGE!!Male population!!55 to 59 years': '%MALE_55_59',\n",
        "    'Percent!!SEX AND AGE!!Male population!!60 to 64 years': '%MALE_60_64',\n",
        "    'Percent!!SEX AND AGE!!Male population!!65 to 69 years': '%MALE_65_69',\n",
        "    'Percent!!SEX AND AGE!!Male population!!70 to 74 years': '%MALE_70_74',\n",
        "    'Percent!!SEX AND AGE!!Male population!!75 to 79 years': '%MALE_75_79',\n",
        "    'Percent!!SEX AND AGE!!Male population!!80 to 84 years': '%MALE_80_84',\n",
        "    'Percent!!SEX AND AGE!!Male population!!85 years and over': '%MALE_85+',\n",
        "    'Percent!!SEX AND AGE!!Female population!!20 to 24 years': '%FEMALE_20_24',\n",
        "    'Percent!!SEX AND AGE!!Female population!!25 to 29 years': '%FEMALE_25_29',\n",
        "    'Percent!!SEX AND AGE!!Female population!!30 to 34 years': '%FEMALE_30_34',\n",
        "    'Percent!!SEX AND AGE!!Female population!!35 to 39 years': '%FEMALE_35_39',\n",
        "    'Percent!!SEX AND AGE!!Female population!!40 to 44 years': '%FEMALE_40_44',\n",
        "    'Percent!!SEX AND AGE!!Female population!!45 to 49 years': '%FEMALE_45_49',\n",
        "    'Percent!!SEX AND AGE!!Female population!!50 to 54 years': '%FEMALE_50_54',\n",
        "    'Percent!!SEX AND AGE!!Female population!!55 to 59 years': '%FEMALE_55_59',\n",
        "    'Percent!!SEX AND AGE!!Female population!!60 to 64 years': '%FEMALE_60_64',\n",
        "    'Percent!!SEX AND AGE!!Female population!!65 to 69 years': '%FEMALE_65_69',\n",
        "    'Percent!!SEX AND AGE!!Female population!!70 to 74 years': '%FEMALE_70_74',\n",
        "    'Percent!!SEX AND AGE!!Female population!!75 to 79 years': '%FEMALE_75_79',\n",
        "    'Percent!!SEX AND AGE!!Female population!!80 to 84 years': '%FEMALE_80_84',\n",
        "    'Percent!!SEX AND AGE!!Female population!!85 years and over': '%FEMALE_85+'})"
      ],
      "metadata": {
        "id": "De4qR0GyuiF1"
      },
      "id": "De4qR0GyuiF1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Continue to CLEAN median age data"
      ],
      "metadata": {
        "id": "9DlCv3ZYKDh7"
      },
      "id": "9DlCv3ZYKDh7"
    },
    {
      "cell_type": "code",
      "source": [
        "# Rename median variables\n",
        "DHC_clean = DHC_clean.rename(columns={\n",
        "    'Count!!MEDIAN AGE BY SEX!!Both sexes': 'MED_AGE',\n",
        "    'Count!!MEDIAN AGE BY SEX!!Male': 'MED_AGE_M',\n",
        "    'Count!!MEDIAN AGE BY SEX!!Female': 'MED_AGE_F'})\n",
        "\n",
        "# Reorder columns to move 'Median Age' next to population totals\n",
        "cols = DHC_clean.columns.tolist()\n",
        "cols.insert(20, cols.pop(cols.index('MED_AGE')))\n",
        "cols.insert(21, cols.pop(cols.index('MED_AGE_M')))\n",
        "cols.insert(22, cols.pop(cols.index('MED_AGE_F')))\n",
        "DHC_clean = DHC_clean[cols]"
      ],
      "metadata": {
        "id": "BWod75k007fx"
      },
      "id": "BWod75k007fx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Continue to CLEAN race data"
      ],
      "metadata": {
        "id": "4L6xmq_tiHyr"
      },
      "id": "4L6xmq_tiHyr"
    },
    {
      "cell_type": "code",
      "source": [
        "# Rename race variables to keep, drop remaining\n",
        "DHC_clean = DHC_clean.rename(columns={\n",
        "    'Percent!!HISPANIC OR LATINO BY RACE!!Total population!!Not Hispanic or Latino!!White alone': '%RACE_White',\n",
        "    'Percent!!HISPANIC OR LATINO BY RACE!!Total population!!Not Hispanic or Latino!!Black or African American alone': '%RACE_Black',\n",
        "    'Percent!!HISPANIC OR LATINO BY RACE!!Total population!!Hispanic or Latino': '%RACE_Latino',\n",
        "    'Percent!!HISPANIC OR LATINO BY RACE!!Total population!!Not Hispanic or Latino!!American Indian and Alaska Native alone': '%RACE_Native',\n",
        "    'Percent!!HISPANIC OR LATINO BY RACE!!Total population!!Not Hispanic or Latino!!Asian alone': '%RACE_Asian',\n",
        "    'Percent!!HISPANIC OR LATINO BY RACE!!Total population!!Not Hispanic or Latino!!Native Hawaiian and Other Pacific Islander alone': '%RACE_HI_PI',\n",
        "    'Percent!!HISPANIC OR LATINO BY RACE!!Total population!!Not Hispanic or Latino!!Some Other Race alone': '%RACE_Other',\n",
        "    'Percent!!HISPANIC OR LATINO BY RACE!!Total population!!Not Hispanic or Latino!!Two or More Races': '%RACE_Mixed'})\n",
        "\n",
        "columns_race_drop = [\n",
        "    'Count!!RACE!!Total population',\n",
        "    'Count!!RACE!!Total population!!One Race',\n",
        "    'Count!!RACE!!Total population!!One Race!!White',\n",
        "    'Count!!RACE!!Total population!!One Race!!Black or African American',\n",
        "    'Count!!RACE!!Total population!!One Race!!American Indian and Alaska Native',\n",
        "    'Count!!RACE!!Total population!!One Race!!Asian',\n",
        "    'Count!!RACE!!Total population!!One Race!!Native Hawaiian and Other Pacific Islander',\n",
        "    'Count!!RACE!!Total population!!One Race!!Some Other Race',\n",
        "    'Count!!RACE!!Total population!!Two or More Races',\n",
        "    'Count!!TOTAL RACES TALLIED [1]!!Total races tallied',\n",
        "    'Count!!TOTAL RACES TALLIED [1]!!Total races tallied!!White alone or in combination with one or more other races',\n",
        "    'Count!!TOTAL RACES TALLIED [1]!!Total races tallied!!Black or African American alone or in combination with one or more other races',\n",
        "    'Count!!TOTAL RACES TALLIED [1]!!Total races tallied!!American Indian and Alaska Native alone or in combination with one or more other races',\n",
        "    'Count!!TOTAL RACES TALLIED [1]!!Total races tallied!!Asian alone or in combination with one or more other races',\n",
        "    'Count!!TOTAL RACES TALLIED [1]!!Total races tallied!!Native Hawaiian and Other Pacific Islander alone or in combination with one or more other races',\n",
        "    'Count!!TOTAL RACES TALLIED [1]!!Total races tallied!!Some Other Race alone or in combination with one or more other races',\n",
        "    'Count!!HISPANIC OR LATINO!!Total population',\n",
        "    'Count!!HISPANIC OR LATINO!!Total population!!Hispanic or Latino (of any race)',\n",
        "    'Count!!HISPANIC OR LATINO!!Total population!!Not Hispanic or Latino',\n",
        "    'Count!!HISPANIC OR LATINO BY RACE!!Total population','Count!!HISPANIC OR LATINO BY RACE!!Total population!!Hispanic or Latino',\n",
        "    'Count!!HISPANIC OR LATINO BY RACE!!Total population!!Hispanic or Latino!!White alone',\n",
        "    'Count!!HISPANIC OR LATINO BY RACE!!Total population!!Hispanic or Latino!!Black or African American alone',\n",
        "    'Count!!HISPANIC OR LATINO BY RACE!!Total population!!Hispanic or Latino!!American Indian and Alaska Native alone',\n",
        "    'Count!!HISPANIC OR LATINO BY RACE!!Total population!!Hispanic or Latino!!Asian alone',\n",
        "    'Count!!HISPANIC OR LATINO BY RACE!!Total population!!Hispanic or Latino!!Native Hawaiian and Other Pacific Islander alone',\n",
        "    'Count!!HISPANIC OR LATINO BY RACE!!Total population!!Hispanic or Latino!!Some Other Race alone',\n",
        "    'Count!!HISPANIC OR LATINO BY RACE!!Total population!!Hispanic or Latino!!Two or More Races',\n",
        "    'Count!!HISPANIC OR LATINO BY RACE!!Total population!!Not Hispanic or Latino',\n",
        "    'Count!!HISPANIC OR LATINO BY RACE!!Total population!!Not Hispanic or Latino!!White alone',\n",
        "    'Count!!HISPANIC OR LATINO BY RACE!!Total population!!Not Hispanic or Latino!!Black or African American alone',\n",
        "    'Count!!HISPANIC OR LATINO BY RACE!!Total population!!Not Hispanic or Latino!!American Indian and Alaska Native alone',\n",
        "    'Count!!HISPANIC OR LATINO BY RACE!!Total population!!Not Hispanic or Latino!!Asian alone',\n",
        "    'Count!!HISPANIC OR LATINO BY RACE!!Total population!!Not Hispanic or Latino!!Native Hawaiian and Other Pacific Islander alone',\n",
        "    'Count!!HISPANIC OR LATINO BY RACE!!Total population!!Not Hispanic or Latino!!Some Other Race alone',\n",
        "    'Count!!HISPANIC OR LATINO BY RACE!!Total population!!Not Hispanic or Latino!!Two or More Races']\n",
        "\n",
        "# Drop the specified columns\n",
        "DHC_clean.drop(columns=columns_race_drop, inplace=True)"
      ],
      "metadata": {
        "id": "JP9dD4aUwp-i"
      },
      "id": "JP9dD4aUwp-i",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Continue to CLEAN relationship data"
      ],
      "metadata": {
        "id": "KOCtj_u4ieqA"
      },
      "id": "KOCtj_u4ieqA"
    },
    {
      "cell_type": "code",
      "source": [
        "# Rename relationship variables to keep, drop remaining\n",
        "DHC_clean = DHC_clean.rename(columns={\n",
        "    'Percent!!RELATIONSHIP!!Total population!!In households!!Opposite-sex spouse': '%REL_OP_SEX_MAR',\n",
        "    'Percent!!RELATIONSHIP!!Total population!!In households!!Same-sex spouse': '%REL_S_SEX_MAR',\n",
        "    'Percent!!RELATIONSHIP!!Total population!!In households!!Opposite-sex unmarried partner': '%REL_OP_SEX_UNMAR',\n",
        "    'Percent!!RELATIONSHIP!!Total population!!In households!!Same-sex unmarried partner': '%REL_S_SEX_UNMAR',\n",
        "    'Percent!!RELATIONSHIP!!Total population!!In households!!Other relatives': '%REL_W_RELATIVES',\n",
        "    'Percent!!RELATIONSHIP!!Total population!!In households!!Nonrelatives': '%REL_NON_REL',\n",
        "    'Percent!!RELATIONSHIP!!Total population!!In group quarters!!Institutionalized population:!!Male': '%REL_MALE_JAILED',\n",
        "    'Percent!!RELATIONSHIP!!Total population!!In group quarters!!Institutionalized population:!!Female': '%REL_FEMALE_JAILED',\n",
        "    'Percent!!RELATIONSHIP!!Total population!!In group quarters!!Noninstitutionalized population:!!Male': '%REL_MALE_GRP_DORM',\n",
        "    'Percent!!RELATIONSHIP!!Total population!!In group quarters!!Noninstitutionalized population:!!Female': '%REL_FEMALE_GRP_DORM'})\n",
        "\n",
        "columns_rel_drop = [\n",
        "    'Count!!RELATIONSHIP!!Total population',\n",
        "    'Count!!RELATIONSHIP!!Total population!!In households',\n",
        "    'Count!!RELATIONSHIP!!Total population!!In households!!Householder',\n",
        "    'Count!!RELATIONSHIP!!Total population!!In households!!Opposite-sex spouse',\n",
        "    'Count!!RELATIONSHIP!!Total population!!In households!!Same-sex spouse',\n",
        "    'Count!!RELATIONSHIP!!Total population!!In households!!Opposite-sex unmarried partner',\n",
        "    'Count!!RELATIONSHIP!!Total population!!In households!!Same-sex unmarried partner',\n",
        "    'Count!!RELATIONSHIP!!Total population!!In households!!Child [2]',\n",
        "    'Count!!RELATIONSHIP!!Total population!!In households!!Child [2]!!Under 18 years',\n",
        "    'Count!!RELATIONSHIP!!Total population!!In households!!Grandchild',\n",
        "    'Count!!RELATIONSHIP!!Total population!!In households!!Grandchild!!Under 18 years',\n",
        "    'Count!!RELATIONSHIP!!Total population!!In households!!Other relatives',\n",
        "    'Count!!RELATIONSHIP!!Total population!!In households!!Nonrelatives',\n",
        "    'Count!!RELATIONSHIP!!Total population!!In group quarters',\n",
        "    'Count!!RELATIONSHIP!!Total population!!In group quarters!!Institutionalized population:',\n",
        "    'Count!!RELATIONSHIP!!Total population!!In group quarters!!Institutionalized population:!!Male',\n",
        "    'Count!!RELATIONSHIP!!Total population!!In group quarters!!Institutionalized population:!!Female',\n",
        "    'Count!!RELATIONSHIP!!Total population!!In group quarters!!Noninstitutionalized population:',\n",
        "    'Count!!RELATIONSHIP!!Total population!!In group quarters!!Noninstitutionalized population:!!Male',\n",
        "    'Count!!RELATIONSHIP!!Total population!!In group quarters!!Noninstitutionalized population:!!Female']\n",
        "\n",
        "# Drop the specified columns\n",
        "DHC_clean.drop(columns=columns_rel_drop, inplace=True)"
      ],
      "metadata": {
        "id": "rmBJ6pbpgSVj"
      },
      "id": "rmBJ6pbpgSVj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Continue to CLEAN household data"
      ],
      "metadata": {
        "id": "QXWHC0OWxVQn"
      },
      "id": "QXWHC0OWxVQn"
    },
    {
      "cell_type": "code",
      "source": [
        "# Rename household variables to keep, drop remaining\n",
        "DHC_clean = DHC_clean.rename(columns={\n",
        "    'Percent!!HOUSEHOLDS BY TYPE!!Total households!!Married couple household': '%HH_MARRIED',\n",
        "    'Percent!!HOUSEHOLDS BY TYPE!!Total households!!Married couple household!!With own children under 18 [3]': '%HH_MAR_W_KIDS',\n",
        "    'Percent!!HOUSEHOLDS BY TYPE!!Total households!!Cohabiting couple household': '%HH_NOT_MAR',\n",
        "    'Percent!!HOUSEHOLDS BY TYPE!!Total households!!Cohabiting couple household!!With own children under 18 [3]': '%HH_NOT_MAR_W_KIDS',\n",
        "    'Percent!!HOUSEHOLDS BY TYPE!!Total households!!Male householder, no spouse or partner present:!!Living alone': '%HH_MALE_ALONE',\n",
        "    'Percent!!HOUSEHOLDS BY TYPE!!Total households!!Male householder, no spouse or partner present:!!Living alone!!65 years and over': '%HH_MALE_65+',\n",
        "    'Percent!!HOUSEHOLDS BY TYPE!!Total households!!Male householder, no spouse or partner present:!!With own children under 18 [3]': '%HH_MALE_W_KIDS',\n",
        "    'Percent!!HOUSEHOLDS BY TYPE!!Total households!!Female householder, no spouse or partner present:!!Living alone': '%HH_FEMALE_ALONE',\n",
        "    'Percent!!HOUSEHOLDS BY TYPE!!Total households!!Female householder, no spouse or partner present:!!Living alone!!65 years and over': '%HH_FEMALE_65+',\n",
        "    'Percent!!HOUSEHOLDS BY TYPE!!Total households!!Female householder, no spouse or partner present:!!With own children under 18 [3]': '%HH_FEMALE_W_KIDS'})\n",
        "\n",
        "columns_hhold_drop = [\n",
        "    'Count!!HOUSEHOLDS BY TYPE!!Total households',\n",
        "    'Count!!HOUSEHOLDS BY TYPE!!Total households!!Married couple household',\n",
        "    'Count!!HOUSEHOLDS BY TYPE!!Total households!!Married couple household!!With own children under 18 [3]',\n",
        "    'Count!!HOUSEHOLDS BY TYPE!!Total households!!Cohabiting couple household',\n",
        "    'Count!!HOUSEHOLDS BY TYPE!!Total households!!Cohabiting couple household!!With own children under 18 [3]',\n",
        "    'Count!!HOUSEHOLDS BY TYPE!!Total households!!Male householder, no spouse or partner present:',\n",
        "    'Count!!HOUSEHOLDS BY TYPE!!Total households!!Male householder, no spouse or partner present:!!Living alone',\n",
        "    'Count!!HOUSEHOLDS BY TYPE!!Total households!!Male householder, no spouse or partner present:!!Living alone!!65 years and over',\n",
        "    'Count!!HOUSEHOLDS BY TYPE!!Total households!!Male householder, no spouse or partner present:!!With own children under 18 [3]',\n",
        "    'Count!!HOUSEHOLDS BY TYPE!!Total households!!Female householder, no spouse or partner present:',\n",
        "    'Count!!HOUSEHOLDS BY TYPE!!Total households!!Female householder, no spouse or partner present:!!Living alone',\n",
        "    'Count!!HOUSEHOLDS BY TYPE!!Total households!!Female householder, no spouse or partner present:!!With own children under 18 [3]',\n",
        "    'Count!!HOUSEHOLDS BY TYPE!!Total households!!Female householder, no spouse or partner present:!!Living alone!!65 years and over',\n",
        "    'Count!!HOUSEHOLDS BY TYPE!!Total households!!Households with individuals under 18 years',\n",
        "    'Count!!HOUSEHOLDS BY TYPE!!Total households!!Households with individuals 65 years and over']\n",
        "\n",
        "# Drop the specified columns\n",
        "DHC_clean.drop(columns=columns_hhold_drop, inplace=True)"
      ],
      "metadata": {
        "id": "vgBwqhmCx1WM"
      },
      "id": "vgBwqhmCx1WM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Continue to CLEAN housing data"
      ],
      "metadata": {
        "id": "IhI-ByvxxlLJ"
      },
      "id": "IhI-ByvxxlLJ"
    },
    {
      "cell_type": "code",
      "source": [
        "# Rename housing variables to keep, drop remaining\n",
        "DHC_clean = DHC_clean.rename(columns={\n",
        "    'Percent!!HOUSING TENURE!!Occupied housing units!!Owner-occupied housing units': '%OWN_HOME',\n",
        "    'Percent!!HOUSING TENURE!!Occupied housing units!!Renter-occupied housing units': '%RENT_HOME'})\n",
        "\n",
        "columns_housing_drop = [\n",
        "    'Count!!HOUSING OCCUPANCY!!Total housing units',\n",
        "    'Count!!HOUSING OCCUPANCY!!Total housing units!!Occupied housing units',\n",
        "    'Count!!HOUSING OCCUPANCY!!Total housing units!!Vacant housing units',\n",
        "    'Count!!HOUSING OCCUPANCY!!Total housing units!!Vacant housing units!!For rent',\n",
        "    'Count!!HOUSING OCCUPANCY!!Total housing units!!Vacant housing units!!Rented, not occupied',\n",
        "    'Count!!HOUSING OCCUPANCY!!Total housing units!!Vacant housing units!!For sale only',\n",
        "    'Count!!HOUSING OCCUPANCY!!Total housing units!!Vacant housing units!!Sold, not occupied',\n",
        "    'Count!!HOUSING OCCUPANCY!!Total housing units!!Vacant housing units!!For seasonal, recreational, or occasional use',\n",
        "    'Count!!HOUSING OCCUPANCY!!Total housing units!!Vacant housing units!!All other vacants',\n",
        "    'Count!!VACANCY RATES!!Homeowner vacancy rate (percent) [4]',\n",
        "    'Count!!VACANCY RATES!!Rental vacancy rate (percent) [5]',\n",
        "    'Count!!HOUSING TENURE!!Occupied housing units',\n",
        "    'Count!!HOUSING TENURE!!Occupied housing units!!Owner-occupied housing units',\n",
        "    'Count!!HOUSING TENURE!!Occupied housing units!!Renter-occupied housing units']\n",
        "\n",
        "# Drop the specified columns\n",
        "DHC_clean.drop(columns=columns_housing_drop, inplace=True)"
      ],
      "metadata": {
        "id": "yQOTQoxC2fGu"
      },
      "id": "yQOTQoxC2fGu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Continue to CLEAN percentage data"
      ],
      "metadata": {
        "id": "KdNnz5UYSXTC"
      },
      "id": "KdNnz5UYSXTC"
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop remaining percentage variables\n",
        "columns_percent_drop = [\n",
        "    'Percent!!SEX AND AGE!!Total population',\n",
        "    'Percent!!SEX AND AGE!!Total population!!Under 5 years',\n",
        "    'Percent!!SEX AND AGE!!Total population!!5 to 9 years',\n",
        "    'Percent!!SEX AND AGE!!Total population!!10 to 14 years',\n",
        "    'Percent!!SEX AND AGE!!Total population!!15 to 19 years',\n",
        "    'Percent!!SEX AND AGE!!Total population!!Selected Age Categories!!16 years and over',\n",
        "    'Percent!!SEX AND AGE!!Total population!!Selected Age Categories!!18 years and over',\n",
        "    'Percent!!SEX AND AGE!!Total population!!Selected Age Categories!!21 years and over',\n",
        "    'Percent!!SEX AND AGE!!Total population!!Selected Age Categories!!62 years and over',\n",
        "    'Percent!!SEX AND AGE!!Total population!!Selected Age Categories!!65 years and over',\n",
        "    'Percent!!SEX AND AGE!!Male population',\n",
        "    'Percent!!SEX AND AGE!!Male population!!Under 5 years',\n",
        "    'Percent!!SEX AND AGE!!Male population!!5 to 9 years',\n",
        "    'Percent!!SEX AND AGE!!Male population!!10 to 14 years',\n",
        "    'Percent!!SEX AND AGE!!Male population!!15 to 19 years',\n",
        "    'Percent!!SEX AND AGE!!Male population!!Selected Age Categories!!16 years and over',\n",
        "    'Percent!!SEX AND AGE!!Male population!!Selected Age Categories!!18 years and over',\n",
        "    'Percent!!SEX AND AGE!!Male population!!Selected Age Categories!!21 years and over',\n",
        "    'Percent!!SEX AND AGE!!Male population!!Selected Age Categories!!62 years and over',\n",
        "    'Percent!!SEX AND AGE!!Male population!!Selected Age Categories!!65 years and over',\n",
        "    'Percent!!SEX AND AGE!!Female population',\n",
        "    'Percent!!SEX AND AGE!!Female population!!Under 5 years',\n",
        "    'Percent!!SEX AND AGE!!Female population!!5 to 9 years',\n",
        "    'Percent!!SEX AND AGE!!Female population!!10 to 14 years',\n",
        "    'Percent!!SEX AND AGE!!Female population!!15 to 19 years',\n",
        "    'Percent!!SEX AND AGE!!Female population!!Selected Age Categories!!16 years and over',\n",
        "    'Percent!!SEX AND AGE!!Female population!!Selected Age Categories!!18 years and over',\n",
        "    'Percent!!SEX AND AGE!!Female population!!Selected Age Categories!!21 years and over',\n",
        "    'Percent!!SEX AND AGE!!Female population!!Selected Age Categories!!62 years and over',\n",
        "    'Percent!!SEX AND AGE!!Female population!!Selected Age Categories!!65 years and over',\n",
        "    'Percent!!MEDIAN AGE BY SEX!!Both sexes',\n",
        "    'Percent!!MEDIAN AGE BY SEX!!Male',\n",
        "    'Percent!!MEDIAN AGE BY SEX!!Female',\n",
        "    'Percent!!RACE!!Total population',\n",
        "    'Percent!!RACE!!Total population!!One Race',\n",
        "    'Percent!!RACE!!Total population!!One Race!!White',\n",
        "    'Percent!!RACE!!Total population!!One Race!!Black or African American',\n",
        "    'Percent!!RACE!!Total population!!One Race!!American Indian and Alaska Native',\n",
        "    'Percent!!RACE!!Total population!!One Race!!Asian',\n",
        "    'Percent!!RACE!!Total population!!One Race!!Native Hawaiian and Other Pacific Islander',\n",
        "    'Percent!!RACE!!Total population!!One Race!!Some Other Race',\n",
        "    'Percent!!RACE!!Total population!!Two or More Races',\n",
        "    'Percent!!TOTAL RACES TALLIED [1]!!Total races tallied',\n",
        "    'Percent!!TOTAL RACES TALLIED [1]!!Total races tallied!!White alone or in combination with one or more other races',\n",
        "    'Percent!!TOTAL RACES TALLIED [1]!!Total races tallied!!Black or African American alone or in combination with one or more other races',\n",
        "    'Percent!!TOTAL RACES TALLIED [1]!!Total races tallied!!American Indian and Alaska Native alone or in combination with one or more other races',\n",
        "    'Percent!!TOTAL RACES TALLIED [1]!!Total races tallied!!Asian alone or in combination with one or more other races',\n",
        "    'Percent!!TOTAL RACES TALLIED [1]!!Total races tallied!!Native Hawaiian and Other Pacific Islander alone or in combination with one or more other races',\n",
        "    'Percent!!TOTAL RACES TALLIED [1]!!Total races tallied!!Some Other Race alone or in combination with one or more other races',\n",
        "    'Percent!!HISPANIC OR LATINO!!Total population',\n",
        "    'Percent!!HISPANIC OR LATINO!!Total population!!Hispanic or Latino (of any race)',\n",
        "    'Percent!!HISPANIC OR LATINO!!Total population!!Not Hispanic or Latino',\n",
        "    'Percent!!HISPANIC OR LATINO BY RACE!!Total population',\n",
        "    'Percent!!HISPANIC OR LATINO BY RACE!!Total population!!Hispanic or Latino!!White alone',\n",
        "    'Percent!!HISPANIC OR LATINO BY RACE!!Total population!!Hispanic or Latino!!Black or African American alone',\n",
        "    'Percent!!HISPANIC OR LATINO BY RACE!!Total population!!Hispanic or Latino!!American Indian and Alaska Native alone',\n",
        "    'Percent!!HISPANIC OR LATINO BY RACE!!Total population!!Hispanic or Latino!!Asian alone',\n",
        "    'Percent!!HISPANIC OR LATINO BY RACE!!Total population!!Hispanic or Latino!!Native Hawaiian and Other Pacific Islander alone',\n",
        "    'Percent!!HISPANIC OR LATINO BY RACE!!Total population!!Hispanic or Latino!!Some Other Race alone',\n",
        "    'Percent!!HISPANIC OR LATINO BY RACE!!Total population!!Hispanic or Latino!!Two or More Races',\n",
        "    'Percent!!HISPANIC OR LATINO BY RACE!!Total population!!Not Hispanic or Latino',\n",
        "    'Percent!!RELATIONSHIP!!Total population',\n",
        "    'Percent!!RELATIONSHIP!!Total population!!In households',\n",
        "    'Percent!!RELATIONSHIP!!Total population!!In households!!Householder',\n",
        "    'Percent!!RELATIONSHIP!!Total population!!In households!!Child [2]',\n",
        "    'Percent!!RELATIONSHIP!!Total population!!In households!!Child [2]!!Under 18 years',\n",
        "    'Percent!!RELATIONSHIP!!Total population!!In households!!Grandchild',\n",
        "    'Percent!!RELATIONSHIP!!Total population!!In households!!Grandchild!!Under 18 years',\n",
        "    'Percent!!RELATIONSHIP!!Total population!!In group quarters',\n",
        "    'Percent!!RELATIONSHIP!!Total population!!In group quarters!!Institutionalized population:',\n",
        "    'Percent!!RELATIONSHIP!!Total population!!In group quarters!!Noninstitutionalized population:',\n",
        "    'Percent!!HOUSEHOLDS BY TYPE!!Total households',\n",
        "    'Percent!!HOUSEHOLDS BY TYPE!!Total households!!Male householder, no spouse or partner present:',\n",
        "    'Percent!!HOUSEHOLDS BY TYPE!!Total households!!Female householder, no spouse or partner present:',\n",
        "    'Percent!!HOUSEHOLDS BY TYPE!!Total households!!Households with individuals under 18 years',\n",
        "    'Percent!!HOUSEHOLDS BY TYPE!!Total households!!Households with individuals 65 years and over',\n",
        "    'Percent!!HOUSING OCCUPANCY!!Total housing units',\n",
        "    'Percent!!HOUSING OCCUPANCY!!Total housing units!!Occupied housing units',\n",
        "    'Percent!!HOUSING OCCUPANCY!!Total housing units!!Vacant housing units',\n",
        "    'Percent!!HOUSING OCCUPANCY!!Total housing units!!Vacant housing units!!For rent',\n",
        "    'Percent!!HOUSING OCCUPANCY!!Total housing units!!Vacant housing units!!Rented, not occupied',\n",
        "    'Percent!!HOUSING OCCUPANCY!!Total housing units!!Vacant housing units!!For sale only',\n",
        "    'Percent!!HOUSING OCCUPANCY!!Total housing units!!Vacant housing units!!Sold, not occupied',\n",
        "    'Percent!!HOUSING OCCUPANCY!!Total housing units!!Vacant housing units!!For seasonal, recreational, or occasional use',\n",
        "    'Percent!!HOUSING OCCUPANCY!!Total housing units!!Vacant housing units!!All other vacants',\n",
        "    'Percent!!VACANCY RATES!!Homeowner vacancy rate (percent) [4]',\n",
        "    'Percent!!VACANCY RATES!!Rental vacancy rate (percent) [5]',\n",
        "    'Percent!!HOUSING TENURE!!Occupied housing units']\n",
        "\n",
        "# Drop the specified columns\n",
        "DHC_clean.drop(columns=columns_percent_drop, inplace=True)\n",
        "\n",
        "#Confirm\n",
        "#print(DHC_clean.info())"
      ],
      "metadata": {
        "id": "aF5omwI8P8Kf",
        "collapsed": true
      },
      "id": "aF5omwI8P8Kf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transform DHC data"
      ],
      "metadata": {
        "id": "y-ZwBs1UgqYw"
      },
      "id": "y-ZwBs1UgqYw"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create new 'Under 18' and '18-19' age groups"
      ],
      "metadata": {
        "id": "5Zkp0T5ZS9nj"
      },
      "id": "5Zkp0T5ZS9nj"
    },
    {
      "cell_type": "code",
      "source": [
        "# Create new working dataframe\n",
        "DHC_transform = DHC_clean.copy()\n",
        "\n",
        "# Calculate 'Under 18' by subtracting '18 years and over' from 'totals'\n",
        "DHC_transform['Total_U18'] = DHC_transform[\n",
        "    'Pop_total'] - DHC_transform['Total_18+']\n",
        "DHC_transform['Male_U18'] = DHC_transform[\n",
        "    'Male_total'] - DHC_transform['Male_18+']\n",
        "DHC_transform['Female_U18'] = DHC_transform[\n",
        "    'Female_total'] - DHC_transform['Female_18+']\n",
        "\n",
        "# Calculate 'Total_18-19' by adding all ages 0-19 and subtracting U18\n",
        "DHC_transform['Total_18_19'] = (DHC_transform['Total_U5'] +\n",
        "    DHC_transform['Total_5_9'] + DHC_transform['Total_10_14'] +\n",
        "    DHC_transform['Total_15_19'] - DHC_transform['Total_U18'])\n",
        "\n",
        "# Repeat for Male 18-19\n",
        "DHC_transform['Male_18_19'] = (DHC_transform['Male_U5'] +\n",
        "    DHC_transform['Male_5_9'] + DHC_transform['Male_10_14'] +\n",
        "    DHC_transform['Male_15_19'] - DHC_transform['Male_U18'])\n",
        "\n",
        "# Repeat for Female 18-19\n",
        "DHC_transform['Female_18_19'] = (DHC_transform['Female_U5'] +\n",
        "    DHC_transform['Female_5_9'] + DHC_transform['Female_10_14'] +\n",
        "    DHC_transform['Female_15_19'] - DHC_transform['Female_U18'])\n",
        "\n",
        "# Calculate '%_18-19' by dividing by 'totals'\n",
        "DHC_transform['%TOTAL_18_19'] = (\n",
        "    DHC_transform['Total_18_19'] / DHC_transform['Pop_total']* 100).round(2)\n",
        "DHC_transform['%MALE_18_19'] = (\n",
        "    DHC_transform['Male_18_19'] / DHC_transform['Male_total']* 100).round(2)\n",
        "DHC_transform['%FEMALE_18_19'] = (\n",
        "    DHC_transform['Female_18_19'] / DHC_transform['Female_total']* 100).round(2)\n",
        "\n",
        "# Can now drop these columns\n",
        "columns_tform_drop = [\n",
        "    'Total_U5', 'Male_U5', 'Female_U5',\n",
        "    'Total_5_9', 'Male_5_9', 'Female_5_9',\n",
        "    'Total_10_14', 'Male_10_14', 'Female_10_14',\n",
        "    'Total_15_19', 'Male_15_19', 'Female_15_19',\n",
        "    'Total_18+', 'Male_18+', 'Female_18+',\n",
        "    'Total_U18', 'Male_U18', 'Female_U18',\n",
        "    'Total_18_19', 'Male_18_19', 'Female_18_19']\n",
        "DHC_transform.drop(columns=columns_tform_drop, inplace=True)\n",
        "\n",
        "# Reorder columns to move '18-19' before '20-24'\n",
        "cols = DHC_transform.columns.tolist()\n",
        "cols.insert(8, cols.pop(cols.index('%TOTAL_18_19')))\n",
        "cols.insert(23, cols.pop(cols.index('%MALE_18_19')))\n",
        "cols.insert(38, cols.pop(cols.index('%FEMALE_18_19')))\n",
        "DHC_transform = DHC_transform[cols]\n",
        "\n",
        "#Confirm\n",
        "#pd.set_option('display.max_columns', None)\n",
        "#print(DHC_transform.head())\n",
        "#print(DHC_transform.info())"
      ],
      "metadata": {
        "id": "dAQB-_Ny8iXx",
        "collapsed": true
      },
      "id": "dAQB-_Ny8iXx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save DHC cleaned data"
      ],
      "metadata": {
        "id": "zplfSEYVg3Dh"
      },
      "id": "zplfSEYVg3Dh"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c26101d-493b-4562-b987-f9bdc233820f",
      "metadata": {
        "id": "3c26101d-493b-4562-b987-f9bdc233820f"
      },
      "outputs": [],
      "source": [
        "# Create the tidy dataframe\n",
        "DHC_tidy = DHC_transform.copy()\n",
        "\n",
        "DHC_tidy.to_csv('DHC_tidy.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import P2 data (2 of 3)\n",
        "\n",
        "P2 data is the population living in urban or rural (PUR) areas within each county **[number of households also available (H2)]**.  \n",
        "For the 2020 Census, an urban area will comprise a densely settled core of census blocks that meet minimum population density requirements. This includes adjacent territory containing non-residential urban land uses. To qualify as an urban area, the territory identified according to criteria must have a population of at least 5,000. *(Note on Alaska: See accompanying 'Alaska County' amalgamation file on github for method used to match census area to state senate district. PUR datafile combines 30 census areas into 14 'County_fips' created for this analysis)*. Also dropped Kalawao County, Hawaii: 82 rural residents, none of them voted, dropping will align it with MEDSL file when Kalawao county_fips (15005) is cleaned from MEDSL data.    \n",
        "https://data.census.gov/all?q=urban+and+rural&g=010XX00US$0500000"
      ],
      "metadata": {
        "id": "7b5RnzYIcT3Q"
      },
      "id": "7b5RnzYIcT3Q"
    },
    {
      "cell_type": "code",
      "source": [
        "# Import next dataset\n",
        "PUR_import = pd.read_csv(\n",
        "    'DECENNIALDHC2020.P2-AKfix.csv', header=1)\n",
        "\n",
        "# Inspect\n",
        "print(PUR_import.info())\n",
        "print(PUR_import.head())"
      ],
      "metadata": {
        "id": "97ihi91wgP4B",
        "collapsed": true
      },
      "id": "97ihi91wgP4B",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Clean PUR data"
      ],
      "metadata": {
        "id": "a_e9eDxJhbQe"
      },
      "id": "a_e9eDxJhbQe"
    },
    {
      "cell_type": "code",
      "source": [
        "# Create new working dataframe\n",
        "PUR_clean = PUR_import.copy()\n",
        "\n",
        "# Remove Puerto Rico: rows where GEOID is US72000 or greater\n",
        "PUR_clean = PUR_clean[~PUR_clean['Geography'].str.startswith('0500000US72')]\n",
        "\n",
        "# Rename variables to keep and drop remaining\n",
        "PUR_clean = PUR_clean.rename(columns={\n",
        "    'Geography': 'GEOID',\n",
        "    ' !!Total:': 'Pop_total',\n",
        "    ' !!Total:!!Urban': 'Pop_Urban',\n",
        "    ' !!Total:!!Rural': 'Pop_Rural'})\n",
        "\n",
        "PUR_clean['GEOID'] = PUR_clean['GEOID'].str[-5:]\n",
        "\n",
        "# Calculate Urban percent\n",
        "PUR_clean['%Urban_pop'] = (\n",
        "    (PUR_clean['Pop_Urban'] / PUR_clean['Pop_total']) * 100).round(2)\n",
        "\n",
        "# Drop the specified columns\n",
        "columns_PUR_drop = ['Pop_total',\n",
        "                    'Geographic Area Name',\n",
        "                    ' !!Total:!!Not defined for this file']\n",
        "PUR_clean.drop(columns=columns_PUR_drop, inplace=True)\n",
        "\n",
        "# Confirm\n",
        "#print(PUR_clean.info())"
      ],
      "metadata": {
        "collapsed": true,
        "id": "npT_eugNhcIU"
      },
      "id": "npT_eugNhcIU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save PUR data"
      ],
      "metadata": {
        "id": "QGz3_A_Yicwu"
      },
      "id": "QGz3_A_Yicwu"
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the tidy dataframe\n",
        "PUR_tidy = PUR_clean.copy()\n",
        "\n",
        "PUR_tidy.to_csv('PUR_tidy.csv', index=False)"
      ],
      "metadata": {
        "id": "JfAS76fjie58"
      },
      "id": "JfAS76fjie58",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import MEDSL data (3 of 3)\n",
        "2020 general election results for most* (46) of the 50 states and D.C. downloaded from MEDSL (the MIT Election Data and Science Lab) https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/NT66Z3  \n",
        "\n",
        "* ALASKA: voting data is not gathered by county, MEDSL 'county_fips' is empty. Used https://www.elections.alaska.gov/results/20GENR/Map/ Votes aggregated to state senate districts (1 - 40). See accompanying 'Alaska County' amalgamation file on github for method used to match 30 census areas to 40 state senate districts. MEDSL datafile uses 14 County_fips created for this analysis. Datafile only has the 4 variables that will be utilized here.\n",
        "\n",
        "* INDIANA: MEDSL missing multiple county results. Used https://indianavoters.in.gov/ENRHistorical/ElectionResults  Datafile only has the 4 variables that will be utilized here, aggregated to the county level  \n",
        "\n",
        "* NEW MEXICO: To protect the privacy of voters, New Mexico 'masks' vote totals in precinct results for candidates with small vote tallies. Used https://electionstats.sos.nm.gov/contest/13250  Datafile only has the 4 variables that will be utilized here, aggregated to the county level  \n",
        "\n",
        "* NEVADA: To protect the privacy of voters, Nevada 'masks' vote totals in precinct results for candidates with 1-10 vote tallies. Used https://www.nvsos.gov/SOSelectionPages/results/2020StateWideGeneral/ElectionSummary.aspx  Datafile only has the 4 variables that will be utilized here, aggregated to the county level\n",
        "\n",
        "##Pre-import processing Notes:  \n",
        "The below adjustments were made to the MEDSL datafiles to standardize cleaning and processing.   \n",
        "\n",
        "1. HAWAII: Adjusted DHC and PUR data regarding Kalawao County, Hawaii. Both have fips 15005, but there are no official votes cast, removed so all files align  \n",
        "\n",
        "1. MAINE: Uniformed and Overseas Citizens Absentee Voting tallied seperately in 23000 fips, 23000 deleted to match DHC and PUR with votes added to 23005 (most populous county)  \n",
        "\n",
        "1. MICHIGAN: MEDSL precinct data contains precinct '9999', which are 'statistical adjustments' rows. There were minor corrections needed to match official results at https://www.michigan.gov/sos/elections/election-results-and-data/candidate-listings-and-election-results-by-county  \n",
        "\n",
        "1. MINNESOTA: 'DEMOCRATIC FARMER LABOR' party changed to 'DEMOCRAT'  \n",
        "\n",
        "1. MISSOURI: MEDSL tallied Kansas City votes seperately in 36000 fips. Utillized https://www.sos.mo.gov/CMSImages/ElectionResultsStatistics/November3_2020GeneralElection.pdf to aportion some votes to Jackson County with remainder assigned to Clay County (official results not available on https://www.voteclaycountymo.gov/election-results), but totals match State official numbers  \n",
        "\n",
        "1.  NEW YORK: 'CONSERVATIVE' party changed to 'REPUBLICAN'  \n",
        "'WORKING FAMILIES' party changed to 'DEMOCRAT'  \n",
        "\n",
        "1.  NORTH DAKOTA: 'DEMOCRATIC-NPL' party changed to 'DEMOCRAT' and 'county_fips' for OGLALA LAKOTA County changed from 46113 to 46102 to match data from DHC and PUR  \n",
        "\n",
        "1.  OREGON: Sherman County included cadidate 'BALLOTS CAST' which totaled all votes in each precinct: Deleted  \n",
        "\n",
        "1.  PENNSYLVANIA: 1 blank 'party_detailed' vote cast for Trump, party changed to 'REPUBLICAN'  \n",
        "\n",
        "1.  VERMONT: 3 blank 'party_detailed' votes cast for Trump, party changed to 'REPUBLICAN'  \n",
        "6 blank 'party_detailed' votes cast for Biden, party changed to 'DEMOCRAT'  \n",
        "\n",
        "##Post-import cleaning Notes:\n",
        "1.  All blanks in 'party_detailed' have been verified as writein votes cast for 'THIRD' party candidates  \n",
        "\n",
        "2.  In Nov 2020, there were over 50 recognized political parties in the US.  \n",
        "DEM and REP ballots accounted for 96% of total votes. Third parties accounted for 1-4% of the vote in each state. 'THIRD' will combine any vote NOT for Presidents Biden or Trump.  \n",
        "\n"
      ],
      "metadata": {
        "id": "3BLXE3tAg-ZV"
      },
      "id": "3BLXE3tAg-ZV"
    },
    {
      "cell_type": "code",
      "source": [
        "# Define list of all 51 voter CSV files to process (50 states plus D.C.)\n",
        "file_list = glb.glob('2020-*-precinct-general.csv')\n",
        "\n",
        "# Define function to read, select features, and clean a single CSV file\n",
        "def process_file(file_path):\n",
        "\n",
        "    try:\n",
        "# Specify data types, let 'votes' be float during import\n",
        "        dtype_spec = {'office': str, 'county_fips': str,\n",
        "                      'party_detailed': str, 'votes': float}\n",
        "        df = pd.read_csv(file_path, dtype=dtype_spec, low_memory=False)\n",
        "\n",
        "# Filter for President in 'office' to avoid counting multiple votes per person\n",
        "# Only analyze US Presidential race (it has the most voter participation)\n",
        "        df = df[df['office'] == 'US PRESIDENT'].copy()\n",
        "        df = df[['office', 'county_fips', 'party_detailed', 'votes']]\n",
        "        df = df.rename(columns={\n",
        "            'county_fips': 'GEOID',\n",
        "            'party_detailed': 'PARTY',\n",
        "            'votes': 'VOTES'})\n",
        "        return df\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f'Error processing {file_path}: {e}')\n",
        "        return None"
      ],
      "metadata": {
        "id": "1XNttjNMPRry"
      },
      "id": "1XNttjNMPRry",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Iterate through the file list, apply function, and store dataframes\n",
        "all_processed_dataframes = [\n",
        "    process_file(file_path) for file_path in file_list]\n",
        "\n",
        "# Filter out any None values if errors occurred during processing\n",
        "all_processed_dataframes = [\n",
        "    df for df in all_processed_dataframes if df is not None]\n",
        "\n",
        "# Concatenate all processed files into single dataframe\n",
        "US_combined = pd.concat(all_processed_dataframes, ignore_index=True)\n",
        "\n",
        "# Confirm\n",
        "#print(US_combined.info())\n",
        "#print(US_combined.head())"
      ],
      "metadata": {
        "id": "LN4oYN6UiPiT"
      },
      "id": "LN4oYN6UiPiT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Clean MEDSL data"
      ],
      "metadata": {
        "id": "8szuOb6tuQy_"
      },
      "id": "8szuOb6tuQy_"
    },
    {
      "cell_type": "code",
      "source": [
        "# Fill missing values with 'THIRD'\n",
        "US_combined.loc[:, 'PARTY'] = US_combined['PARTY'].fillna('THIRD')\n",
        "\n",
        "# Create list of parties to rename\n",
        "print(sorted(US_combined['PARTY'].unique()))"
      ],
      "metadata": {
        "id": "C17J4CPBTpTj"
      },
      "id": "C17J4CPBTpTj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6f5ae6da-c471-4027-8073-665b6ea95549",
      "metadata": {
        "id": "6f5ae6da-c471-4027-8073-665b6ea95549",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# Define other parties to replace with 'THIRD' (remove DEMOCRAT and REPUBLICAN from 'US_combined' output)\n",
        "other_parties = [\n",
        "    'ALLIANCE', 'ALLIANCE PARTY', 'AMERICAN', 'AMERICAN CONSTITUTION', 'AMERICAN SHOPPING', 'AMERICAN SOLIDARITY', 'APPROVAL VOTING', 'BECOMING ONE NATION', 'BIRTHDAY', 'BLANK', 'BOILING FROG', 'BREAD AND ROSES', 'BULL MOOSE', 'C.U.P', 'CONSTITUTION', 'CONSTITUTION PARTY', 'CUP', 'FREEDOM AND PROSPERITY', 'GENEALOGY KNOW YOUR FAMILY HISTORY', 'GREEN', 'GREEN INDEPENDENT', 'GREEN-RAINBOW', 'GRUMPY OLD PATRIOTS', 'INDEPENDENCE', 'INDEPENDENCE ALLIANCE', 'INDEPENDENT', 'INDEPENDENT AMERICAN', 'LIBERTARIAN', 'LIBERTY UNION', 'LIFE', 'LIFE LIBERTY CONSTITUTION', 'NATURAL LAW PARTY', 'NONE', 'NONPARTISAN', 'OREGON PROGRESSIVE', 'OTHER', 'PACIFIC GREEN', 'PARTY FOR SOCIALISM AND LIBERATION', 'PROGRESSIVE', 'PROHIBITION', 'REFORM', 'SOCIALISM', 'SOCIALISM AND LIBERATION', 'SOCIALIST', 'SOCIALIST EQUALITY', 'SOCIALIST WORKERS', 'STATEWIDE GREEN', 'UNAFFILIATED', 'UNITY', 'UNITY AMERICA', 'UNITY OF COLORADO', 'US TAXPAYERS PARTY']\n",
        "\n",
        "# Replace these other parties with 'THIRD'\n",
        "US_combined['PARTY'] = US_combined['PARTY'].replace(other_parties, 'THIRD')\n",
        "\n",
        "# Tally Presidential votes\n",
        "PRES_votes = (US_combined.groupby('PARTY', as_index=False)['VOTES']\n",
        "    .sum().sort_values(by='VOTES', ascending=False))\n",
        "\n",
        "# Confirm\n",
        "print(US_combined.info())\n",
        "print(US_combined['PARTY'].unique())\n",
        "print(US_combined['PARTY'].value_counts(dropna=False))\n",
        "print(PRES_votes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2f3bb4e7-fbae-403b-af0a-1dc74dc75f8a",
      "metadata": {
        "scrolled": true,
        "id": "2f3bb4e7-fbae-403b-af0a-1dc74dc75f8a",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# Pivot to get vote counts by Party\n",
        "US_transform = (US_combined.groupby(['GEOID', 'PARTY'])['VOTES']\n",
        "    .sum()\n",
        "    .unstack(fill_value=0)\n",
        "    .reset_index())\n",
        "\n",
        "# Rename columns that were the party names after unstacking\n",
        "US_transform = US_transform.rename(columns={\n",
        "    'DEMOCRAT': 'DEM_VOTES',\n",
        "    'REPUBLICAN': 'REP_VOTES',\n",
        "    'THIRD': 'THRD_VOTES'})\n",
        "\n",
        "# Change vote columns to int32\n",
        "vote_cols = ['DEM_VOTES', 'REP_VOTES', 'THRD_VOTES']\n",
        "US_transform[vote_cols] = US_transform[vote_cols].astype('int32')\n",
        "\n",
        "# Confirm\n",
        "#print(US_transform.info())\n",
        "#print(US_transform.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create share of vote feature"
      ],
      "metadata": {
        "id": "pfkaBMaAHa-V"
      },
      "id": "pfkaBMaAHa-V"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6adb69cf-c68d-4197-86a0-4d0cf787fb99",
      "metadata": {
        "scrolled": true,
        "id": "6adb69cf-c68d-4197-86a0-4d0cf787fb99"
      },
      "outputs": [],
      "source": [
        "# Create new working dataframe\n",
        "US_tranfm2 = US_transform.copy()\n",
        "\n",
        "# Compute TOTAL_VOTES, drop any where the sum of all votes = 0\n",
        "US_tranfm2['TOTAL_VOTES'] = US_tranfm2[vote_cols].sum(axis=1).astype('int32')\n",
        "US_tranfm2 = US_tranfm2[US_tranfm2['TOTAL_VOTES'] != 0]\n",
        "\n",
        "# Compute shares of votes\n",
        "US_tranfm2['DEM_SHARE'] = (\n",
        "    (US_tranfm2['DEM_VOTES'] / US_tranfm2['TOTAL_VOTES'])* 100).round(2)\n",
        "US_tranfm2['REP_SHARE'] = (\n",
        "    (US_tranfm2['REP_VOTES'] / US_tranfm2['TOTAL_VOTES'])* 100).round(2)\n",
        "US_tranfm2['THRD_SHARE'] = (\n",
        "    (US_tranfm2['THRD_VOTES'] / US_tranfm2['TOTAL_VOTES'])* 100).round(2)\n",
        "\n",
        "# Confirm\n",
        "#print(US_tranfm2.info())\n",
        "#print(US_tranfm2.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create political leaning feature"
      ],
      "metadata": {
        "id": "tV4-3lQkHnV0"
      },
      "id": "tV4-3lQkHnV0"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4a29ead3"
      },
      "source": [
        "# View values of DEM_SHARE, ensure all >0\n",
        "print(sorted(US_tranfm2['DEM_SHARE'].unique()))"
      ],
      "id": "4a29ead3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "861238c1-9836-410e-af43-2b6a918f9788",
      "metadata": {
        "scrolled": true,
        "id": "861238c1-9836-410e-af43-2b6a918f9788",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# Create new working dataframe\n",
        "US_tranfm3 = US_tranfm2.copy()\n",
        "\n",
        "# Define the political leaning function\n",
        "def determine_win(row):\n",
        "# Only DEM or REP win, only consider their shares for determining lead\n",
        "    shares = {\n",
        "        'DEM': row['DEM_SHARE'],\n",
        "        'REP': row['REP_SHARE']}\n",
        "\n",
        "    # Determine the winning party between DEM and REP\n",
        "    if shares['DEM'] > shares['REP']:\n",
        "        party_win = 1 # Democrat wins = Positive lead for DEM\n",
        "        party_lead = (shares['DEM'] - shares['REP']) / 100\n",
        "    elif shares['REP'] > shares['DEM']: # Corrected from else\n",
        "        party_win = 0 # Republican wins = Negative lead for REP\n",
        "        party_lead = (shares['DEM'] - shares['REP']) / 100\n",
        "    else: # Tie (very unlikely)\n",
        "        party_win = 2\n",
        "        party_lead = 0.0\n",
        "\n",
        "    return party_win, round(party_lead, 2)\n",
        "\n",
        "# Apply function and create two new variables\n",
        "US_tranfm3[['PARTY_WIN', 'PARTY_LEAD']] = US_tranfm3.apply(\n",
        "    determine_win, axis=1).apply(pd.Series)\n",
        "\n",
        "# Convert 'PARTY_WIN' to int\n",
        "US_tranfm3['PARTY_WIN'] = US_tranfm3['PARTY_WIN'].astype('int32')\n",
        "\n",
        "# Confirm\n",
        "print(US_tranfm3.head(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save MEDSL data"
      ],
      "metadata": {
        "id": "u62yAvnWHu4i"
      },
      "id": "u62yAvnWHu4i"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "83768481-ad9d-44d3-b4f4-46a4b3768ab1",
      "metadata": {
        "scrolled": true,
        "id": "83768481-ad9d-44d3-b4f4-46a4b3768ab1",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# Order variables\n",
        "final_cols = ['GEOID', 'TOTAL_VOTES',\n",
        "              'DEM_VOTES', 'DEM_SHARE',\n",
        "              'REP_VOTES', 'REP_SHARE',\n",
        "              'THRD_VOTES', 'THRD_SHARE',\n",
        "              'PARTY_WIN', 'PARTY_LEAD']\n",
        "\n",
        "# Create the tidy dataframe\n",
        "MEDSL_tidy = US_tranfm3[final_cols]\n",
        "\n",
        "MEDSL_tidy.to_csv('MEDSL_tidy.csv', index=False)\n",
        "\n",
        "# Confirm\n",
        "print(MEDSL_tidy.info())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Merge data files"
      ],
      "metadata": {
        "id": "2vanEG68IBcU"
      },
      "id": "2vanEG68IBcU"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import _tidy files here if you do not want to clean the data"
      ],
      "metadata": {
        "id": "i5-7tTinDzML"
      },
      "id": "i5-7tTinDzML"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "# Import here if utilizing the _tidy files\n",
        "DHC_tidy = pd.read_csv('DHC_tidy.csv')\n",
        "PUR_tidy = pd.read_csv('PUR_tidy.csv')\n",
        "MEDSL_tidy = pd.read_csv('MEDSL_tidy.csv')\n",
        "\n",
        "# Confirm\n",
        "print(DHC_tidy.info())\n",
        "print(PUR_tidy.info())\n",
        "print(MEDSL_tidy.info())"
      ],
      "metadata": {
        "id": "O0jpcAOQDylG",
        "collapsed": true
      },
      "id": "O0jpcAOQDylG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "M0qZvjDvCTUB",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# Merge first two files\n",
        "TWO_join = pd.merge(DHC_tidy, PUR_tidy, on='GEOID', how='outer')\n",
        "\n",
        "# Confirm\n",
        "#print(TWO_join.info())"
      ],
      "id": "M0qZvjDvCTUB"
    },
    {
      "cell_type": "code",
      "source": [
        "# Merge with third dataset\n",
        "FULL_DF = pd.merge(TWO_join, MEDSL_tidy, on='GEOID', how='outer')\n",
        "\n",
        "# change GEOID type\n",
        "FULL_DF['GEOID'] = FULL_DF['GEOID'].astype(str)\n",
        "\n",
        "# Confirm\n",
        "#print(FULL_DF.info())"
      ],
      "metadata": {
        "id": "r3_SMWlhcTpi",
        "collapsed": true
      },
      "id": "r3_SMWlhcTpi",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create new working dataframe\n",
        "FULL_transform = FULL_DF.copy()\n",
        "\n",
        "# Split 'Name' into 'County' and 'State'\n",
        "FULL_transform[['County', 'State']] = FULL_transform[\n",
        "    'County'].str.split(', ', expand=True)\n",
        "\n",
        "# Reorder columns to move 'State' to index 1\n",
        "cols = FULL_transform.columns.tolist()\n",
        "cols.insert(1, cols.pop(cols.index('State')))\n",
        "MERGED_DF = FULL_transform[cols]"
      ],
      "metadata": {
        "id": "jQLuxhTtiFzG"
      },
      "id": "jQLuxhTtiFzG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save MERGED datafile"
      ],
      "metadata": {
        "id": "tcE3avVMUVmZ"
      },
      "id": "tcE3avVMUVmZ"
    },
    {
      "cell_type": "code",
      "source": [
        "MERGED_DF.to_csv('MERGED_DF.csv', index=False)"
      ],
      "metadata": {
        "id": "c9t1xr4nUaO4"
      },
      "id": "c9t1xr4nUaO4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exploratory Data Analysis (with MERGED_DF)"
      ],
      "metadata": {
        "id": "Db35tTfshpfU"
      },
      "id": "Db35tTfshpfU"
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup exploration environment\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "%matplotlib inline\n",
        "print('Environment Ready')"
      ],
      "metadata": {
        "id": "vCHwEDBjtOhJ"
      },
      "id": "vCHwEDBjtOhJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d6416223-79d2-4eea-8253-104ed250f502",
      "metadata": {
        "id": "d6416223-79d2-4eea-8253-104ed250f502",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "MERGED_DF = pd.read_csv('MERGED_DF.csv')\n",
        "# ensure GEOID is an object\n",
        "MERGED_DF['GEOID'] = MERGED_DF['GEOID'].astype(str)\n",
        "\n",
        "# Confirm\n",
        "print(MERGED_DF.info())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Basic Information"
      ],
      "metadata": {
        "id": "digFEi-Pf4-E"
      },
      "id": "digFEi-Pf4-E"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a0b7a653"
      },
      "source": [
        "# Display descriptive statistics for numerical columns\n",
        "print('Number of rows:', MERGED_DF.shape[0], '(Number of counties)')\n",
        "print('Number of columns:', MERGED_DF.shape[1])\n",
        "print('\\nMissing Values: None')\n",
        "print(MERGED_DF.isna().sum().sort_values(ascending=False))\n",
        "\n",
        "print('\\nDescriptive Statistics for Numerical Columns:')\n",
        "display(MERGED_DF.describe())\n",
        "\n",
        "# Display value counts for categorical column (PARTY_WIN)\n",
        "print('\\nValue Counts for 'PARTY_WIN' \\n0: Republican Win\\n1: Democrat Win:')\n",
        "display(MERGED_DF['PARTY_WIN'].value_counts())"
      ],
      "id": "a0b7a653",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualizations"
      ],
      "metadata": {
        "id": "6U447_NdnbvO"
      },
      "id": "6U447_NdnbvO"
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize the distribution of the target variable 'PARTY_WIN'\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.countplot(x='PARTY_WIN', data=MERGED_DF)\n",
        "plt.title('Distribution of PARTY_WIN (0: Republican Win, 1: Democrat Win)')\n",
        "plt.xlabel('Party Win')\n",
        "plt.ylabel('Count')\n",
        "plt.xticks([0, 1], ['Republican Win', 'Democrat Win'])\n",
        "plt.show()\n",
        "\n",
        "print('')\n",
        "# Visualize the distribution of 'PARTY_LEAD'\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15,5))\n",
        "sns.histplot(MERGED_DF['DEM_SHARE'], bins=30, kde=True, ax=axes[0], color='blue')\n",
        "axes[0].set_title('Democratic Vote Share')\n",
        "sns.histplot(MERGED_DF['REP_SHARE'], bins=30, kde=True, ax=axes[1], color='red')\n",
        "axes[1].set_title('Republican Vote Share')\n",
        "sns.histplot(MERGED_DF['PARTY_LEAD'], bins=30, kde=True, ax=axes[2], color='purple')\n",
        "axes[2].set_title('Margin of Victory (Party Lead)')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print('')\n",
        "sns.histplot(MERGED_DF['Pop_total'], bins=50, kde=True)\n",
        "plt.title('County Population Distribution')\n",
        "plt.xlabel('Population')\n",
        "plt.ylabel('Number of Counties')\n",
        "plt.show()\n",
        "\n",
        "print('')\n",
        "sns.histplot(MERGED_DF['%Urban_pop'], bins=30, kde=True)\n",
        "plt.title('Urban Population Share by County')\n",
        "plt.xlabel('% Urban Population')\n",
        "plt.ylabel('Number of Counties')\n",
        "plt.show()\n",
        "\n",
        "print('')\n",
        "sns.histplot(MERGED_DF['MED_AGE'], bins=30, kde=True)\n",
        "plt.title('Median Age Distribution')\n",
        "plt.xlabel('Median Age')\n",
        "plt.ylabel('Number of Counties')\n",
        "plt.show()\n",
        "\n",
        "print('')\n",
        "race_cols = ['%RACE_White', '%RACE_Black', '%RACE_Latino', '%RACE_Asian']\n",
        "MERGED_DF[race_cols].plot(kind='box', figsize=(8,6))\n",
        "plt.title('Distribution of Racial Composition by County')\n",
        "plt.ylabel('Percentage')\n",
        "plt.show()\n",
        "\n",
        "print('')\n",
        "sns.scatterplot(x='%OWN_HOME', y='%RENT_HOME', data=MERGED_DF)\n",
        "plt.title('Own vs Rent in Counties')\n",
        "plt.xlabel('% Own Home')\n",
        "plt.ylabel('% Rent Home')\n",
        "plt.show()\n",
        "\n",
        "print('')\n",
        "sns.scatterplot(x='%Urban_pop', y='DEM_SHARE', data=MERGED_DF, alpha=0.6)\n",
        "plt.title('Urban Population vs Democratic Vote Share')\n",
        "plt.show()\n",
        "\n",
        "print('')\n",
        "sns.scatterplot(x='%RACE_White', y='REP_SHARE', data=MERGED_DF, alpha=0.6, color='red')\n",
        "plt.title('% White Population vs Republican Vote Share')\n",
        "plt.show()\n",
        "\n",
        "print('')\n",
        "sns.scatterplot(x='MED_AGE', y='REP_SHARE', data=MERGED_DF, alpha=0.6, color='green')\n",
        "plt.title('Median Age vs Republican Vote Share')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "71v2mSxuncdD"
      },
      "id": "71v2mSxuncdD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Correlation checks on separated groups of features"
      ],
      "metadata": {
        "id": "bu3scl36eF51"
      },
      "id": "bu3scl36eF51"
    },
    {
      "cell_type": "code",
      "source": [
        "#corr_vars = ['Pop_total', 'MED_AGE', '%Urban_pop',\n",
        "#             '%RACE_White', '%RACE_Black', '%RACE_Latino',\n",
        "#             '%OWN_HOME', '%RENT_HOME',\n",
        "#             'DEM_SHARE', 'REP_SHARE', 'PARTY_LEAD']\n",
        "\n",
        "#corr = MERGED_DF[corr_vars].corr()\n",
        "MERGED_num = MERGED_DF.select_dtypes(include=np.number)\n",
        "\n",
        "corr = MERGED_num.corr()\n",
        "\n",
        "plt.figure(figsize=(10,8))\n",
        "sns.heatmap(corr, annot=False, fmt='.2f', cmap='coolwarm', center=0)\n",
        "plt.title('Correlation Heatmap')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "LmiEfl2TP3ZD"
      },
      "id": "LmiEfl2TP3ZD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Analyze Age data"
      ],
      "metadata": {
        "id": "T6sApGow-rrc"
      },
      "id": "T6sApGow-rrc"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d0cb92f9"
      },
      "source": [
        "# Create new working dataframe\n",
        "MERGED_trform = MERGED_DF.copy()\n",
        "\n",
        "# Define column groups for total, male, and female age percentages\n",
        "age_total_cols = [\n",
        "    col for col in MERGED_trform.columns if col.startswith('%TOTAL_')]\n",
        "age_male_cols  = [\n",
        "    col for col in MERGED_trform.columns if col.startswith('%MALE_')]\n",
        "age_female_cols = [\n",
        "    col for col in MERGED_trform.columns if col.startswith('%FEMALE_')]\n",
        "\n",
        "# Combine all percentage age columns and the target variables\n",
        "features_for_age = age_total_cols + age_male_cols + age_female_cols + [\n",
        "    'PARTY_WIN', 'PARTY_LEAD']\n",
        "\n",
        "# Calculate the correlation matrix for the selected features\n",
        "corr_age = MERGED_trform[features_for_age].corr()\n",
        "\n",
        "# Select and display only the correlations with PARTY_WIN and PARTY_LEAD\n",
        "corr_age_subset = corr_age[['PARTY_WIN', 'PARTY_LEAD']].loc[\n",
        "    age_total_cols + age_male_cols + age_female_cols]\n",
        "\n",
        "# Plot heatmap for better visualization of correlations\n",
        "plt.figure(figsize=(10, 15)) # Adjust figure size as needed\n",
        "sns.heatmap(corr_age_subset,\n",
        "            cmap='seismic_r',\n",
        "            annot=True, fmt='.2f',\n",
        "            vmin=-1, vmax=1)\n",
        "plt.title('Correlation Heatmap: Percentage Age Groups vs PARTY_WIN and PARTY_LEAD')\n",
        "plt.yticks(rotation=0)\n",
        "plt.show()"
      ],
      "id": "d0cb92f9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### With a clear divergence around age 55, compare 2 vs 3 age groupings  \n",
        "- yng, mid, old: Looks to break groups into pos, neutral (between -0.1 and 0.1), neg  \n",
        "- young, older: Looks to break age groups into positive and negative only  "
      ],
      "metadata": {
        "id": "StFYlTn8-4pI"
      },
      "id": "StFYlTn8-4pI"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4506464d",
        "collapsed": true
      },
      "source": [
        "# Define lists of age columns for young, middle (cutoff is |0.1|), and old\n",
        "age_male_yng = [col for col in MERGED_trform.columns if col.startswith('%MALE_') and any(age in col for age in ['18_19', '20_24', '25_29', '30_34', '35_39'])]\n",
        "age_male_mid = [col for col in MERGED_trform.columns if col.startswith('%MALE_') and any(age in col for age in ['40_44', '45_49', '50_54'])]\n",
        "age_male_old = [col for col in MERGED_trform.columns if col.startswith('%MALE_') and any(age in col for age in ['55_59', '60_64', '65_69', '70_74', '75_79', '80_84', '85+'])]\n",
        "age_female_yng = [col for col in MERGED_trform.columns if col.startswith('%FEMALE_') and any(age in col for age in ['18_19', '20_24', '25_29', '30_34', '35_39', '40_44'])]\n",
        "age_female_mid = [col for col in MERGED_trform.columns if col.startswith('%FEMALE_') and any(age in col for age in ['45_49', '50_54'])]\n",
        "age_female_old = [col for col in MERGED_trform.columns if col.startswith('%FEMALE_') and any(age in col for age in ['55_59', '60_64', '65_69', '70_74', '75_79', '80_84', '85+'])]\n",
        "\n",
        "# Define lists of age columns for young (cutoff is 0) and older\n",
        "age_male_young = [col for col in MERGED_trform.columns if col.startswith('%MALE_') and any(age in col for age in ['18_19', '20_24', '25_29', '30_34', '35_39', '40_44', '45_49'])]\n",
        "age_male_older = [col for col in MERGED_trform.columns if col.startswith('%MALE_') and any(age in col for age in ['50_54', '55_59', '60_64', '65_69', '70_74', '75_79', '80_84', '85+'])]\n",
        "age_female_young = [col for col in MERGED_trform.columns if col.startswith('%FEMALE_') and any(age in col for age in ['18_19', '20_24', '25_29', '30_34', '35_39', '40_44', '45_49', '50_54'])]\n",
        "age_female_older = [col for col in MERGED_trform.columns if col.startswith('%FEMALE_') and any(age in col for age in ['55_59', '60_64', '65_69', '70_74', '75_79', '80_84', '85+'])]\n",
        "\n",
        "# Calculate the new aggregated percentage age groups\n",
        "MERGED_trform['%AGE_MALE_YNG'] = MERGED_trform[age_male_yng].sum(axis=1).round(2)\n",
        "MERGED_trform['%AGE_MALE_MID'] = MERGED_trform[age_male_mid].sum(axis=1).round(2)\n",
        "MERGED_trform['%AGE_MALE_OLD'] = MERGED_trform[age_male_old].sum(axis=1).round(2)\n",
        "MERGED_trform['%AGE_MALE_YOUNG'] = MERGED_trform[age_male_young].sum(axis=1).round(2)\n",
        "MERGED_trform['%AGE_MALE_OLDER'] = MERGED_trform[age_male_older].sum(axis=1).round(2)\n",
        "\n",
        "MERGED_trform['%AGE_FEMALE_YNG'] = MERGED_trform[age_female_yng].sum(axis=1).round(2)\n",
        "MERGED_trform['%AGE_FEMALE_MID'] = MERGED_trform[age_female_mid].sum(axis=1).round(2)\n",
        "MERGED_trform['%AGE_FEMALE_OLD'] = MERGED_trform[age_female_old].sum(axis=1).round(2)\n",
        "MERGED_trform['%AGE_FEMALE_YOUNG'] = MERGED_trform[age_female_young].sum(axis=1).round(2)\n",
        "MERGED_trform['%AGE_FEMALE_OLDER'] = MERGED_trform[age_female_older].sum(axis=1).round(2)\n",
        "\n",
        "# Confirm\n",
        "print(MERGED_trform.info())\n",
        "print(MERGED_trform.head())"
      ],
      "id": "4506464d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Analyze race groups"
      ],
      "metadata": {
        "id": "7iSBoE5IFpZ2"
      },
      "id": "7iSBoE5IFpZ2"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "61a03ee5"
      },
      "source": [
        "# Define the list of race percentage columns\n",
        "race_cols = [col for col in MERGED_trform.columns if col.startswith('%RACE_')]\n",
        "\n",
        "# Combine race percentage columns and the target variables\n",
        "features_for_race = race_cols + ['PARTY_WIN', 'PARTY_LEAD']\n",
        "\n",
        "# Calculate the correlation matrix\n",
        "corr_race = MERGED_trform[features_for_race].corr()\n",
        "\n",
        "# Select and display only the correlations with PARTY_WIN and PARTY_LEAD\n",
        "corr_race_subset = corr_race[['PARTY_WIN', 'PARTY_LEAD']].loc[race_cols]\n",
        "\n",
        "# Display the correlations\n",
        "print('Correlation of Race/Ethnic Group Percentages with PARTY_WIN and PARTY_LEAD:')\n",
        "display(corr_race_subset.sort_values(by='PARTY_LEAD', key=abs, ascending=False))\n",
        "\n",
        "# Plot heatmap for better visualization\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(corr_race_subset, cmap='seismic_r', annot=True, fmt='.2f', vmin=-1, vmax=1)\n",
        "plt.title('Correlation Heatmap: Race/Ethnic Group Percentages vs PARTY_WIN and PARTY_LEAD')\n",
        "plt.yticks(rotation=0)\n",
        "plt.show()"
      ],
      "id": "61a03ee5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "With clear racial differences, I will try two variations of race groups  \n",
        "- White and Non-White  \n",
        "- White, strong political lean, more neutral lean"
      ],
      "metadata": {
        "id": "emS9onA2FBET"
      },
      "id": "emS9onA2FBET"
    },
    {
      "cell_type": "code",
      "source": [
        "# Define lists to compare 2 groups: non-whites or with a cutoff of |0.2|\n",
        "RACE_NonWhite = [col for col in MERGED_trform.columns if col.startswith('%RACE_') and any(race in col for race in ['Asian', 'Black', 'Other', 'Latino', 'Native', 'HI_PI', 'Mixed'])]\n",
        "RACE_BAO = [col for col in MERGED_trform.columns if col.startswith('%RACE_') and any(race in col for race in ['Black', 'Asian', 'Other'])]\n",
        "RACE_LNHM = [col for col in MERGED_trform.columns if col.startswith('%RACE_') and any(race in col for race in ['Latino', 'Native', 'HI_PI', 'Mixed'])]\n",
        "\n",
        "# Calculate the new aggregated percentage race groups\n",
        "MERGED_trform['%RACE_NonWhite'] = MERGED_trform[RACE_NonWhite].sum(axis=1).round(2)\n",
        "MERGED_trform['%RACE_BAO'] = MERGED_trform[RACE_BAO].sum(axis=1).round(2)\n",
        "MERGED_trform['%RACE_LNHM'] = MERGED_trform[RACE_LNHM].sum(axis=1).round(2)"
      ],
      "metadata": {
        "id": "6UnXrijUFOiU"
      },
      "id": "6UnXrijUFOiU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Analyze relationship groups"
      ],
      "metadata": {
        "id": "lFf97gr-gqR_"
      },
      "id": "lFf97gr-gqR_"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dc56dff3"
      },
      "source": [
        "# Filter for columns starting with '%REL_'\n",
        "rel_cols = [col for col in MERGED_trform.columns if col.startswith('%REL_')]\n",
        "\n",
        "# Calculate the correlation of these columns with PARTY_WIN and PARTY_LEAD\n",
        "corr_rel = MERGED_trform[rel_cols + ['PARTY_WIN', 'PARTY_LEAD']].corr()\n",
        "\n",
        "# Select and display only the correlations with PARTY_WIN and PARTY_LEAD\n",
        "corr_rel_subset = corr_rel[['PARTY_WIN', 'PARTY_LEAD']].loc[rel_cols]\n",
        "\n",
        "# Display the correlations\n",
        "print('Correlation of Relationship Variables with PARTY_WIN and PARTY_LEAD:')\n",
        "display(corr_rel_subset.sort_values(by='PARTY_LEAD', key=abs, ascending=False))\n",
        "\n",
        "# Optional: Visualize correlations as a heatmap\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(corr_rel_subset,\n",
        "            cmap='seismic_r',\n",
        "            annot=True, fmt='.2f',\n",
        "            vmin=-1, vmax=1)\n",
        "plt.title('Correlation Heatmap: Relationship Variables vs PARTY_WIN and PARTY_LEAD')\n",
        "plt.yticks(rotation=0)\n",
        "plt.show()"
      ],
      "id": "dc56dff3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Analyze household groups"
      ],
      "metadata": {
        "id": "Yodje20Rg4cx"
      },
      "id": "Yodje20Rg4cx"
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter for columns starting with '%HH_'\n",
        "hh_cols = [col for col in MERGED_trform.columns if col.startswith('%HH_')]\n",
        "\n",
        "# Add own and urban columns\n",
        "hh_cols.extend(['%OWN_HOME', '%Urban_pop'])\n",
        "\n",
        "# Correlate these columns with PARTY_WIN and PARTY_LEAD\n",
        "corr_hh = MERGED_trform[hh_cols + ['PARTY_WIN', 'PARTY_LEAD']].corr()\n",
        "\n",
        "# Select and display only the correlations with PARTY_WIN and PARTY_LEAD\n",
        "corr_hh_subset = corr_hh[['PARTY_WIN', 'PARTY_LEAD']].loc[hh_cols]\n",
        "\n",
        "# Display the correlations\n",
        "print('Correlation of Household, Ownership, and Urban Variables with PARTY_WIN and PARTY_LEAD:')\n",
        "display(corr_hh_subset.sort_values(by='PARTY_LEAD', key=abs, ascending=False))\n",
        "\n",
        "# Optional: Visualize correlations as a heatmap\n",
        "plt.figure(figsize=(8, 10)) # Adjusted figure size\n",
        "sns.heatmap(corr_hh_subset,\n",
        "            cmap='seismic_r',\n",
        "            annot=True, fmt='.2f',\n",
        "            vmin=-1, vmax=1)\n",
        "plt.title('Correlation Heatmap: Household, Ownership, and Urban Variables vs PARTY_WIN and PARTY_LEAD')\n",
        "plt.yticks(rotation=0)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "OpxhE7HlN6D3"
      },
      "id": "OpxhE7HlN6D3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save VOTE_DF"
      ],
      "metadata": {
        "id": "XQzEWtOiy6hj"
      },
      "id": "XQzEWtOiy6hj"
    },
    {
      "cell_type": "code",
      "source": [
        "# List columns to keep (drop HH_totals, only keep M_total and F_total as ref)\n",
        "columns_to_keep = [\n",
        "    'GEOID', 'Male_total', 'Female_total', '%AGE_MALE_YNG', '%AGE_MALE_MID', '%AGE_MALE_OLD', '%AGE_MALE_YOUNG', '%AGE_MALE_OLDER', '%AGE_FEMALE_YNG', '%AGE_FEMALE_MID', '%AGE_FEMALE_OLD', '%AGE_FEMALE_YOUNG', '%AGE_FEMALE_OLDER', '%RACE_White', '%RACE_Black', '%RACE_Latino', '%RACE_Native', '%RACE_Asian', '%RACE_HI_PI', '%RACE_Other', '%RACE_Mixed', '%RACE_NonWhite', '%RACE_BAO', '%RACE_LNHM', '%REL_OP_SEX_MAR', '%REL_OP_SEX_UNMAR', '%REL_S_SEX_MAR', '%REL_S_SEX_UNMAR', '%REL_W_RELATIVES', '%REL_NON_REL', '%REL_MALE_JAILED', '%REL_FEMALE_JAILED', '%REL_MALE_GRP_DORM', '%REL_FEMALE_GRP_DORM', '%HH_MARRIED', '%HH_MAR_W_KIDS','%HH_NOT_MAR',  '%HH_NOT_MAR_W_KIDS', '%HH_MALE_ALONE', '%HH_MALE_65+', '%HH_MALE_W_KIDS', '%HH_FEMALE_ALONE', '%HH_FEMALE_65+', '%HH_FEMALE_W_KIDS', '%OWN_HOME', '%Urban_pop', 'PARTY_WIN', 'PARTY_LEAD']\n",
        "\n",
        "# Create VOTE dataframe\n",
        "VOTE_DF = MERGED_trform[columns_to_keep].copy()\n",
        "\n",
        "VOTE_DF.to_csv('VOTE_DF.csv', index=False)"
      ],
      "metadata": {
        "id": "GcrFAQVraqCb",
        "collapsed": true
      },
      "id": "GcrFAQVraqCb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## EDA complete; dataframe cleaned, merged, transformed, partially reduced, and ready for analysis"
      ],
      "metadata": {
        "id": "DVfNdSAXw94R"
      },
      "id": "DVfNdSAXw94R"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature analysis (with VOTE_DF)"
      ],
      "metadata": {
        "id": "02vUwlYly7Bl"
      },
      "id": "02vUwlYly7Bl"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import numpy.typing as npt\n",
        "import statsmodels.api as sm\n",
        "from typing import Literal, Tuple, Union\n",
        "from scipy.stats import shapiro, mannwhitneyu, rankdata, norm\n",
        "from sklearn.linear_model import LogisticRegression, LassoCV\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, mean_squared_error, r2_score, mean_absolute_error, precision_score, recall_score, f1_score, roc_auc_score\n",
        "from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.feature_selection import RFECV\n",
        "from sklearn.inspection import permutation_importance, PartialDependenceDisplay\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "from statsmodels.multivariate.manova import MANOVA\n",
        "from collections import Counter\n",
        "%matplotlib inline\n",
        "print('Environment Ready')"
      ],
      "metadata": {
        "id": "SkOYUXVGtj34"
      },
      "id": "SkOYUXVGtj34",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import VOTE_DF file here for analysis of features\n",
        "(Looked at feature interactions as well, but opted to keep simple as few improved modeling)"
      ],
      "metadata": {
        "id": "AiJlPNEASeb_"
      },
      "id": "AiJlPNEASeb_"
    },
    {
      "cell_type": "code",
      "source": [
        "VOTE_DF = pd.read_csv('VOTE_DF.csv')\n",
        "# ensure GEOID is an object\n",
        "VOTE_DF['GEOID'] = VOTE_DF['GEOID'].astype(str)\n",
        "\n",
        "# Inspect\n",
        "#print(VOTE_DF.info())"
      ],
      "metadata": {
        "id": "-3Mwukkuy-ex"
      },
      "id": "-3Mwukkuy-ex",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Variance check"
      ],
      "metadata": {
        "id": "fzazFZcGgG69"
      },
      "id": "fzazFZcGgG69"
    },
    {
      "cell_type": "code",
      "source": [
        "# Select numerical columns\n",
        "VOTE_num = VOTE_DF.select_dtypes(include=np.number)\n",
        "\n",
        "variances = VOTE_num.var()\n",
        "\n",
        "# Sort variances in descending order\n",
        "var_sorted = variances.sort_values(ascending=True)\n",
        "\n",
        "# Set pandas display option to show float format\n",
        "pd.set_option('display.float_format', '{:.2f}'.format)\n",
        "\n",
        "# Confirm (Consider dropping features with low variance >0.05)\n",
        "print('\\nFeature Variances (sorted):')\n",
        "print(var_sorted.head(20))"
      ],
      "metadata": {
        "collapsed": true,
        "id": "-MYZcsF3-A2l"
      },
      "execution_count": null,
      "outputs": [],
      "id": "-MYZcsF3-A2l"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0ddeafb"
      },
      "source": [
        "## Compute VIF for VOTE_DF"
      ],
      "id": "b0ddeafb"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "98e045b4"
      },
      "source": [
        "# Remove independent variables\n",
        "VOTE_features = VOTE_num.drop(columns=['PARTY_WIN', 'PARTY_LEAD'])\n",
        "\n",
        "# Add required constant\n",
        "VOTE_features = sm.add_constant(VOTE_features)\n",
        "\n",
        "# Compute Variance Inflation Factor for each feature\n",
        "VOTE_VIF = pd.DataFrame()\n",
        "VOTE_VIF['Feature'] = VOTE_features.columns\n",
        "# Compute VIF, handling potential inf values which occur with perfect multicollinearity\n",
        "VOTE_VIF['VIF'] = [variance_inflation_factor(VOTE_features.values, i) for i in range(VOTE_features.shape[1])]\n",
        "\n",
        "# Sort by VIF in descending order for easier analysis\n",
        "VOTE_VIF = VOTE_VIF.sort_values(by='VIF', ascending=False)\n",
        "\n",
        "# Set display to show float format\n",
        "pd.set_option('display.float_format', '{:.2f}'.format)\n",
        "\n",
        "print('VIF for VOTE_DF:')\n",
        "display(VOTE_VIF)"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "98e045b4"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Correlations"
      ],
      "metadata": {
        "id": "x0ZNe24WqY2G"
      },
      "id": "x0ZNe24WqY2G"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "946999ad"
      },
      "source": [
        "## Pearson Correlation Matrix"
      ],
      "id": "946999ad"
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "20233d7d"
      },
      "source": [
        "# Compute Pearson correlation matrix\n",
        "pearson_corr_matrix = VOTE_DF.corr(method='pearson')\n",
        "\n",
        "# Display the correlations\n",
        "print('Pearson Correlation Matrix:')\n",
        "display(pearson_corr_matrix)\n",
        "\n",
        "# Sort Pearson correlations with PARTY_WIN and PARTY_LEAD\n",
        "pearson_corr_win = pearson_corr_matrix['PARTY_WIN'].sort_values(ascending=False)\n",
        "pearson_corr_lead = pearson_corr_matrix['PARTY_LEAD'].sort_values(ascending=False)"
      ],
      "id": "20233d7d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2bf3b66a"
      },
      "source": [
        "## Spearman Correlation Matrix"
      ],
      "id": "2bf3b66a"
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "a6cc88d6"
      },
      "source": [
        "# Compute Spearman correlation matrix\n",
        "spearman_corr_matrix = VOTE_DF.corr(method='spearman')\n",
        "\n",
        "# Display the correlations\n",
        "print('\\nSpearman Correlation Matrix:')\n",
        "display(spearman_corr_matrix)\n",
        "\n",
        "# Sort and store Spearman correlation results\n",
        "spearman_corr_win = spearman_corr_matrix['PARTY_WIN'].sort_values(ascending=False)\n",
        "spearman_corr_lead = spearman_corr_matrix['PARTY_LEAD'].sort_values(ascending=False)"
      ],
      "id": "a6cc88d6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chatterjee's Correlation  \n",
        "In 2020, a paper titled 'A New Coefficient of Correlation' introduced a new coefficient measure ξ (“Xi”) which measures how much the dependent variable is a function of the independent. The result equals 0 if the two variables are independent and will be closer to 1 as the relationship strengthens. Also includes some theoretical properties that allow for hypothesis testing prior to making assumptions about the data.  \n",
        "\n",
        "Along with the article, the R package 'XICOR' was released which contains the function xicor() which calculates ξ when X and Y vectors or matrices are provided (provides p-values for hypothesis testing).\n",
        "\n",
        "S. Chatterjee, *A New Coefficient of Correlation* (2020), Journal of the American Statistical Association.\n",
        "https://doi.org/10.48550/arXiv.1909.10140\n",
        "\n",
        "The below code is a python xicor function based on one written by Tim Sumner https://medium.com/data-science/a-new-coefficient-of-correlation-64ae4f260310"
      ],
      "metadata": {
        "id": "UyZcNq2_fayo"
      },
      "id": "UyZcNq2_fayo"
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute Chatterjee's Correlation\n",
        "def xicor(X, Y, ties='auto', return_p=True):\n",
        "    np.random.seed(1)\n",
        "    X = np.asarray(X)\n",
        "    Y = np.asarray(Y)\n",
        "    Y_sorted = Y[np.argsort(X)]\n",
        "    n = len(X)\n",
        "\n",
        "    if ties == 'auto':\n",
        "        ties = len(np.unique(Y)) < n\n",
        "\n",
        "    if ties:\n",
        "        r = rankdata(Y_sorted, method='ordinal')\n",
        "        l = rankdata(Y_sorted, method='max')\n",
        "        xi = 1 - n * np.sum(np.abs(np.diff(r))) / (2 * np.sum(l * (n - l)))\n",
        "    else:\n",
        "        r = rankdata(Y_sorted, method='ordinal')\n",
        "        xi = 1 - 3 * np.sum(np.abs(np.diff(r))) / (n**2 - 1)\n",
        "\n",
        "# p-value approximation\n",
        "    p_value = norm.sf(xi, scale=2/5/np.sqrt(n))\n",
        "\n",
        "    if return_p:\n",
        "        return xi, p_value\n",
        "    else:\n",
        "        return xi\n",
        "\n",
        "# Define the independent and dependent variables\n",
        "features = [col for col in VOTE_DF.columns if col not in [\n",
        "    'PARTY_WIN', 'PARTY_LEAD',\n",
        "    'Male_total', 'Female_total']]\n",
        "\n",
        "target_win = VOTE_DF['PARTY_WIN']\n",
        "target_lead = VOTE_DF['PARTY_LEAD']\n",
        "\n",
        "# Store xicor results\n",
        "xicor_results_win = {}\n",
        "xicor_results_lead = {}\n",
        "\n",
        "# Compute xicor for each feature against PARTY_WIN\n",
        "for feature in features:\n",
        "    x_data = VOTE_DF[feature]\n",
        "    xi_stat, xi_p_value = xicor(x_data, target_win)\n",
        "    xicor_results_win[feature] = {'statistic': xi_stat, 'p_value': xi_p_value}\n",
        "    #print(f'{feature}: Statistic={xi_stat:.2f}, P-value={xi_p_value:.2f}')\n",
        "\n",
        "# Compute xicor for each feature against PARTY_LEAD\n",
        "for feature in features:\n",
        "    x_data = VOTE_DF[feature]\n",
        "    xi_stat, xi_p_value = xicor(x_data, target_lead)\n",
        "    xicor_results_lead[feature] = {'statistic': xi_stat, 'p_value': xi_p_value}\n",
        "    #print(f'{feature}: Statistic={xi_stat:.2f}, P-value={xi_p_value:.2f}')\n",
        "\n",
        "# Store Chatterjee correlation results\n",
        "xi_corr_win = pd.DataFrame.from_dict(xicor_results_win, orient='index')\n",
        "xi_corr_lead = pd.DataFrame.from_dict(xicor_results_lead, orient='index')"
      ],
      "metadata": {
        "id": "jN_NUj06zQc1"
      },
      "id": "jN_NUj06zQc1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Compare Correlation Coefficients"
      ],
      "metadata": {
        "id": "GWZ-52yYuVVY"
      },
      "id": "GWZ-52yYuVVY"
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine all correlation results into a single DataFrame\n",
        "correlation_comparison = pd.concat([\n",
        "    xi_corr_lead['statistic'].rename('Xi_Corr_LEAD'),\n",
        "    xi_corr_win['statistic'].rename('Xi_Corr_WIN'),\n",
        "    pearson_corr_lead,\n",
        "    pearson_corr_win,\n",
        "    spearman_corr_lead,\n",
        "    spearman_corr_win,\n",
        "], axis=1)\n",
        "\n",
        "# Remove the target variables  if included\n",
        "correlation_comparison.drop(['PARTY_WIN', 'PARTY_LEAD'], errors='ignore', inplace=True)\n",
        "\n",
        "# Rename features\n",
        "Correlation_Table = correlation_comparison.rename(columns={\n",
        "    'Pearson_Corr_PARTY_LEAD': 'Pearson_LEAD',\n",
        "    'Pearson_Corr_PARTY_WIN': 'Pearson_WIN',\n",
        "    'Spearman_Corr_PARTY_LEAD': 'Spearman_LEAD',\n",
        "    'Spearman_Corr_PARTY_WIN': 'Spearman_WIN'})\n",
        "\n",
        "# Display all correlations\n",
        "print('Comparison of Xi, Pearson, and Spearman Correlations:')\n",
        "display(Correlation_Table.round(4).sort_values(by='Xi_Corr_LEAD', ascending=False))"
      ],
      "metadata": {
        "id": "TeAT4snMuSuQ"
      },
      "id": "TeAT4snMuSuQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6d525c65"
      },
      "source": [
        "# Statistical test (Test for normality first)  \n"
      ],
      "id": "6d525c65"
    },
    {
      "cell_type": "code",
      "source": [
        "# Separate the dataframe into two groups based on PARTY_WIN\n",
        "group_Republican = VOTE_num[VOTE_num['PARTY_WIN'] == 0]\n",
        "group_Democrat = VOTE_num[VOTE_num['PARTY_WIN'] == 1]\n",
        "\n",
        "features_for_norm = VOTE_num.columns.tolist()\n",
        "features_for_norm.remove('PARTY_WIN')\n",
        "\n",
        "normality_results = {}\n",
        "\n",
        "for feature in features_for_norm:\n",
        "    data1 = group_Republican[feature]\n",
        "    data2 = group_Democrat[feature]\n",
        "\n",
        "    if len(data1) > 2 and len(data2) > 2:\n",
        "        stat1, p_norm1 = shapiro(data1)\n",
        "        stat2, p_norm2 = shapiro(data2)\n",
        "\n",
        "        normality_results[feature] = {\n",
        "            'Rep_p': f'{p_norm1:.2f}',\n",
        "            'Dem_p': f'{p_norm2:.2f}'}\n",
        "    else:\n",
        "        normality_results[feature] = {\n",
        "            'Rep_p': None,\n",
        "            'Dem_p': None}\n",
        "\n",
        "# Convert to DataFrame\n",
        "normality_df = pd.DataFrame(normality_results).T\n",
        "\n",
        "# Confirm (Normality will be defined as above a threshhold of 0.05)\n",
        "print(normality_df)"
      ],
      "metadata": {
        "id": "PB0iFHm4Gbl4"
      },
      "id": "PB0iFHm4Gbl4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Almost every feature is way below 0.05 in both groups: normality is violated with one exception: Will not use T-test.\n",
        "\n",
        "## Run Mann-Whitney U Test"
      ],
      "metadata": {
        "id": "hxMHyq_zIdAs"
      },
      "id": "hxMHyq_zIdAs"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0988c1d4"
      },
      "source": [
        "mannwhit_results = []\n",
        "\n",
        "for feature in features_for_norm:\n",
        "    if feature == 'PARTY_LEAD':\n",
        "        continue\n",
        "\n",
        "    data1 = group_Republican[feature]\n",
        "    data2 = group_Democrat[feature]\n",
        "\n",
        "    if len(data1) < 2 or len(data2) < 2:\n",
        "        continue\n",
        "\n",
        "    U_stat, p_value = mannwhitneyu(data1, data2, alternative='two-sided')\n",
        "\n",
        "    if p_value < 0.05:\n",
        "        mannwhit_results.append({\n",
        "            'Feature': feature,\n",
        "            'DEM_median': data2.median(),\n",
        "            'REP_median': data1.median(),\n",
        "            'U_stat': U_stat,\n",
        "            'p_value': p_value,\n",
        "            'n_dem': len(data2),\n",
        "            'n_rep': len(data1)})\n",
        "\n",
        "mannwhit_df = pd.DataFrame(mannwhit_results)\n",
        "\n",
        "# Derive additional stats\n",
        "mannwhit_df['diff_median'] = mannwhit_df['DEM_median'] - mannwhit_df['REP_median']\n",
        "\n",
        "mannwhit_df['R_biserial'] = 1 - (2 * mannwhit_df['U_stat'] / (\n",
        "                            mannwhit_df['n_dem'] * mannwhit_df['n_rep']))\n",
        "\n",
        "mannwhit_df['Cohens_d'] = (2 * mannwhit_df['R_biserial']\n",
        "                          ) / np.sqrt(1 - mannwhit_df['R_biserial']**2)\n",
        "\n",
        "# Add qualitative labels\n",
        "def label_effect_size(d):\n",
        "    d = abs(d)\n",
        "    if d < 0.2:\n",
        "        return 'Negligible'\n",
        "    elif d < 0.5:\n",
        "        return 'Small'\n",
        "    elif d < 0.8:\n",
        "        return 'Medium'\n",
        "    else:\n",
        "        return 'Large'\n",
        "\n",
        "mannwhit_df['Effect_size'] = mannwhit_df['Cohens_d'].astype(float).apply(label_effect_size)\n",
        "\n",
        "# Reorder columns for priority in table (consider dropping n_ features)\n",
        "cols = mannwhit_df.columns.tolist()\n",
        "cols.insert(3, cols.pop(cols.index('diff_median')))\n",
        "cols.insert(5, cols.pop(cols.index('Cohens_d')))\n",
        "cols.insert(6, cols.pop(cols.index('Effect_size')))\n",
        "cols.insert(7, cols.pop(cols.index('R_biserial')))\n",
        "mannwhit_df = mannwhit_df[cols]\n",
        "\n",
        "# Format after sorting\n",
        "mannwhit_df['DEM_median'] = mannwhit_df['DEM_median'].map(lambda x: f'{x:.2f}')\n",
        "mannwhit_df['REP_median'] = mannwhit_df['REP_median'].map(lambda x: f'{x:.2f}')\n",
        "mannwhit_df['diff_median'] = mannwhit_df['diff_median'].map(lambda x: f'{x:.2f}')\n",
        "mannwhit_df['Cohens_d'] = mannwhit_df['Cohens_d'].map(lambda x: f'{x:.2f}')\n",
        "mannwhit_df['R_biserial'] = mannwhit_df['R_biserial'].map(lambda x: f'{x:.2f}')\n",
        "mannwhit_df['p_value'] = mannwhit_df['p_value'].map(lambda x: f'{x:.2f}')\n",
        "\n",
        "# Confirm\n",
        "display(mannwhit_df.sort_values(by='Cohens_d', ascending=False))"
      ],
      "id": "0988c1d4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature importance"
      ],
      "metadata": {
        "id": "s0z6-6WJqrFL"
      },
      "id": "s0z6-6WJqrFL"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4a180b4"
      },
      "source": [
        "## Feature Importance for PARTY_WIN from Logistic Regression"
      ],
      "id": "e4a180b4"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "072737b3",
        "collapsed": true
      },
      "source": [
        "# Define the features to exclude based on p-values\n",
        "# Could drop Same_Sex features, but not ready to drop yet\n",
        "features_to_exclude = ['']\n",
        "\n",
        "# Select features for logistic regression, excluding the specified ones\n",
        "features_for_logit = [col for col in VOTE_DF.columns if col not in features_to_exclude + ['GEOID', 'Male_total', 'Female_total', 'PARTY_WIN', 'PARTY_LEAD']]\n",
        "\n",
        "X0 = VOTE_DF[features_for_logit]\n",
        "y0 = VOTE_DF['PARTY_WIN']\n",
        "\n",
        "# Split data into training and testing sets (recommended for model evaluation)\n",
        "X0_train, X0_test, y0_train, y0_test = train_test_split(\n",
        "    X0, y0, test_size=0.2, random_state=1,\n",
        "    stratify=y0) # To maintain class distribution\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X0_train_scaled = scaler.fit_transform(X0_train)\n",
        "X0_test_scaled = scaler.transform(X0_test)\n",
        "\n",
        "# Initialize and train the Logistic Regression model with regularization\n",
        "# Using default L2 penalty and balanced class weight\n",
        "logit_model_sklearn = LogisticRegression(\n",
        "    random_state=1,\n",
        "    class_weight='balanced',\n",
        "    max_iter=1000) # Increased max_iter for convergence\n",
        "logit_model_sklearn.fit(X0_train_scaled, y0_train)\n",
        "\n",
        "# Confirm feature importances from the trained model (coefficients)\n",
        "print('Feature Importance (Coefficients from Regularized Logistic Regression):')\n",
        "logit_feature_importance = pd.Series(\n",
        "    logit_model_sklearn.coef_[0], index=features_for_logit)\n",
        "print(logit_feature_importance.sort_values(ascending=False))\n",
        "\n",
        "# Plot feature importances\n",
        "plt.figure(figsize=(10, 8)) # Adjusted figure size for better readability\n",
        "logit_feature_importance.sort_values().plot(kind='barh')\n",
        "plt.title('Feature Importance from Logistic Regression(WIN)')\n",
        "plt.xlabel('Coefficient Value')\n",
        "plt.ylabel('Feature')\n",
        "plt.show()\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "y0_pred_logit = logit_model_sklearn.predict(X0_test_scaled)\n",
        "\n",
        "print('\\nLogistic Regression Model Evaluation (on test set):')\n",
        "print(f'Accuracy: {accuracy_score(y0_test, y0_pred_logit):.2f}')\n",
        "print('Classification Report:')\n",
        "print(classification_report(y0_test, y0_pred_logit))\n",
        "print('Confusion Matrix:')\n",
        "print(confusion_matrix(y0_test, y0_pred_logit))"
      ],
      "id": "072737b3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature Importance for PARTY_WIN from Decision Tree Classifier"
      ],
      "metadata": {
        "id": "vay-KdHNsueQ"
      },
      "id": "vay-KdHNsueQ"
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the features (X) and the target variable (y)\n",
        "# Exclude the target variables themselves from the features\n",
        "features_for_dtc = [col for col in VOTE_DF.columns if col not in [\n",
        "    'GEOID', 'Male_total', 'Female_total', 'PARTY_WIN', 'PARTY_LEAD']]\n",
        "\n",
        "# Define the features (X) and the target variable (y)\n",
        "X1 = VOTE_DF[features_for_dtc]\n",
        "y1 = VOTE_DF['PARTY_WIN']\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X1_train, X1_test, y1_train, y1_test = train_test_split(\n",
        "    X1, y1, test_size=0.2, random_state=1)\n",
        "\n",
        "# Initialize and train the Decision Tree Regressor model\n",
        "dtc_model = DecisionTreeClassifier(\n",
        "    random_state=1,\n",
        "    max_depth=5,\n",
        "    min_samples_split=20,\n",
        "    min_samples_leaf=10)\n",
        "\n",
        "dtc_model.fit(X1_train, y1_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y1_pred_dtc = dtc_model.predict(X1_test)\n",
        "\n",
        "# Evaluate the model\n",
        "mse = mean_squared_error(y1_test, y1_pred_dtc)\n",
        "rmse = np.sqrt(mse) # Calculate RMSE manually\n",
        "mae = mean_absolute_error(y1_test, y1_pred_dtc)\n",
        "r2 = r2_score(y1_test, y1_pred_dtc)\n",
        "\n",
        "print('Decision Tree Regressor Model Evaluation (on test set):')\n",
        "print(f'Mean Squared Error (MSE): {mse:.2f}')\n",
        "print(f'Root Mean Squared Error (RMSE): {rmse:.2f}')\n",
        "print(f'Mean Absolute Error (MAE): {mae:.2f}')\n",
        "print(f'R-squared (R2): {r2:.2f}')\n",
        "\n",
        "# Plot feature importances from the trained model\n",
        "print('\\nFeature Importance from Decision Tree Regressor:')\n",
        "dtc_feature_importance = pd.Series(dtc_model.feature_importances_, index=features_for_dtc)\n",
        "\n",
        "# Sort and print feature importances\n",
        "print(dtc_feature_importance.sort_values(ascending=False).head(15))\n",
        "\n",
        "# Plot feature importances\n",
        "plt.figure(figsize=(10, 8)) # Adjusted figure size\n",
        "dtc_feature_importance.sort_values().plot(kind='barh')\n",
        "plt.title('Feature Importance from Decision Tree Regressor(WIN)')\n",
        "plt.xlabel('Importance Score')\n",
        "plt.ylabel('Feature')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "JmMVTcvCXKfp",
        "collapsed": true
      },
      "id": "JmMVTcvCXKfp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature Importance for PARTY_LEAD from Decision Tree Regressor"
      ],
      "metadata": {
        "id": "mrMYzNbQXDr7"
      },
      "id": "mrMYzNbQXDr7"
    },
    {
      "cell_type": "code",
      "source": [
        "X2 = VOTE_DF[features_for_dtc]\n",
        "y2 = VOTE_DF['PARTY_LEAD']\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X2_train, X2_test, y2_train, y2_test = train_test_split(\n",
        "    X2, y2, test_size=0.2, random_state=1)\n",
        "\n",
        "# Initialize and train the Decision Tree Regressor model\n",
        "dtr_model = DecisionTreeRegressor(\n",
        "    random_state=1,\n",
        "    max_depth=5,\n",
        "    min_samples_split=20,\n",
        "    min_samples_leaf=10)\n",
        "\n",
        "dtr_model.fit(X2_train, y2_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y2_pred_dtr = dtr_model.predict(X2_test)\n",
        "\n",
        "# Evaluate the model\n",
        "mse = mean_squared_error(y2_test, y2_pred_dtr)\n",
        "rmse = np.sqrt(mse) # Calculate RMSE manually\n",
        "mae = mean_absolute_error(y2_test, y2_pred_dtr)\n",
        "r2 = r2_score(y2_test, y2_pred_dtr)\n",
        "\n",
        "print('Decision Tree Regressor Model Evaluation (on test set):')\n",
        "print(f'Mean Squared Error (MSE): {mse:.2f}')\n",
        "print(f'Root Mean Squared Error (RMSE): {rmse:.2f}')\n",
        "print(f'Mean Absolute Error (MAE): {mae:.2f}')\n",
        "print(f'R-squared (R2): {r2:.2f}')\n",
        "\n",
        "# Get and plot feature importances from the trained model\n",
        "print('\\nFeature Importance from Decision Tree Regressor:')\n",
        "dtr_feature_importance = pd.Series(dtr_model.feature_importances_, index=features_for_dtc)\n",
        "\n",
        "# Sort and print feature importances\n",
        "print(dtr_feature_importance.sort_values(ascending=False).head(20))\n",
        "\n",
        "# Plot feature importances\n",
        "plt.figure(figsize=(10, 8)) # Adjusted figure size\n",
        "dtr_feature_importance.sort_values().plot(kind='barh')\n",
        "plt.title('Feature Importance from Decision Tree Regressor(LEAD)')\n",
        "plt.xlabel('Importance Score')\n",
        "plt.ylabel('Feature')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "UB8RDvm8s4KO",
        "collapsed": true
      },
      "id": "UB8RDvm8s4KO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2c30e6de"
      },
      "source": [
        "## Feature Importance for PARTY_WIN from Random Forest"
      ],
      "id": "2c30e6de"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4469fad4"
      },
      "source": [
        "# Select features for the Random Forest model\n",
        "# We can use the same set of features that worked for the logistic regression.\n",
        "features_for_rf = features_for_logit\n",
        "\n",
        "X3 = VOTE_DF[features_for_rf]\n",
        "y3 = VOTE_DF['PARTY_WIN']\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X3_train, X3_test, y3_train, y3_test = train_test_split(\n",
        "    X3, y3, test_size=0.2, random_state=1, stratify=y3)\n",
        "\n",
        "# Initialize and train the Random Forest Classifier\n",
        "# Use a reasonable number of estimators (n_estimators) and a random state for reproducibility\n",
        "rf_model = RandomForestClassifier(\n",
        "    n_estimators=300,\n",
        "    max_depth=None,        # control/avoid overfitting\n",
        "    min_samples_split=10,  # avoid tiny splits\n",
        "    min_samples_leaf=5,    # smoother trees\n",
        "    random_state=1,\n",
        "    class_weight='balanced')\n",
        "rf_model.fit(X3_train, y3_train)\n",
        "\n",
        "# Get feature importances from the trained model\n",
        "rf_feature_importance = pd.Series(\n",
        "    rf_model.feature_importances_, index=features_for_rf)\n",
        "\n",
        "# Sort and print feature importances\n",
        "print('Feature Importance from Random Forest:')\n",
        "print(rf_feature_importance.sort_values(ascending=False))\n",
        "\n",
        "# Plot feature importances\n",
        "plt.figure(figsize=(10, 6))\n",
        "rf_feature_importance.sort_values().plot(kind='barh')\n",
        "plt.title('Feature Importance from Random Forest(WIN)')\n",
        "plt.xlabel('Importance Score (Mean Decrease in Impurity)')\n",
        "plt.ylabel('Feature')\n",
        "plt.show()\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "y3_pred_rf = rf_model.predict(X3_test)\n",
        "\n",
        "print('\\nRandom Forest Model Evaluation (on test set):')\n",
        "print(f'Accuracy: {accuracy_score(y3_test, y3_pred_rf):.2f}')\n",
        "print('Classification Report:')\n",
        "print(classification_report(y3_test, y3_pred_rf))\n",
        "print('Confusion Matrix:')\n",
        "print(confusion_matrix(y3_test, y3_pred_rf))"
      ],
      "id": "4469fad4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define permutation importance function (with optional cross-validation)"
      ],
      "metadata": {
        "id": "n7YXZMIne-zS"
      },
      "id": "n7YXZMIne-zS"
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute permutation importance (PI) or cross-validated PI (CV-PI)\n",
        "def get_PI(model, X3, y3, cv=False, n_splits=5, n_repeats=10, random_state=1):\n",
        "    '''\n",
        "    Parameters\n",
        "    ----------\n",
        "    model : estimator\n",
        "        Trained model (must support predict).\n",
        "    X : DataFrame\n",
        "        Features used for prediction.\n",
        "    y : Series or array-like\n",
        "        Target values.\n",
        "    cv : bool, default=False\n",
        "        If True, performs cross-validated permutation importance.\n",
        "    n_splits : int, default=5\n",
        "        Number of CV folds (only used if cv=True).\n",
        "    n_repeats : int, default=10\n",
        "        Number of shuffles for permutation importance.\n",
        "    random_state : int, default=1\n",
        "        Random seed for reproducibility.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    importance_df : DataFrame\n",
        "        Feature importances sorted by mean decrease in score.\n",
        "    '''\n",
        "\n",
        "    if not cv:\n",
        "# Standard PI on a single fitted model\n",
        "        result = permutation_importance(model, X3, y3,\n",
        "                                        n_repeats=n_repeats,\n",
        "                                        random_state=random_state,\n",
        "                                        n_jobs=-1)\n",
        "        importance_df = pd.DataFrame({\n",
        "            'Feature': X3.columns,\n",
        "            'Importance Mean': result.importances_mean,\n",
        "            'Importance Std': result.importances_std\n",
        "        }).sort_values(by='Importance Mean', ascending=False)\n",
        "\n",
        "    else:\n",
        "# Cross-validated PI\n",
        "        skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
        "        importances = []\n",
        "\n",
        "        for train_idx, test_idx in skf.split(X3, y3):\n",
        "            X3_train, X3_test = X3.iloc[train_idx], X3.iloc[test_idx]\n",
        "            y3_train, y3_test = y3.iloc[train_idx], y3.iloc[test_idx]\n",
        "\n",
        "            model.fit(X3_train, y3_train)\n",
        "            result = permutation_importance(model, X3_test, y3_test,\n",
        "                                            n_repeats=n_repeats,\n",
        "                                            random_state=random_state,\n",
        "                                            n_jobs=-1)\n",
        "            importances.append(result.importances_mean)\n",
        "\n",
        "        mean_importances = np.mean(importances, axis=0)\n",
        "        std_importances = np.std(importances, axis=0)\n",
        "\n",
        "        importance_df = pd.DataFrame({\n",
        "            'Feature': X3.columns,\n",
        "            'Importance Mean': mean_importances,\n",
        "            'Importance Std': std_importances\n",
        "        }).sort_values(by='Importance Mean', ascending=False)\n",
        "    return importance_df\n",
        "\n",
        "# Confirm\n",
        "pd.set_option('display.max_rows', None)\n",
        "pd.reset_option('display.float_format')\n",
        "\n",
        "RF_PI = get_PI(rf_model, X3_test, y3_test, cv=False)\n",
        "print(RF_PI)\n",
        "\n",
        "# Plot permutation importances\n",
        "plt.figure(figsize=(10, 6))\n",
        "# Convert 'Importance Mean' to numeric before plotting\n",
        "RF_PI['Importance Mean'] = pd.to_numeric(RF_PI['Importance Mean'])\n",
        "RF_PI.sort_values(by='Importance Mean', ascending=True).plot(kind='barh')\n",
        "plt.title('Permutation Importance from Random Forest(WIN)')\n",
        "plt.xlabel('Importance Score (Importance decrease in Mean)')\n",
        "plt.ylabel('Feature')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "tX3KCh5yfLWn"
      },
      "id": "tX3KCh5yfLWn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Run RFECV with Random Forest to confirm best features"
      ],
      "metadata": {
        "id": "Wd1SSWaExjpc"
      },
      "id": "Wd1SSWaExjpc"
    },
    {
      "cell_type": "code",
      "source": [
        "# Utillize X, y, train, test from Logit (X0, y0)\n",
        "# RFECV with Random Forest\n",
        "rf = RandomForestClassifier(n_estimators=500, random_state=1, class_weight='balanced')\n",
        "\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n",
        "selector = RFECV(estimator=rf, step=1, cv=cv, scoring='accuracy', n_jobs=-1)\n",
        "selector.fit(X0_train, y0_train)\n",
        "\n",
        "# Best features\n",
        "best_features = X0.columns[selector.support_].tolist()\n",
        "print('Best feature subset:')\n",
        "print(best_features)\n",
        "\n",
        "# Retrain final model with best features\n",
        "rf_ECV = RandomForestClassifier(n_estimators=500, random_state=1, class_weight='balanced')\n",
        "rf_ECV.fit(X0_train[best_features], y0_train)\n",
        "y_pred_ECV = rf_ECV.predict(X0_test[best_features])\n",
        "\n",
        "print('\\nFinal Model Evaluation with Best Features:')\n",
        "print(f'Accuracy: {accuracy_score(y0_test, y_pred_ECV):.4f}')\n",
        "print('Classification Report:')\n",
        "print(classification_report(y0_test, y_pred_ECV))\n",
        "print('Confusion Matrix:')\n",
        "print(confusion_matrix(y0_test, y_pred_ECV))"
      ],
      "metadata": {
        "id": "ROv9Mv8C7d2_"
      },
      "id": "ROv9Mv8C7d2_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature importance for PARTY_LEAD from Lasso Regression after Cross-validate Alpha"
      ],
      "metadata": {
        "id": "wKZL1KAiwy4W"
      },
      "id": "wKZL1KAiwy4W"
    },
    {
      "cell_type": "code",
      "source": [
        "# Define features and exclude target variables\n",
        "features_for_lasso_cv = [col for col in VOTE_DF.columns if col not in ['GEOID', 'PARTY_WIN', 'PARTY_LEAD']]\n",
        "X4 = VOTE_DF[features_for_lasso_cv]\n",
        "y4 = VOTE_DF['PARTY_LEAD']\n",
        "\n",
        "# Split into train and test sets\n",
        "X4_train, X4_test, y4_train, y4_test = train_test_split(\n",
        "    X4, y4, test_size=0.2, random_state=1)\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X4_train_scaled = scaler.fit_transform(X4_train)\n",
        "X4_test_scaled = scaler.transform(X4_test)\n",
        "\n",
        "# Let LassoCV automatically generates an alpha grid to test\n",
        "lasso_cv_model = LassoCV(cv=5, random_state=1, max_iter=10000)\n",
        "lasso_cv_model.fit(X4_train_scaled, y4_train)\n",
        "\n",
        "# Confirm the optimal alpha found by LassoCV\n",
        "optimal_alpha = lasso_cv_model.alpha_\n",
        "print(f'Optimal alpha found by LassoCV: {optimal_alpha:.4f}')\n",
        "print('')\n",
        "# Plot the MSE as a function of alpha\n",
        "mse_path = lasso_cv_model.mse_path_\n",
        "alphas = lasso_cv_model.alphas_\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(alphas, mse_path, linestyle='-', marker='o')\n",
        "plt.xscale('log') # Often useful to plot alpha on a log scale\n",
        "plt.xlabel('Alpha')\n",
        "plt.ylabel('Mean Squared Error (across folds)')\n",
        "plt.title('Mean Squared Error vs. Alpha during Cross-validation')\n",
        "plt.axvline(optimal_alpha, color='red', linestyle='--', label=f'Optimal Alpha = {optimal_alpha:.4f}')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# List the coefficients with the optimal alpha\n",
        "print('\\nFeature Importance from LassoCV (CV = 0.0002):')\n",
        "feature_importance_lasso_cv = pd.Series(lasso_cv_model.coef_, index=features_for_lasso_cv)\n",
        "print(feature_importance_lasso_cv.sort_values(ascending=False))\n",
        "\n",
        "# Plot feature importances with optimal alpha\n",
        "plt.figure(figsize=(10, 10))\n",
        "feature_importance_lasso_cv.sort_values().plot(kind='barh')\n",
        "plt.title(f'Feature Importance from Lasso Regression(LEAD) with Optimal Alpha = {optimal_alpha:.4f}')\n",
        "plt.xlabel('Coefficient Value')\n",
        "plt.ylabel('Feature')\n",
        "plt.show()\n",
        "\n",
        "# Evaluate the final Lasso model with the optimal alpha on the test set\n",
        "y4_pred_lasso_cv = lasso_cv_model.predict(X4_test_scaled)\n",
        "\n",
        "print('\\nLasso Regression Model Evaluation (with Optimal Alpha):')\n",
        "mse_test = mean_squared_error(y4_test, y4_pred_lasso_cv)\n",
        "rmse_test = np.sqrt(mse_test)\n",
        "r2_test = r2_score(y4_test, y4_pred_lasso_cv)\n",
        "\n",
        "print(f'Mean Squared Error (MSE) on test set: {mse_test:.4f}')\n",
        "print(f'Root Mean Squared Error (RMSE) on test set: {rmse_test:.4f}')\n",
        "print(f'R-squared (R2) on test set: {r2_test:.4f}')"
      ],
      "metadata": {
        "id": "is2-5D_DQUxz",
        "collapsed": true
      },
      "id": "is2-5D_DQUxz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Compare feature importance"
      ],
      "metadata": {
        "id": "zP3rnW8z6VkD"
      },
      "id": "zP3rnW8z6VkD"
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_series(s, keep_sign=True):\n",
        "    '''Normalize feature importance to 0–1 scale, optionally keeping sign.'''\n",
        "    s = s.fillna(0)\n",
        "    if keep_sign:\n",
        "        return s / s.abs().max()  # scale to -1..1, preserving sign\n",
        "    else:\n",
        "        scaler = MinMaxScaler()\n",
        "        return pd.Series(scaler.fit_transform(s.values.reshape(-1, 1)).flatten(), index=s.index)\n",
        "\n",
        "# Collect raw importances into a DataFrame\n",
        "feature_importances = pd.DataFrame({\n",
        "    'LogReg': logit_feature_importance,\n",
        "    'DecTreeClass': dtc_feature_importance,\n",
        "    'DecTreeReg': dtr_feature_importance,\n",
        "    'RandomForest': rf_feature_importance,\n",
        "    'RF_PI': RF_PI.set_index('Feature')['Importance Mean'],  # permutation importance\n",
        "    'Lasso_LogReg': feature_importance_lasso_cv})\n",
        "\n",
        "# Normalize each column (preserving signs)\n",
        "for col in feature_importances.columns:\n",
        "    if col in ['LogReg', 'Lasso_LogReg']:  # signed coefficients\n",
        "        feature_importances[col] = normalize_series(feature_importances[col], keep_sign=True)\n",
        "    else:  # tree-based importances are ≥ 0\n",
        "        feature_importances[col] = normalize_series(feature_importances[col], keep_sign=False)\n",
        "\n",
        "# Compute mean rank or average importance across models\n",
        "feature_importances['Avg_Importance'] = feature_importances.abs().mean(axis=1)\n",
        "\n",
        "# Sort by average importance\n",
        "feature_importances = feature_importances.sort_values(by='Avg_Importance', ascending=False)\n",
        "\n",
        "# Print the top features\n",
        "print('\\nTop 25 Features Across Models (normalized):')\n",
        "display(feature_importances.round(4))"
      ],
      "metadata": {
        "id": "TPFzdRAd-SB7"
      },
      "id": "TPFzdRAd-SB7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RFECV Feature Selection for final model"
      ],
      "metadata": {
        "id": "eBwGovenQL60"
      },
      "id": "eBwGovenQL60"
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare Data, drop reference and overlap features\n",
        "drop_features = [\n",
        "    'Male_total', 'Female_total', # reference only\n",
        "    '%AGE_MALE_YNG', '%AGE_MALE_MID', '%AGE_MALE_OLD', # overlap\n",
        "    '%AGE_FEMALE_YNG', '%AGE_FEMALE_MID', '%AGE_FEMALE_OLD', # overlap\n",
        "    '%RACE_NonWhite', '%RACE_BAO', '%RACE_LNHM'] # overlap\n",
        "\n",
        "VOTE_FULL = VOTE_DF.drop(\n",
        "    columns=drop_features,\n",
        "    errors='ignore')\n",
        "\n",
        "X = VOTE_FULL.drop(\n",
        "    columns=['GEOID', 'PARTY_WIN', 'PARTY_LEAD'])\n",
        "y = VOTE_FULL['PARTY_WIN']\n",
        "\n",
        "# Train/test split\n",
        "XF_train, XF_test, yF_train, yF_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=1, stratify=y)\n",
        "\n",
        "# Recursive Feature Elimination with Cross-Validation\n",
        "rf = RandomForestClassifier(\n",
        "    n_estimators=400, random_state=1, class_weight='balanced')\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n",
        "selector = RFECV(\n",
        "    estimator=rf, step=1, cv=cv, scoring='accuracy', n_jobs=-1)\n",
        "selector.fit(XF_train, yF_train)\n",
        "\n",
        "# Plot accuracy vs. number of features\n",
        "n_features = np.arange(\n",
        "    1, len(selector.cv_results_['mean_test_score']) + 1)\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.plot(n_features, selector.cv_results_['mean_test_score'], marker='o')\n",
        "plt.axhline(0.93, color='red', linestyle='--', label='93% threshold')\n",
        "plt.axhline(max(selector.cv_results_['mean_test_score']), color='green', linestyle='--', label='Best Acc')\n",
        "plt.xlabel('Number of Features Selected')\n",
        "plt.ylabel('Cross-Validated Accuracy')\n",
        "plt.title('Accuracy vs. Number of Features (RFECV)')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Retrain with Best Features\n",
        "best_features = X.columns[\n",
        "    selector.support_].tolist()\n",
        "print('\\nBest Feature Subset:')\n",
        "print(best_features)\n",
        "\n",
        "rf_final = RandomForestClassifier(\n",
        "    n_estimators=400, random_state=1, class_weight='balanced')\n",
        "rf_final.fit(\n",
        "    XF_train[best_features], yF_train)\n",
        "yF_pred = rf_final.predict(\n",
        "    XF_test[best_features])\n",
        "\n",
        "print('\\nFinal Model Evaluation with Best Features:')\n",
        "print(f'Accuracy: {accuracy_score(yF_test, yF_pred):.4f}')\n",
        "print('Classification Report:')\n",
        "print(classification_report(yF_test, yF_pred))\n",
        "print('Confusion Matrix:')\n",
        "print(confusion_matrix(yF_test, yF_pred))\n",
        "\n",
        "# Plot Feature Importance\n",
        "importances = rf_final.feature_importances_\n",
        "feat_imp = pd.DataFrame({\n",
        "    'Feature': best_features,\n",
        "    'Importance': importances\n",
        "}).sort_values(by='Importance', ascending=False)\n",
        "\n",
        "print('\\nTop Features Driving Model Accuracy:')\n",
        "display(feat_imp)\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.barh(feat_imp['Feature'], feat_imp['Importance'], color='steelblue')\n",
        "plt.gca().invert_yaxis()\n",
        "plt.xlabel('Feature Importance (RF)')\n",
        "plt.title('Key Demographic Predictors of Voting Patterns')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "K-T03hP9CCVo"
      },
      "id": "K-T03hP9CCVo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train FINAL MODEL"
      ],
      "metadata": {
        "id": "oeQkkBXBNWyn"
      },
      "id": "oeQkkBXBNWyn"
    },
    {
      "cell_type": "code",
      "source": [
        "# Select final features: From 'accuracy vs. number of features' plot, ideal number of features start at 8, 9, or 10, compare 8-10 features for accuracy and MANOVA scores (dropped HH_MARRIED due to correlation with REL_xxx_MAR)\n",
        "\n",
        "final_features = [ # Drop '%AGE_FEMALE_YOUNG', '%REL_W_RELATIVES' for best results\n",
        "    'GEOID', 'PARTY_WIN', 'PARTY_LEAD',\n",
        "    '%RACE_White', '%RACE_Asian', '%Urban_pop', '%REL_S_SEX_MAR',\n",
        "    '%REL_OP_SEX_MAR', '%OWN_HOME', '%REL_NON_REL', '%RACE_Black']\n",
        "VOTE_FINAL = VOTE_DF[final_features]\n",
        "\n",
        "# Set features for final model\n",
        "X_final = [col for col in VOTE_FINAL.columns if col not in [\n",
        "    'GEOID', 'PARTY_WIN', 'PARTY_LEAD']]\n",
        "y_final = VOTE_FINAL['PARTY_WIN']\n",
        "\n",
        "# Train/test split\n",
        "XF_train, XF_test, yF_train, yF_test = train_test_split(\n",
        "    VOTE_FINAL[X_final], y_final,\n",
        "    test_size=0.2, random_state=1,\n",
        "    stratify=y_final)\n",
        "\n",
        "# Train Random Forest\n",
        "rf_final = RandomForestClassifier(\n",
        "    n_estimators=500,\n",
        "    random_state=1,\n",
        "    class_weight='balanced')\n",
        "rf_final.fit(XF_train, yF_train)\n",
        "\n",
        "# Evaluate\n",
        "yF_pred = rf_final.predict(XF_test)\n",
        "\n",
        "print('\\nFinal Model Evaluation:')\n",
        "print(f'Accuracy: {accuracy_score(yF_test, yF_pred):.4f}')\n",
        "print('Classification Report:')\n",
        "print(classification_report(yF_test, yF_pred))\n",
        "print('Confusion Matrix:')\n",
        "print(confusion_matrix(yF_test, yF_pred))"
      ],
      "metadata": {
        "id": "isQiPuMBQVry"
      },
      "id": "isQiPuMBQVry",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Use MANOVA to assess whether multiple features jointly differ between Democrats and Republicans"
      ],
      "metadata": {
        "id": "ei1Fc3phWpsy"
      },
      "id": "ei1Fc3phWpsy"
    },
    {
      "cell_type": "code",
      "source": [
        "maov = MANOVA(endog=VOTE_FINAL[X_final], exog=VOTE_FINAL[[y_final.name]])\n",
        "print(maov.mv_test())"
      ],
      "metadata": {
        "id": "ubqRzPOShnav"
      },
      "id": "ubqRzPOShnav",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run CV PI"
      ],
      "metadata": {
        "id": "wVVFLvVyNfIo"
      },
      "id": "wVVFLvVyNfIo"
    },
    {
      "cell_type": "code",
      "source": [
        "# Cross-validate Permutation Importance\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n",
        "importances = []\n",
        "\n",
        "# Pass feature data to skf.split\n",
        "for train_fcv, test_fcv in skf.split(VOTE_FINAL[X_final], y_final):\n",
        "    XF_train, XF_test = VOTE_FINAL[X_final].iloc[train_fcv], VOTE_FINAL[X_final].iloc[test_fcv]\n",
        "    yF_train, yF_test = y_final.iloc[train_fcv], y_final.iloc[test_fcv]\n",
        "\n",
        "    rf_final.fit(XF_train, yF_train)\n",
        "    result = permutation_importance(\n",
        "        rf_final, XF_test, yF_test,\n",
        "        n_repeats=10, random_state=1, n_jobs=-1)\n",
        "    importances.append(result.importances_mean)\n",
        "\n",
        "mean_importances = np.mean(importances, axis=0)\n",
        "std_importances = np.std(importances, axis=0)\n",
        "\n",
        "# Build PI DF\n",
        "pi_df = pd.DataFrame({\n",
        "    'Feature': X_final, # Use X_final for feature names\n",
        "    'Importance Mean': mean_importances.round(4),\n",
        "    'Importance Std': std_importances.round(4)\n",
        "}).sort_values(by='Importance Mean', ascending=False)\n",
        "\n",
        "# Confirm\n",
        "print('\\nCross-validated Permutation Importance:')\n",
        "print(pi_df)\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "pi_df.set_index('Feature')['Importance Mean'].sort_values().plot(kind='barh')\n",
        "plt.title('Final Model - CV Permutation Importance')\n",
        "plt.xlabel('Mean Importance (± CV variation)')\n",
        "plt.ylabel('Feature')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "OzKUEo_CXvz-"
      },
      "id": "OzKUEo_CXvz-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Profile counties with **extremely high PARTY_LEAD** (-30 > LEAD > +30)  \n",
        "\n",
        "Do the demographics of partisan counties match final feature importance?"
      ],
      "metadata": {
        "id": "UW637VgsYaVd"
      },
      "id": "UW637VgsYaVd"
    },
    {
      "cell_type": "code",
      "source": [
        "VOTE_FINAL.to_csv('VOTE_FINAL.csv', index=False)"
      ],
      "metadata": {
        "id": "KcGlrFaJY5nN"
      },
      "id": "KcGlrFaJY5nN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Equal Cutoff Strongholds (±0.5)"
      ],
      "metadata": {
        "id": "OCqURRbheo9y"
      },
      "id": "OCqURRbheo9y"
    },
    {
      "cell_type": "code",
      "source": [
        "def profile_group(dataframe, name, features):\n",
        "    '''Calculates the mean of specified features for a group and returns a Series.'''\n",
        "# Ensure only numeric columns in features are selected for mean calculation\n",
        "    numeric_features = dataframe[features].select_dtypes(include=np.number).columns.tolist()\n",
        "    profile = dataframe[numeric_features].mean()\n",
        "    profile.name = name\n",
        "    return profile\n",
        "\n",
        "# Set cutoff value to 0.50\n",
        "cutoff_val = 0.50\n",
        "\n",
        "# Use _FIN to allow R and D access to all variables\n",
        "extreme_counties = VOTE_FULL[np.abs(VOTE_FULL['PARTY_LEAD']) > cutoff_val]\n",
        "\n",
        "# Republican strongholds\n",
        "extreme_R = extreme_counties[extreme_counties['PARTY_LEAD'] < -cutoff_val]\n",
        "\n",
        "# Democratic strongholds\n",
        "extreme_D = extreme_counties[extreme_counties['PARTY_LEAD'] > cutoff_val]\n",
        "\n",
        "print(f'Republican strongholds (cutoff -{cutoff_val}):', extreme_R.shape[0])\n",
        "print(f'Democratic strongholds (cutoff +{cutoff_val}):', extreme_D.shape[0])\n",
        "\n",
        "# Select demographic features only (drop outcomes and GEOID)\n",
        "demo_features = [col for col in VOTE_FULL.columns if col not in ['PARTY_WIN', 'PARTY_LEAD', 'GEOID']]\n",
        "\n",
        "# Profiles for cutoff-based groups\n",
        "cutoff_profiles_combined = pd.concat([\n",
        "    profile_group(extreme_R, 'R_characteristics', demo_features),\n",
        "    profile_group(extreme_D, 'D_characteristics', demo_features),\n",
        "], axis=1)\n",
        "\n",
        "# Add absolute difference column\n",
        "cutoff_profiles_combined['Abs_Diff'] = np.abs(cutoff_profiles_combined['R_characteristics'] - cutoff_profiles_combined['D_characteristics'])\n",
        "\n",
        "print(f'\\n=== Cutoff-based Stronghold Profiles (>{cutoff_val} Party Lead) ===')\n",
        "print(cutoff_profiles_combined.sort_values(by='Abs_Diff', ascending=False))"
      ],
      "metadata": {
        "id": "DG6o7sPmuJzq"
      },
      "id": "DG6o7sPmuJzq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Balanced Strongholds (Top/Bottom 10% quantiles)"
      ],
      "metadata": {
        "id": "Ez_MMt7k1BmF"
      },
      "id": "Ez_MMt7k1BmF"
    },
    {
      "cell_type": "code",
      "source": [
        "# Set cutoff value\n",
        "lower_10 = VOTE_FULL['PARTY_LEAD'].quantile(0.10)   # bottom 10% cutoff\n",
        "upper_10 = VOTE_FULL['PARTY_LEAD'].quantile(0.90)   # top 10% cutoff\n",
        "\n",
        "R_stnghd_bal = VOTE_FULL[VOTE_FULL['PARTY_LEAD'] <= lower_10].copy()\n",
        "D_stnghd_bal = VOTE_FULL[VOTE_FULL['PARTY_LEAD'] >= upper_10].copy()\n",
        "\n",
        "print(f'Republican strongholds (quantile-based): {len(R_stnghd_bal)} counties (<= {lower_10:.2f})')\n",
        "print(f'Democratic strongholds (quantile-based): {len(D_stnghd_bal)} counties (>= {upper_10:.2f})')\n",
        "\n",
        "# Select demographic features only (drop outcomes and GEOID)\n",
        "demo_features = [col for col in VOTE_FULL.columns if col not in [\n",
        "    'GEOID', 'PARTY_WIN', 'PARTY_LEAD']]\n",
        "\n",
        "# Profiles for quantile-based groups\n",
        "balanced_profiles = pd.concat([\n",
        "    profile_group(R_stnghd_bal, 'R_characteristics', demo_features),\n",
        "    profile_group(D_stnghd_bal, 'D_characteristics', demo_features),\n",
        "], axis=1)\n",
        "\n",
        "# Add absolute difference column\n",
        "balanced_profiles['Abs_Diff'] = np.abs(balanced_profiles['R_characteristics'] - balanced_profiles['D_characteristics'])\n",
        "\n",
        "print('\\n=== Quantile-based Stronghold Profiles (Top/Bottom 10%) ===')\n",
        "print(balanced_profiles.sort_values(by='Abs_Diff', ascending=False))"
      ],
      "metadata": {
        "id": "CYEfEDEYqsmy"
      },
      "id": "CYEfEDEYqsmy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Comparison Table"
      ],
      "metadata": {
        "id": "qPQOzyr65O31"
      },
      "id": "qPQOzyr65O31"
    },
    {
      "cell_type": "code",
      "source": [
        "# Build combined comparison table\n",
        "\n",
        "# Profiles for cutoff-based groups\n",
        "# Select demographic features only (drop outcomes and GEOID)\n",
        "demo_features = [col for col in VOTE_FULL.columns if col not in ['PARTY_WIN', 'PARTY_LEAD', 'GEOID']]\n",
        "\n",
        "cutoff_profiles = pd.concat([\n",
        "    profile_group(extreme_R, 'R_cutoff', demo_features),\n",
        "    profile_group(extreme_D, 'D_cutoff', demo_features)\n",
        "], axis=1)\n",
        "\n",
        "# Profiles for quantile-based groups\n",
        "# Select demographic features only (drop outcomes and GEOID)\n",
        "demo_features = [col for col in VOTE_FULL.columns if col not in ['PARTY_WIN', 'PARTY_LEAD', 'GEOID']]\n",
        "\n",
        "balanced_profiles = pd.concat([\n",
        "    profile_group(R_stnghd_bal, 'R_quantile', demo_features),\n",
        "    profile_group(D_stnghd_bal, 'D_quantile', demo_features)\n",
        "], axis=1)\n",
        "\n",
        "# Combine both into one big table\n",
        "comparison_table = pd.concat([cutoff_profiles, balanced_profiles], axis=1)\n",
        "\n",
        "# Add difference columns (D – R) for clarity of spread\n",
        "comparison_table['Diff_cutoff'] = comparison_table['D_cutoff'] - comparison_table['R_cutoff']\n",
        "comparison_table['Diff_quantile'] = comparison_table['D_quantile'] - comparison_table['R_quantile']\n",
        "\n",
        "comparison_table = comparison_table.round(2)\n",
        "\n",
        "# Confirm\n",
        "print('\\n=== Combined Stronghold Profiles (Cutoff vs Quantile) ===')\n",
        "display(comparison_table.sort_values(by='R_cutoff', ascending=False))"
      ],
      "metadata": {
        "id": "vaz2G7a_5XT_"
      },
      "id": "vaz2G7a_5XT_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Locations of Extremes"
      ],
      "metadata": {
        "id": "HhW9Hq7yevwv"
      },
      "id": "HhW9Hq7yevwv"
    },
    {
      "cell_type": "code",
      "source": [
        "# Pad GEOIDs with a leading zero if length is less than 5 and extract state FIPS\n",
        "R_geoids = [f'{int(geo):05d}' if len(geo) < 5 else geo for geo in extreme_R['GEOID'].unique()]\n",
        "D_geoids = [f'{int(geo):05d}' if len(geo) < 5 else geo for geo in extreme_D['GEOID'].unique()]\n",
        "\n",
        "R_fips = [geo[:2] for geo in R_geoids]\n",
        "D_fips = [geo[:2] for geo in D_geoids]\n",
        "\n",
        "R_counts = Counter(R_fips)\n",
        "D_counts = Counter(D_fips)\n",
        "\n",
        "print('\\nCounts for counties above 50% party lead (Min 75-25% split):')\n",
        "# Sort state_counts by 2-digit state FIPS)\n",
        "sorted_R_counts = dict(sorted(R_counts.items()))\n",
        "sorted_D_counts = dict(sorted(D_counts.items()))\n",
        "print(sorted_R_counts)\n",
        "print(sorted_D_counts)"
      ],
      "metadata": {
        "id": "S282Z_AkaweE"
      },
      "id": "S282Z_AkaweE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Results"
      ],
      "metadata": {
        "id": "kLLzTeuG7I89"
      },
      "id": "kLLzTeuG7I89"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Model Performance & Prediction Quality"
      ],
      "metadata": {
        "id": "grgTS-Nv7eAW"
      },
      "id": "grgTS-Nv7eAW"
    },
    {
      "cell_type": "code",
      "source": [
        "# Predictions: Use the subset of features that the model was trained on\n",
        "yF_pred = rf_final.predict(XF_test)\n",
        "y_pred_proba = rf_final.predict_proba(XF_test)[:, 1] if hasattr(\n",
        "               rf_final, 'predict_proba') else None\n",
        "\n",
        "# Collect metrics\n",
        "performance = {\n",
        "    'Accuracy': accuracy_score(yF_test, yF_pred),\n",
        "    'Precision': precision_score(yF_test, yF_pred, zero_division=0),\n",
        "    'Recall': recall_score(yF_test, yF_pred, zero_division=0),\n",
        "    'F1 Score': f1_score(yF_test, yF_pred, zero_division=0)}\n",
        "\n",
        "if y_pred_proba is not None:\n",
        "    performance['ROC-AUC'] = roc_auc_score(yF_test, y_pred_proba)\n",
        "\n",
        "# Create performance DataFrame\n",
        "perf_df = pd.DataFrame(performance, index=['Final Model']).T\n",
        "display(perf_df)"
      ],
      "metadata": {
        "id": "tqn0PurM7LXr"
      },
      "id": "tqn0PurM7LXr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Feature Importance"
      ],
      "metadata": {
        "id": "k7DxriQ87ahq"
      },
      "id": "k7DxriQ87ahq"
    },
    {
      "cell_type": "code",
      "source": [
        "print('\\nCross-validated Permutation Importance:')\n",
        "print(pi_df)\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "pi_df.set_index('Feature')['Importance Mean'].sort_values().plot(kind='barh')\n",
        "plt.title('Final Model - CV Permutation Importance')\n",
        "plt.xlabel('Mean Importance (± CV variation)')\n",
        "plt.ylabel('Feature')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1ldgozQLupRO"
      },
      "id": "1ldgozQLupRO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Extreme County Locations"
      ],
      "metadata": {
        "id": "FZ5NKr2z7rLv"
      },
      "id": "FZ5NKr2z7rLv"
    },
    {
      "cell_type": "markdown",
      "source": [
        "STRONG REPUBLICAN COUNTIES-----STRONG DEMOCRAT\n",
        "COUNTIES  \n",
        "01: Alabama (23),------------------------------Alabama (2),  \n",
        "04: Arizona (1),  \n",
        "05: Arkansas(34),  \n",
        "06: California (1),------------------------------California (6),  \n",
        "08: Colorado (14),-----------------------------Colorado (4),  \n",
        "11: ----------------------------------------------------D.C. (1),  \n",
        "12: Florida (13),  \n",
        "13: Georgia (40),--------------------------------Georgia (2),  \n",
        "16: Idaho (27),  \n",
        "17: Illinois (26),  \n",
        "18: Indiana (31),  \n",
        "19: Iowa (8),  \n",
        "20: Kansas (67),  \n",
        "21: Kentucky (69),  \n",
        "22: Louisiana (15),-----------------------------Louisiana (1),  \n",
        "24: Maryland (1),--------------------------------Maryland (3),  \n",
        "25: -----------------------------------------------------Massachusetts (2),  \n",
        "26: Michigan (1),   \n",
        "27: Minnesota (1),  \n",
        "28: Mississippi (15),---------------------------Mississippi (4),  \n",
        "29: Missouri (78),-------------------------------Missouri (1),  \n",
        "30: Montana (24),  \n",
        "31: Nebraska (67),  \n",
        "32: New Hampshire (8),  \n",
        "34: -----------------------------------------------------New Jersey (1),  \n",
        "35: New Mexico (3),---------------------------New Mexico (2),  \n",
        "36: -----------------------------------------------------New York (3),  \n",
        "37: North Carolina (12),----------------------North Carolina (2),  \n",
        "38: North Dakota (28),  \n",
        "39: Ohio (26),  \n",
        "40: Oklahoma (59),  \n",
        "41: Oregon (5),------------------------------------Oregon (1),  \n",
        "42: Pennsylvania (11),-------------------------Pennsylvania (1),  \n",
        "45: South Carolina (1),------------------------South Carolina (1),  \n",
        "46: South Dakota (23),------------------------South Dakota (2),  \n",
        "47: Tennessee (61),  \n",
        "48: Texas (160),  \n",
        "49: Utah (17),  \n",
        "50: ------------------------------------------------------Vermont (1),  \n",
        "51: Virginia (17),----------------------------------Virginia (6),  \n",
        "53: ------------------------------------------------------Washington (2),  \n",
        "54: West Virginia (31),  \n",
        "55: -----------------------------------------------------Wisconsin (2),  \n",
        "56: Wyoming (16)"
      ],
      "metadata": {
        "id": "cdHCJPXqvSu_"
      },
      "id": "cdHCJPXqvSu_"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Looking at the distribution of strongholds, the Republican base is both wide and dense, with strong states in the Deep South, the Midwest, and the Plains states—particularly Missouri, Kansas, Nebraska, Oklahoma, and Texas.  \n",
        "Democrats, by contrast, have far fewer strongholds, often limited to isolated urban counties scattered within overwhelmingly Republican states. The most surprising pattern is in California: despite its reputation as a Democratic stronghold at the state level, only six counties emerged as strongly Democratic, compared to a single Republican county. Equally notable are the scattered Democratic enclaves in heavily Republican states such as Mississippi, South Dakota, and Louisiana, showing how local dynamics can carve out exceptions even in states dominated by the opposite party.  \n",
        "These findings highlight that state-level reputation can sometimes mask county-level complexity in partisan alignment."
      ],
      "metadata": {
        "id": "Qk-PHo8JN9It"
      },
      "id": "Qk-PHo8JN9It"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Extreme County Profiles"
      ],
      "metadata": {
        "id": "HGxJDkf37uvY"
      },
      "id": "HGxJDkf37uvY"
    },
    {
      "cell_type": "markdown",
      "source": [
        "The comparison of extreme Republican‐leaning and Democratic‐leaning counties reveals demographic and social patterns that align closely with prior research on U.S. voting behavior.  \n",
        "- Republican counties tend to be in rural areas, majority white, married, older, and more likely to own their homes. There are also somewhat higher shares of older residents living alone.\n",
        "\n",
        "- By contrast, Democratic counties stand out for their racial and ethnic diversity, with substantially higher percentages of Black, Latino, Asian, Native, and mixed‐race residents. They are also much more urbanized, with over 70% of the population living in urban areas compared to under 20% in Republican strongholds, which leads to higher rentals and lower home ownership. Democratic counties show higher shares of unmarried households, non‐relatives living together, and female‐headed households with children. Younger age distributions also feature more prominently, with both male and female populations skewing younger than in Republican counties.\n",
        "\n",
        "Overall, the patterns are not surprising. They mirror well‐documented demographic divides in U.S. elections: rural, older, and predominantly White populations lean Republican, while urban, younger, and racially diverse populations lean Democratic. These results validate the modeling approach, as the county‐level features align strongly with real‐world voting dynamics.\n",
        "\n"
      ],
      "metadata": {
        "id": "Zbx2TmPg8GXH"
      },
      "id": "Zbx2TmPg8GXH"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Future Steps and Broader Insights"
      ],
      "metadata": {
        "id": "VM2wMB50KzM3"
      },
      "id": "VM2wMB50KzM3"
    },
    {
      "cell_type": "markdown",
      "source": [
        "While this analysis captures core demographic and household correlations to voting behavior, it omits several dimensions that could strengthen explanatory power and refine predictions. Socioeconomic variables—such as income, education levels, and employment sectors—are known to shape political preferences and could highlight additional divides within and across counties. Migration trends, naturalization status, and geographic mobility could also explain differences in partisan lean, particularly in fast‐growing metro regions.\n",
        "\n",
        "In addition, county‐level aggregates obscure within‐county variation, especially in large metropolitan areas where neighborhoods diverge sharply in demographics and partisanship. Incorporating finer spatial resolution (e.g., census tract or voter precinct) or longitudinal trends over multiple election cycles could help reveal whether these patterns are persistent or shifting. Finally, the integration of turnout variables—distinguishing who is registered, eligible, and actually voting—would broaden the analysis beyond demographic composition to electoral engagement.\n",
        "\n",
        "Together, these additions would not only deepen the descriptive accuracy but also broaden the explanatory scope of the findings, linking demographic patterns more directly with political outcomes."
      ],
      "metadata": {
        "id": "5EzLX1TKKwTx"
      },
      "id": "5EzLX1TKKwTx"
    },
    {
      "cell_type": "markdown",
      "source": [
        "END"
      ],
      "metadata": {
        "id": "Ec1EelkQ_mYs"
      },
      "id": "Ec1EelkQ_mYs"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}