{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/J-Nobull/Migration_Research/blob/main/Migration_Capstone.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Capstone Research Project on Migration Within the USA"
      ],
      "metadata": {
        "id": "73b5grE-eCpw"
      },
      "id": "73b5grE-eCpw"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0d793bbe-9ae4-493e-87a9-cd3613a55070",
      "metadata": {
        "scrolled": true,
        "id": "0d793bbe-9ae4-493e-87a9-cd3613a55070",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "494fdff3-e594-4ab7-d8e0-2a01532ad54f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: census in /usr/local/lib/python3.12/dist-packages (0.8.24)\n",
            "Requirement already satisfied: requests>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from census) (2.32.4)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=1.1.0->census) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=1.1.0->census) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=1.1.0->census) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=1.1.0->census) (2025.11.12)\n",
            "\n",
            "Environment Ready\n"
          ]
        }
      ],
      "source": [
        "# Setup initial environment\n",
        "!pip install census\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import requests\n",
        "import time\n",
        "from io import BytesIO\n",
        "from census import Census\n",
        "\n",
        "print('\\nEnvironment Ready')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define Keys\n",
        "(KEY SECRETS NOT SET UP YET, USER MUST GET THEIR OWN KEY AND REPLACE Key-Here)"
      ],
      "metadata": {
        "id": "WDpra_RieDSa"
      },
      "id": "WDpra_RieDSa"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get API keys:  \n",
        "https://apps.bea.gov/API/signup/  \n",
        "https://data.bls.gov/registrationEngine/  \n",
        "https://api.census.gov/data/key_signup.html"
      ],
      "metadata": {
        "id": "t1GjV13ndXCL"
      },
      "id": "t1GjV13ndXCL"
    },
    {
      "cell_type": "code",
      "source": [
        "# from getpass import getpass\n",
        "'''\n",
        "def get_api_key(name):\n",
        "    key = os.getenv(name)\n",
        "    if not key:\n",
        "        key = getpass(f\"Enter {name} (hidden input): \")\n",
        "    return key\n",
        "\n",
        "API_KEY_BEA = get_api_key('API_KEY_BEA')\n",
        "API_KEY_BLS = get_api_key('API_KEY_BLS')\n",
        "API_KEY_CENSUS = get_api_key('API_KEY_CENSUS')\n",
        "\n",
        "print('API keys loaded')\n",
        "'''"
      ],
      "metadata": {
        "id": "DUZzBQeZ_joV"
      },
      "id": "DUZzBQeZ_joV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define Helper function to create FIPS  \n",
        "(common key to merge all datasets)"
      ],
      "metadata": {
        "id": "V8lro_DiDKVe"
      },
      "id": "V8lro_DiDKVe"
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to create 5-digit FIPS codes\n",
        "def create_fips(state_fips_series, county_fips_series):\n",
        "    return state_fips_series.astype(str).str.zfill(2) + \\\n",
        "           county_fips_series.astype(str).str.zfill(3)"
      ],
      "metadata": {
        "id": "4aLqRe_bDQNq"
      },
      "id": "4aLqRe_bDQNq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Import all datasets"
      ],
      "metadata": {
        "id": "UrYMnDvnd_UE"
      },
      "id": "UrYMnDvnd_UE"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "##Import from Bureau of Economic Analysis (1 of 8)  \n",
        "The following constant variables will be dropped from the respective files:  \n",
        "PARPP-3: 'CL_UNIT' & 'UNIT_MULT'  \n",
        "MARPP-3: 'CL_UNIT' & 'UNIT_MULT'  \n",
        "CAINC1-3: 'CL_UNIT': 'Dollars', 'UNIT_MULT' & 'NoteRef'; as well as  'GeoName'  \n",
        "CAGDP1-1: 'CL_UNIT': 'Thousands of chained 2017 dollars' & 'UNIT_MULT'; as well as 'GeoName'"
      ],
      "metadata": {
        "id": "i8WVJ2GtW3pb"
      },
      "id": "i8WVJ2GtW3pb"
    },
    {
      "cell_type": "code",
      "source": [
        "API_KEY_BEA = 'Key-Here'\n",
        "BEA_URL = 'https://apps.bea.gov/api/data'\n",
        "YEARS = list(range(2011, 2022))\n",
        "\n",
        "# Define Tables with LineCodes and GeoFips\n",
        "TABLES = [\n",
        "    {'name': 'PARPP', 'linecode': '3', # Cost of living for Metro/Non-metro\n",
        "     'geofips': 'PORT', 'desc': 'RPP_portion', 'filename': 'BEA_rpp_p.csv'},\n",
        "    {'name': 'MARPP', 'linecode': '3', # Cost of living for Urban areas (MSAs)\n",
        "     'geofips': 'MSA', 'desc': 'RPP_msa', 'filename': 'BEA_rpp_m.csv'},\n",
        "    {'name': 'CAINC1', 'linecode': '3', # County Per Capita Income (PCI)\n",
        "     'geofips': 'COUNTY', 'desc': 'BEA_pci', 'filename': 'BEA_PCI.csv'},\n",
        "    {'name': 'CAGDP1', 'linecode': '1', # County Gross Domestic Product (GDP)\n",
        "     'geofips': 'COUNTY', 'desc': 'BEA_gdp', 'filename': 'BEA_GDP.csv'}]\n",
        "\n",
        "# Fetch tables\n",
        "print('Downloading BEA data (2011-2021)...')\n",
        "\n",
        "for table in TABLES:\n",
        "\n",
        "    print(f\"\\nFetching {table['desc']} ({table['name']})...\")\n",
        "\n",
        "    params = {\n",
        "        'UserID': API_KEY_BEA,\n",
        "        'method': 'GetData',\n",
        "        'datasetname': 'Regional',\n",
        "        'TableName': table['name'],\n",
        "        'LineCode': table['linecode'],\n",
        "        'Year': YEARS,\n",
        "        'GeoFips': table['geofips'],\n",
        "        'ResultFormat': 'json'}\n",
        "\n",
        "    response = requests.get(BEA_URL, params=params, timeout=120)\n",
        "    data = response.json()\n",
        "\n",
        "# Show errors\n",
        "    if 'Error' in data.get('BEAAPI', {}):\n",
        "        print(f\" ❌ Error: {data['BEAAPI']['Error']['Detail']}\")\n",
        "        continue\n",
        "\n",
        "    df = pd.DataFrame(data['BEAAPI']['Results']['Data'])\n",
        "\n",
        "# Initialize rename_map dynamically based on available columns\n",
        "    rename_map = {}\n",
        "\n",
        "# Rename 'TimePeriod' to 'YEAR'\n",
        "    if 'TimePeriod' in df.columns: rename_map['TimePeriod'] = 'YEAR'\n",
        "\n",
        "# Rename 'DataValue' to table['desc']\n",
        "    if 'DataValue' in df.columns: rename_map['DataValue'] = table['desc']\n",
        "    else:\n",
        "# If 'DataValue' not present, manually create column with NaNs\n",
        "        print(f\"  Warning: 'DataValue' column missing for {table['name']}. Creating '{table['desc']}' with NaN.\")\n",
        "        df[table['desc']] = np.nan\n",
        "\n",
        "# Conditionally rename 'GeoFips'\n",
        "    if 'GeoFips' in df.columns:\n",
        "        if table['name'] in ['CAINC1', 'CAGDP1']:\n",
        "            rename_map['GeoFips'] = 'FIPS'\n",
        "        else: # For PARPP and MARPP\n",
        "            rename_map['GeoFips'] = 'GeoFIPS' # lowercase to uppercase\n",
        "\n",
        "# Apply renaming\n",
        "    df = df.rename(columns=rename_map)\n",
        "\n",
        "# After renaming, this handles errors.\n",
        "    if 'YEAR' not in df.columns:\n",
        "        df['YEAR'] = np.nan\n",
        "        print(f\"  Warning: 'TimePeriod' or 'YEAR' column not found in {table['name']}. 'YEAR' created with NaN.\")\n",
        "    df['YEAR'] = pd.to_numeric(df['YEAR'], errors='coerce')\n",
        "\n",
        "# Convert the 'desc' column to numeric\n",
        "    if table['desc'] in df.columns:\n",
        "        df[table['desc']] = pd.to_numeric(df[table['desc']], errors='coerce')\n",
        "\n",
        "# Keep 'GeoName' and other variables based on table type\n",
        "    if table['name'] in ['PARPP', 'MARPP']:\n",
        "# Ensure 'GeoName' is present before trying to select it\n",
        "        cols_to_keep = ['GeoFIPS', 'YEAR', table['desc']]\n",
        "        if 'GeoName' in df.columns:\n",
        "            cols_to_keep.insert(1, 'GeoName') # Insert GeoName after GeoFips\n",
        "        df = df[cols_to_keep]\n",
        "    else: # For CAINC1 and CAGDP1\n",
        "        # Ensure 'FIPS' column exists after renaming\n",
        "        fips_col = 'FIPS'\n",
        "        if fips_col not in df.columns:\n",
        "            print(f\"  Warning: '{fips_col}' column not found after renaming for {table['name']}. Skipping selection.\")\n",
        "            continue # Skip saving if critical column is missing\n",
        "        df = df[[fips_col, 'YEAR', table['desc']]]\n",
        "\n",
        "# Save to CSV\n",
        "    df.to_csv(table['filename'], index=False)\n",
        "\n",
        "    print(f\"   Saved {len(df):,} rows to {table['filename']}\")\n",
        "# Add check to confirm file existence immediately after saving\n",
        "    if os.path.exists(table['filename']):\n",
        "        print(f\"    (Confirmed: {table['filename']} exists on disk)\")\n",
        "    else:\n",
        "        print(f\"    (ERROR: {table['filename']} NOT found on disk after saving!)\")\n",
        "    print(df.head())\n",
        "    time.sleep(2)\n",
        "\n",
        "print('\\n' + '='*30)\n",
        "print('BEA IMPORT COMPLETE')\n",
        "print('='*30)\n",
        "print('Files created:')\n",
        "for table in TABLES:\n",
        "    print(f\"  - {table['filename']}\")"
      ],
      "metadata": {
        "id": "KNrnDHAoLZDG"
      },
      "id": "KNrnDHAoLZDG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Join BEA_PCI and BEA_GDP on FIPS and Year\n",
        "BEA_PCI = pd.read_csv('BEA_PCI.csv')\n",
        "BEA_PCI['FIPS'] = BEA_PCI['FIPS'].astype(str).str.zfill(5)\n",
        "BEA_GDP = pd.read_csv('BEA_GDP.csv')\n",
        "BEA_GDP['FIPS'] = BEA_GDP['FIPS'].astype(str).str.zfill(5)\n",
        "\n",
        "BEA_join1 = pd.merge(BEA_PCI, BEA_GDP, on=['FIPS', 'YEAR'], how='inner')\n",
        "\n",
        "# Create state variable from first 2 digits of FIPS\n",
        "BEA_join1['STATE'] = BEA_join1[\n",
        "    'FIPS'].str[:2].fillna(0).astype(int).astype(str).str.zfill(2)\n",
        "\n",
        "# Display\n",
        "print(BEA_join1.head())"
      ],
      "metadata": {
        "id": "HkEywl_VS-bY"
      },
      "id": "HkEywl_VS-bY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download CBSA delineation file to convert RPP_MSA to FIPS-State 2-digit\n",
        "url = 'https://www2.census.gov/programs-surveys/metro-micro/geographies/reference-files/2013/delineation-files/list1.xls'\n",
        "\n",
        "# CBSA file will be used to convert MSA into 5-digit FIPS\n",
        "print(\"Downloading Feb 2013 CBSA delineation file...\")\n",
        "response = requests.get(url)\n",
        "cbsa_file = pd.read_excel(BytesIO(response.content), skiprows=2)\n",
        "\n",
        "print(f\"Loaded {len(cbsa_file):,} records from delineation file\")\n",
        "\n",
        "# Create 5-digit FIPS\n",
        "cbsa_file['FIPS'] = create_fips(\n",
        "    cbsa_file['FIPS State Code'].fillna(0).astype(int),\n",
        "    cbsa_file['FIPS County Code'].fillna(0).astype(int))\n",
        "\n",
        "cbsa_file.rename(columns={\n",
        "    'CBSA Code': 'CBSA_code',\n",
        "    'FIPS State Code': 'STATE'}, inplace=True)\n",
        "cbsa_file['STATE'] = cbsa_file['STATE'].fillna(0).astype(int).astype(str).str.zfill(2)\n",
        "\n",
        "BEA_rpp_m = pd.read_csv('BEA_rpp_m.csv')\n",
        "BEA_rpp_m['GeoFIPS'] = BEA_rpp_m['GeoFIPS'].astype(str).str.zfill(5)\n",
        "\n",
        "print(f\"\\nLoaded BEA file with {len(BEA_rpp_m):,} rows\")\n",
        "print(f\"BEA GeoFIPS dtype: {BEA_rpp_m['GeoFIPS'].dtype}\")\n",
        "print(f\"Crosswalk CBSA_code dtype: {cbsa_file['CBSA_code'].dtype}\")\n",
        "\n",
        "# Merge\n",
        "df_merged = BEA_rpp_m.merge(cbsa_file, left_on='GeoFIPS', right_on='CBSA_code', how='left')\n",
        "\n",
        "# Select final columns\n",
        "BEA_join2 = df_merged[['FIPS', 'YEAR', 'RPP_msa', 'STATE']]\n",
        "\n",
        "# Display\n",
        "print(BEA_join2.head())"
      ],
      "metadata": {
        "id": "Y-Z26Gs1vMNc"
      },
      "id": "Y-Z26Gs1vMNc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Line 22 clears unneeded columns.  \n",
        "To visually verify full.merge and .fillna completed cleanly, create docstring: delete # on lines 21 & 23."
      ],
      "metadata": {
        "id": "kznxRdgE0gzF"
      },
      "id": "kznxRdgE0gzF"
    },
    {
      "cell_type": "code",
      "source": [
        "BEA_rpp_p = pd.read_csv('BEA_rpp_p.csv', dtype={'GeoFIPS': str})\n",
        "# Create state variable from first 2 digits of FIPS\n",
        "BEA_rpp_p['FIPS'] = BEA_rpp_p['GeoFIPS'].astype(str)\n",
        "BEA_rpp_p['STATE'] = BEA_rpp_p['FIPS'].str[:2]\n",
        "BEA_rpp_p = BEA_rpp_p[BEA_rpp_p['FIPS'].str.endswith('999')].copy()\n",
        "\n",
        "# Merge BEA_join1 (PCI + GDP) with BEA_join2 (MSA/metro RPP)\n",
        "BEA_full = BEA_join1.merge(BEA_join2[['FIPS', 'YEAR', 'RPP_msa']],\n",
        "                            on=['FIPS', 'YEAR'],\n",
        "                            how='left')\n",
        "\n",
        "# Merge state-level RPP for filling NaN values\n",
        "BEA_import = BEA_full.merge(BEA_rpp_p[['STATE', 'YEAR', 'RPP_portion']],\n",
        "                          on=['STATE', 'YEAR'],\n",
        "                          how='left')\n",
        "\n",
        "# Fill null RPP values with RPP_portion\n",
        "BEA_import['RPP'] = BEA_import['RPP_msa'].fillna(BEA_import['RPP_portion'])\n",
        "\n",
        "# Drop the unneeded columns\n",
        "# '''\n",
        "BEA_import.drop(columns=['RPP_msa', 'RPP_portion', 'STATE'], inplace=True)\n",
        "# '''\n",
        "print(f\"BEA_import complete with {len(BEA_import):,} rows\")\n",
        "print(f\"RPP values filled: {BEA_import['RPP'].notna().sum():,}\")\n",
        "\n",
        "BEA_import.to_csv('BEA_import.csv', index=False)\n",
        "\n",
        "# Display\n",
        "print(BEA_import.head(35))"
      ],
      "metadata": {
        "id": "bJsQSneQkB4d"
      },
      "id": "bJsQSneQkB4d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import from Census Bureau (2 of 8)  \n",
        "import selected variables from Census \"Detailed Tables\""
      ],
      "metadata": {
        "id": "O8D2xIYykGkx"
      },
      "id": "O8D2xIYykGkx"
    },
    {
      "cell_type": "code",
      "source": [
        "API_KEY_CENSUS = 'Key-Here'\n",
        "CENSUS_URL = 'https://api.census.gov/data'\n",
        "YEARS = list(range(2011, 2022))\n",
        "\n",
        "# ACS variable definitions\n",
        "ACS_VARS = {\n",
        "# Population\n",
        "    'B01003_001E': 'total_population',\n",
        "# Age\n",
        "    'B01002_001E': 'median_age',\n",
        "# Housing\n",
        "    'B25003_001E': 'housing_total',\n",
        "    'B25003_002E': 'owner_occupied',\n",
        "    'B25003_003E': 'renter_occupied',\n",
        "    'B25002_003E': 'vacant',\n",
        "    'B25077_001E': 'median_home_value',\n",
        "# Households\n",
        "    'B11001_002E': 'family_households',\n",
        "# Marital Status\n",
        "    'B12001_001E': 'marital_total',\n",
        "    'B12001_003E': 'never_married_male',\n",
        "    'B12001_004E': 'now_married_male',\n",
        "    'B12001_010E': 'divorced_male',\n",
        "    'B12001_009E': 'widowed_male',\n",
        "    'B12001_012E': 'never_married_female',\n",
        "    'B12001_013E': 'now_married_female',\n",
        "    'B12001_018E': 'widowed_female',\n",
        "    'B12001_019E': 'divorced_female',\n",
        "# Children (all under 18)\n",
        "    'B09001_002E': 'under_18_in_hh',\n",
        "# Race/Ethnicity (sums to total)\n",
        "    'B03002_003E': 'white',\n",
        "    'B03002_004E': 'black',\n",
        "    'B03002_005E': 'native',\n",
        "    'B03002_006E': 'asian',\n",
        "    'B03002_007E': 'pacific_islander',\n",
        "    'B03002_008E': 'other_race',\n",
        "    'B03002_009E': 'two_or_more_nh',\n",
        "    'B03002_012E': 'hispanic',\n",
        "# Education\n",
        "    'B15002_001E': 'education_total_sex',\n",
        "    'B15002_011E': 'male_complete_hs',\n",
        "    'B15002_012E': 'male_some_college<1',\n",
        "    'B15002_013E': 'male_some_college>1',\n",
        "    'B15002_014E': 'male_associates',\n",
        "    'B15002_015E': 'male_bachelors',\n",
        "    'B15002_016E': 'male_masters',\n",
        "    'B15002_017E': 'male_professional',\n",
        "    'B15002_018E': 'male_doctorate',\n",
        "    'B15002_028E': 'female_complete_hs',\n",
        "    'B15002_029E': 'female_some_college<1',\n",
        "    'B15002_030E': 'female_some_college>1',\n",
        "    'B15002_031E': 'female_associates',\n",
        "    'B15002_032E': 'female_bachelors',\n",
        "    'B15002_033E': 'female_masters',\n",
        "    'B15002_034E': 'female_professional',\n",
        "    'B15002_035E': 'female_doctorate',\n",
        "# Income\n",
        "    'B19013_001E': 'median_hh_income',\n",
        "# Employment\n",
        "    'B23025_004E': 'employed',\n",
        "    'B23025_005E': 'unemployed',\n",
        "    'B23025_007E': 'not_in_labor_force',\n",
        "# Commute Time\n",
        "    'B08303_002E': 'commute_less_5min',\n",
        "    'B08303_003E': 'commute_5_9min',\n",
        "    'B08303_004E': 'commute_10_14min',\n",
        "    'B08303_005E': 'commute_15_19min',\n",
        "    'B08303_006E': 'commute_20_24min',\n",
        "    'B08303_007E': 'commute_25_29min',\n",
        "    'B08303_008E': 'commute_30_34min',\n",
        "    'B08303_009E': 'commute_35_39min',\n",
        "    'B08303_010E': 'commute_40_44min',\n",
        "    'B08303_011E': 'commute_45_59min',\n",
        "    'B08303_012E': 'commute_60_89min',\n",
        "    'B08303_013E': 'commute_90_plus_min',\n",
        "# Worked from home\n",
        "    'B08137_020E': 'work_in_owned_home',\n",
        "    'B08137_021E': 'work_in_rental',\n",
        "# Estate taxes paid\n",
        "    'B25103_001E': 'median_property_taxes',\n",
        "# Industry\n",
        "    'C24060_001E': 'occupation_total',\n",
        "    'C24060_002E': 'Mgmt_Biz_Sci_Arts',\n",
        "    'C24060_003E': 'Services',\n",
        "    'C24060_004E': 'Sales_Admin',\n",
        "    'C24060_005E': 'Nat-rsrc_Constr_Maint',\n",
        "    'C24060_006E': 'Prod_Transp_Mvng'}\n",
        "\n",
        "def fetch_census_batch(year, variables):\n",
        "    # Fetch one batch of Census variables for all counties.\n",
        "    var_list = ','.join(variables)\n",
        "    params = {\n",
        "        'get': var_list,\n",
        "        'for': 'county:*',\n",
        "        'key': API_KEY_CENSUS}\n",
        "\n",
        "    url = f'{CENSUS_URL}/{year}/acs/acs5'\n",
        "    response = requests.get(url, params=params, timeout=120)\n",
        "\n",
        "    # Check for HTTP errors first\n",
        "    if response.status_code != 200:\n",
        "        print(f\"❌ HTTP Error for {year}: Status Code {response.status_code}\")\n",
        "        print(f\"Response content: {response.text}\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    try:\n",
        "        data = response.json()\n",
        "    except requests.exceptions.JSONDecodeError:\n",
        "        print(f\"❌JSON Decode Error in {year}: Could not parse response as JSON\")\n",
        "        print(f\"Response content: {response.text}\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    # Show errors based on parsed JSON (if any)\n",
        "    if 'error' in data or len(data) <= 1:\n",
        "        print(f\"❌ Error: No data returned or API error for {year}\")\n",
        "        if 'error' in data:\n",
        "            print(f\"API error details: {data['error']}\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    df = pd.DataFrame(data[1:], columns=data[0])\n",
        "    return df\n",
        "\n",
        "# Fetch data\n",
        "print('Downloading Census data (2011-2021)...\\n')\n",
        "\n",
        "all_years = []\n",
        "all_vars = list(ACS_VARS.keys())\n",
        "batch1 = all_vars[:45]  # Census API limit is 50 variables\n",
        "batch2 = all_vars[45:]\n",
        "\n",
        "for year in YEARS:\n",
        "    print(f\"Fetching year {year}...\")\n",
        "\n",
        "    # Fetch both batches\n",
        "    df1 = fetch_census_batch(year, batch1)\n",
        "    df2 = fetch_census_batch(year, batch2)\n",
        "\n",
        "    if df1.empty:\n",
        "        continue\n",
        "\n",
        "    # Merge batches on state and county\n",
        "    if not df2.empty:\n",
        "        year_df = pd.merge(df1, df2, on=['state', 'county'], how='outer')\n",
        "    else:\n",
        "        year_df = df1\n",
        "\n",
        "    # Rename variables\n",
        "    year_df = year_df.rename(columns=ACS_VARS)\n",
        "\n",
        "    # Create FIPS\n",
        "    year_df['FIPS'] = create_fips(year_df['state'], year_df['county'])\n",
        "    year_df['Year'] = year\n",
        "\n",
        "    # Convert to numeric\n",
        "    for col in ACS_VARS.values():\n",
        "        if col in year_df.columns:\n",
        "            year_df[col] = pd.to_numeric(year_df[col], errors='coerce')\n",
        "\n",
        "    all_years.append(year_df)\n",
        "    print(f\"Saved {len(year_df):,} rows\")\n",
        "    time.sleep(0.5)\n",
        "\n",
        "# Combine all years\n",
        "if all_years:\n",
        "    CEN_df = pd.concat(all_years, ignore_index=True)\n",
        "\n",
        "    # Keep only needed columns\n",
        "    keep_cols = ['FIPS', 'Year'] + [col for col in ACS_VARS.values()\n",
        "                                     if col in CEN_df.columns]\n",
        "    CEN_df = CEN_df[keep_cols]\n",
        "\n",
        "    # Save to CSV\n",
        "    CEN_df.to_csv('Census_import.csv', index=False)\n",
        "\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 30)\n",
        "    print('CENSUS IMPORT COMPLETE')\n",
        "    print(\"\\n\" + \"=\" * 30)\n",
        "    print(f\"\\n --Saved {len(CEN_df):,} rows\")\n",
        "    print(f\"   Counties: {CEN_df['FIPS'].nunique()}\")\n",
        "    print(f\"   Years: {CEN_df['Year'].min()}-{CEN_df['Year'].max()}\")\n",
        "    print(f\"   Variables: {len(ACS_VARS)}\")\n",
        "else:\n",
        "    print('\\n❌ Error: No data was downloaded')"
      ],
      "metadata": {
        "id": "tJUfSZ66R--x"
      },
      "id": "tJUfSZ66R--x",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import from Bureau of Labor Statistics (3 of 8)  \n",
        "only import 'unemployment rate'"
      ],
      "metadata": {
        "id": "j8-56UJUAtHr"
      },
      "id": "j8-56UJUAtHr"
    },
    {
      "cell_type": "code",
      "source": [
        "API_KEY_BLS = 'Key-Here'\n",
        "BLS_URL = 'https://api.bls.gov/publicAPI/v2/timeseries/data/'\n",
        "YEARS = list(range(2011, 2022))\n",
        "\n",
        "print('Building BLS series IDs from Census FIPS codes...\\n')\n",
        "\n",
        "# Get FIPS from Census data, ensure FIPS is string and zero-filled\n",
        "census_df = pd.read_csv('Census_import.csv', dtype={'FIPS': str})\n",
        "census_df['FIPS'] = census_df['FIPS'].str.zfill(5)\n",
        "unique_fips = sorted(census_df['FIPS'].unique())\n",
        "\n",
        "# Build series IDs for matching counties\n",
        "all_series = [f'LAUCN{fips}0000000003' for fips in unique_fips]\n",
        "\n",
        "print(f'Found {len(all_series):,} counties from Census data')\n",
        "print(f'Will require {(len(all_series) + 49) // 50} batches\\n')\n",
        "\n",
        "# Download in batches of 50\n",
        "batch_size = 50\n",
        "all_data = []\n",
        "\n",
        "print('Downloading BLS unemployment rate data (2011-2021)...\\n')\n",
        "\n",
        "for i in range(0, len(all_series), batch_size):\n",
        "    batch = all_series[i:i+batch_size]\n",
        "    batch_num = i // batch_size + 1\n",
        "    total_batches = (len(all_series) + 49) // 50\n",
        "\n",
        "    payload = {\n",
        "        'seriesid': batch,\n",
        "        'startyear': '2011',\n",
        "        'endyear': '2021',\n",
        "        'registrationkey': API_KEY_BLS,\n",
        "        'annualaverage': True}\n",
        "\n",
        "    response = requests.post(BLS_URL, json=payload, timeout=120)\n",
        "    data = response.json()\n",
        "\n",
        "# Check for errors\n",
        "    if data.get('status') != 'REQUEST_SUCCEEDED':\n",
        "        print(f'❌ Batch {batch_num}/{total_batches} error: {data.get(\"message\", \"Unknown\")}')\n",
        "        continue\n",
        "\n",
        "# Parse response\n",
        "    for series in data['Results']['series']:\n",
        "        series_id = series['seriesID']\n",
        "        fips = series_id[5:10]  # Extract FIPS\n",
        "\n",
        "        for item in series['data']:\n",
        "            if item['period'] == 'M13':  # Average unemployment rate for year\n",
        "                value = item['value']\n",
        "\n",
        "                # Handle missing data (represented as '-')\n",
        "                if value == '-':\n",
        "                    unemployment_rate = None\n",
        "                else:\n",
        "                    unemployment_rate = float(value)\n",
        "\n",
        "                all_data.append({\n",
        "                    'FIPS': fips,\n",
        "                    'YEAR': int(item['year']),\n",
        "                    'unemploy_rate': unemployment_rate})\n",
        "\n",
        "    print(f'  Batch {batch_num}/{total_batches}')\n",
        "    time.sleep(2)\n",
        "\n",
        "# Save to CSV\n",
        "if all_data:\n",
        "    BLS_import = pd.DataFrame(all_data)\n",
        "    BLS_import.to_csv('BLS_import.csv', index=False)\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 30)\n",
        "    print('BLS IMPORT COMPLETE')\n",
        "    print(\"=\" * 30)\n",
        "    print(f'\\n Saved {len(BLS_import):,} rows')\n",
        "    print(f'  Counties: {BLS_import[\"FIPS\"].nunique()}')\n",
        "    print(f'  Years: {BLS_import[\"YEAR\"].min()}-{BLS_import[\"YEAR\"].max()}')\n",
        "    print(f'  Missing values: {BLS_import[\"unemploy_rate\"].isna().sum()}')\n",
        "else:\n",
        "    print('\\n❌ Error: No data downloaded')\n",
        "\n",
        "# Display\n",
        "print(BLS_import.head(25))"
      ],
      "metadata": {
        "id": "qZ6kvSELDe4j"
      },
      "id": "qZ6kvSELDe4j",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Read data from IRS (4 of 8)  \n",
        "download 11 State-to-state inflow files for 2011-2021 at:  \n",
        "https://www.irs.gov/statistics/soi-tax-stats-migration-data"
      ],
      "metadata": {
        "id": "De2cLDxBbYMJ"
      },
      "id": "De2cLDxBbYMJ"
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the years and file paths\n",
        "years = list(range(2011, 2022))\n",
        "year_mapping = {\n",
        "    2011: 'countyinflow1112.csv',\n",
        "    2012: 'countyinflow1213.csv',\n",
        "    2013: 'countyinflow1314.csv',\n",
        "    2014: 'countyinflow1415.csv',\n",
        "    2015: 'countyinflow1516.csv',\n",
        "    2016: 'countyinflow1617.csv',\n",
        "    2017: 'countyinflow1718.csv',\n",
        "    2018: 'countyinflow1819.csv',\n",
        "    2019: 'countyinflow1920.csv',\n",
        "    2020: 'countyinflow2021.csv',\n",
        "    2021: 'countyinflow2122.csv'}\n",
        "\n",
        "print(\"Starting IRS Migration Data Import...\")\n",
        "\n",
        "all_data = []\n",
        "\n",
        "for year, filepath in year_mapping.items():\n",
        "    print(f\"Processing {year}...\")\n",
        "\n",
        "    IRS_df = pd.read_csv(filepath, encoding='latin-1')\n",
        "    IRS_df['YEAR'] = year\n",
        "\n",
        "    IRS_df['out_FIPS'] = create_fips(IRS_df['y1_statefips'], IRS_df['y1_countyfips'])\n",
        "    IRS_df['in_FIPS'] = create_fips(IRS_df['y2_statefips'], IRS_df['y2_countyfips'])\n",
        "# Rename n2 = Number of Individuals counted on that calendar years' returns\n",
        "    IRS_df = IRS_df.rename(columns={'n2': 'Movers'})\n",
        "    IRS_df.replace(-1, np.nan, inplace=True)\n",
        "\n",
        "    print(f\"  Rows: {len(IRS_df):,}\")\n",
        "\n",
        "    all_data.append(IRS_df)\n",
        "\n",
        "IRS_full = pd.concat(all_data, ignore_index=True)\n",
        "\n",
        "# Order variables\n",
        "final_cols = ['out_FIPS', 'in_FIPS', 'YEAR', 'Movers', 'agi']\n",
        "\n",
        "# Create the dataframe\n",
        "IRS_import = IRS_full[final_cols]\n",
        "IRS_import.to_csv('IRS_import.csv', index=False)\n",
        "\n",
        "\n",
        "print('\\n' + '='*30)\n",
        "print(\"\\n Saved to IRS_import.csv\")\n",
        "print('\\n' + '='*30)\n",
        "print(f\"\\nTotal rows: {len(IRS_import):,}\")\n",
        "\n",
        "# Display\n",
        "IRS_import.head(10)"
      ],
      "metadata": {
        "id": "TNkixxod-o6S",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 901
        },
        "outputId": "324b4b9f-3e4f-418f-e7a3-250098d4b496"
      },
      "id": "TNkixxod-o6S",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting IRS Migration Data Import...\n",
            "Processing 2011...\n",
            "  Rows: 130,101\n",
            "Processing 2012...\n",
            "  Rows: 131,931\n",
            "Processing 2013...\n",
            "  Rows: 86,193\n",
            "Processing 2014...\n",
            "  Rows: 75,527\n",
            "Processing 2015...\n",
            "  Rows: 86,330\n",
            "Processing 2016...\n",
            "  Rows: 98,874\n",
            "Processing 2017...\n",
            "  Rows: 87,932\n",
            "Processing 2018...\n",
            "  Rows: 83,762\n",
            "Processing 2019...\n",
            "  Rows: 87,552\n",
            "Processing 2020...\n",
            "  Rows: 89,850\n",
            "Processing 2021...\n",
            "  Rows: 90,498\n",
            "\n",
            "==============================\n",
            "\n",
            " Saved to IRS_import.csv\n",
            "\n",
            "==============================\n",
            "\n",
            "Total rows: 1,048,550\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  out_FIPS in_FIPS  YEAR    Movers        agi\n",
              "0    96000   01000  2011  238230.0  4549431.0\n",
              "1    97000   01000  2011  235901.0  4500247.0\n",
              "2    97001   01000  2011  135124.0  2381712.0\n",
              "3    97003   01000  2011  100777.0  2118535.0\n",
              "4    98000   01000  2011    2329.0    49184.0\n",
              "5    96000   01001  2011    4618.0    83494.0\n",
              "6    97000   01001  2011    4511.0    80882.0\n",
              "7    97001   01001  2011    2961.0    48904.0\n",
              "8    97003   01001  2011    1550.0    31978.0\n",
              "9    98000   01001  2011     107.0     2612.0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-bd7b3190-9752-4b9f-aaa8-903a1a45992b\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>out_FIPS</th>\n",
              "      <th>in_FIPS</th>\n",
              "      <th>YEAR</th>\n",
              "      <th>Movers</th>\n",
              "      <th>agi</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>96000</td>\n",
              "      <td>01000</td>\n",
              "      <td>2011</td>\n",
              "      <td>238230.0</td>\n",
              "      <td>4549431.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>97000</td>\n",
              "      <td>01000</td>\n",
              "      <td>2011</td>\n",
              "      <td>235901.0</td>\n",
              "      <td>4500247.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>97001</td>\n",
              "      <td>01000</td>\n",
              "      <td>2011</td>\n",
              "      <td>135124.0</td>\n",
              "      <td>2381712.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>97003</td>\n",
              "      <td>01000</td>\n",
              "      <td>2011</td>\n",
              "      <td>100777.0</td>\n",
              "      <td>2118535.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>98000</td>\n",
              "      <td>01000</td>\n",
              "      <td>2011</td>\n",
              "      <td>2329.0</td>\n",
              "      <td>49184.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>96000</td>\n",
              "      <td>01001</td>\n",
              "      <td>2011</td>\n",
              "      <td>4618.0</td>\n",
              "      <td>83494.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>97000</td>\n",
              "      <td>01001</td>\n",
              "      <td>2011</td>\n",
              "      <td>4511.0</td>\n",
              "      <td>80882.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>97001</td>\n",
              "      <td>01001</td>\n",
              "      <td>2011</td>\n",
              "      <td>2961.0</td>\n",
              "      <td>48904.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>97003</td>\n",
              "      <td>01001</td>\n",
              "      <td>2011</td>\n",
              "      <td>1550.0</td>\n",
              "      <td>31978.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>98000</td>\n",
              "      <td>01001</td>\n",
              "      <td>2011</td>\n",
              "      <td>107.0</td>\n",
              "      <td>2612.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-bd7b3190-9752-4b9f-aaa8-903a1a45992b')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-bd7b3190-9752-4b9f-aaa8-903a1a45992b button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-bd7b3190-9752-4b9f-aaa8-903a1a45992b');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-0c95cf39-6fd6-4e5e-a762-b61e30b44445\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-0c95cf39-6fd6-4e5e-a762-b61e30b44445')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-0c95cf39-6fd6-4e5e-a762-b61e30b44445 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "IRS_import"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Read 4 datafiles from USDA (8 of 8)  \n",
        "download 4 files (names are top line in each cell) from:  \n",
        "https://www.ers.usda.gov/data-products"
      ],
      "metadata": {
        "id": "kXt6YeL7bkKR"
      },
      "id": "kXt6YeL7bkKR"
    },
    {
      "cell_type": "code",
      "source": [
        "# 5: County Typology Codes, 2015 Edition\n",
        "# --------------------------------------------------\n",
        "typology = pd.read_csv('erscountytypology2015edition.csv')\n",
        "\n",
        "# Ensure FIPS is 5-digit string\n",
        "typology['FIPStxt'] = typology['FIPStxt'].astype(str).str.zfill(5)\n",
        "\n",
        "# Rename columns\n",
        "typology.rename(columns={\n",
        "    'FIPStxt': 'FIPS',\n",
        "    'Economic Types Type_2015_Update non-overlapping': 'Industry_type',\n",
        "    'Farming_2015_Update': 'Farming',\n",
        "    'Mining_2015-Update': 'Mining',\n",
        "    'Manufacturing_2015_Update': 'Mfging',\n",
        "    'Government_2015_Update': 'Govt',\n",
        "    'Recreation_2015_Update': 'Rec',\n",
        "    'Nonspecialized_2015_Update': 'Nonspec',\n",
        "    'Low_Education_2015_Update': 'Low_Ed_cnty',\n",
        "    'Low_Employment_Cnty_2008_2012_25_64': 'Low_emp_cnty',\n",
        "    'Retirement_Dest_2015_Update': 'Retire_dest_cnty',\n",
        "    'Persistent_Poverty_2013': 'Persistent_Pov_cnty',\n",
        "    'Persistent_Related_Child_Poverty_2013': 'Pers_chld_pov_cnty'}, inplace=True)\n",
        "\n",
        "# Drop variables to avoid duplication\n",
        "typology = typology.drop(columns=['State', 'County_name',\n",
        "    'Metro-nonmetro status, 2013 0=Nonmetro 1=Metro',\n",
        "    'Economic_Type_Label'])\n",
        "\n",
        "print(f\"Economic Typology 2015: {len(typology):,} counties\")\n",
        "# Display\n",
        "typology.head(10)"
      ],
      "metadata": {
        "id": "0Dp-wGzFBm8x"
      },
      "id": "0Dp-wGzFBm8x",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6: Natural Amenities Scale\n",
        "# --------------------------------------------------\n",
        "# Data starts at row 105\n",
        "amenities = pd.read_excel('natamenf_1_.xls', skiprows=104, engine='xlrd')\n",
        "\n",
        "# Ensure FIPS is 5-digit string\n",
        "amenities['FIPS Code'] = amenities['FIPS Code'].astype(str).str.zfill(5)\n",
        "\n",
        "# Rename variables\n",
        "amenities.rename(columns={\n",
        "    'FIPS Code': 'FIPS',\n",
        "    'Scale': 'Amenity_score',\n",
        "    ' 1=Low  7=High': 'Amenity_rank'}, inplace=True)\n",
        "\n",
        "# Select only Amenity variables, drop the component variables\n",
        "amenities = amenities[['FIPS', 'Amenity_score', 'Amenity_rank']]\n",
        "\n",
        "print(f\"Natural Amenities: {len(amenities):,} counties\")\n",
        "# Display\n",
        "amenities.head(10)"
      ],
      "metadata": {
        "id": "0IWAWe3aBcFT"
      },
      "id": "0IWAWe3aBcFT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 7: Rural-Urban Continuum Codes (RUCC) 2013\n",
        "# --------------------------------------------------\n",
        "rucc2013 = pd.read_excel('ruralurbancodes2013.xls')\n",
        "\n",
        "# Ensure FIPS is 5-digit string\n",
        "rucc2013['FIPS'] = rucc2013['FIPS'].astype(str).str.zfill(5)\n",
        "\n",
        "# Rename Population variable\n",
        "rucc2013 = rucc2013.rename(columns={'Population_2010': 'POP_2010'})\n",
        "\n",
        "# Select only RUCC code and population (drop 3 variables)\n",
        "rucc2013 = rucc2013.drop(columns=['Description', 'State', 'County_Name'])\n",
        "\n",
        "print(f\"RUCC 2013: {len(rucc2013):,} counties\")\n",
        "# Display\n",
        "rucc2013.head(10)"
      ],
      "metadata": {
        "id": "1lKVqnIbA_Q9"
      },
      "id": "1lKVqnIbA_Q9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 8: Rural-Urban Continuum Codes (RUCC) 2023\n",
        "# --------------------------------------------------\n",
        "# Data is long format - 3 rows (Population, RUCC code, Description)\n",
        "ruccode2023 = pd.read_csv('Ruralurbancontinuumcodes2023.csv', encoding='latin-1')\n",
        "\n",
        "# Pivot from long to wide\n",
        "rucc2023 = ruccode2023.pivot(\n",
        "    index='FIPS',\n",
        "    columns='Attribute',\n",
        "    values='Value')\n",
        "\n",
        "# Reset the index to turn pivoted index into a regular column\n",
        "rucc2023 = rucc2023.reset_index()\n",
        "\n",
        "# Clear the columns name attribute after pivoting\n",
        "rucc2023.columns.name = None\n",
        "\n",
        "# Ensure FIPS is 5-digit string\n",
        "rucc2023['FIPS'] = rucc2023['FIPS'].astype(str).str.zfill(5)\n",
        "\n",
        "# Rename Population variable\n",
        "rucc2023 = rucc2023.rename(columns={'Population_2020': 'POP_2020'})\n",
        "\n",
        "# Select only RUCC code and population (drop description)\n",
        "rucc2023 = rucc2023[['FIPS', 'POP_2020', 'RUCC_2023']]\n",
        "\n",
        "print(f\"RUCC 2023: {len(rucc2023):,} counties\")\n",
        "# Display\n",
        "rucc2023.head(10)"
      ],
      "metadata": {
        "id": "8j4ui1tbBRLO"
      },
      "id": "8j4ui1tbBRLO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Merge all USDA data together"
      ],
      "metadata": {
        "id": "v0qjGjsyB7yd"
      },
      "id": "v0qjGjsyB7yd"
    },
    {
      "cell_type": "code",
      "source": [
        "# Merge All USDA and Create County-Year Panel, with RUCC2013 as base\n",
        "usda_base = rucc2013[['FIPS', 'POP_2010', 'RUCC_2013']].copy()\n",
        "\n",
        "# Merge all classification variables\n",
        "usda_base = usda_base.merge(rucc2023, on='FIPS', how='left')\n",
        "usda_base = usda_base.merge(amenities, on='FIPS', how='left')\n",
        "usda_base = usda_base.merge(typology, on='FIPS', how='left')\n",
        "\n",
        "print(f\"\\nMerged USDA classifications: {len(usda_base):,} counties\")\n",
        "print(f\"Total variables: {len(usda_base.columns)}\")\n",
        "\n",
        "# Expand to county-year panel (2011-2021)\n",
        "usda_panel = []\n",
        "for year in range(2011, 2022):\n",
        "    df_year = usda_base.copy()\n",
        "    df_year['YEAR'] = year\n",
        "# Move YEAR to second column\n",
        "    cols = df_year.columns.tolist()\n",
        "    cols = [cols[0], 'YEAR'] + [c for c in cols[1:] if c != 'YEAR']\n",
        "    df_year = df_year[cols]\n",
        "    usda_panel.append(df_year)\n",
        "\n",
        "usda_import = pd.concat(usda_panel, ignore_index=True)\n",
        "\n",
        "# Save single consolidated panel\n",
        "usda_import.to_csv('USDA_import.csv', index=False)\n",
        "\n",
        "print('\\n' + '='*30)\n",
        "print(\"USDA MERGE COMPLETE\")\n",
        "print('\\n' + '='*30)\n",
        "print(f\"\\nTotal county-year observations: {len(usda_import):,}\")\n",
        "print(f\"Unique counties: {usda_import['FIPS'].nunique():,}\")\n",
        "print(f\"Total variables: {len(usda_import.columns)}\")\n",
        "# Display\n",
        "usda_import.head(10)"
      ],
      "metadata": {
        "id": "yF9MLTwSBvuf"
      },
      "id": "yF9MLTwSBvuf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inspect and Clean datasets"
      ],
      "metadata": {
        "id": "QFi0Nv9hor_Z"
      },
      "id": "QFi0Nv9hor_Z"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bbda4e6b",
        "outputId": "c3803441-4889-4481-fb3d-e6f07f1a2743"
      },
      "source": [
        "BEA_clean = pd.read_csv('BEA_import.csv')\n",
        "\n",
        "# Convert 'FIPS' to 5-digit string\n",
        "BEA_clean['FIPS'] = BEA_clean['FIPS'].astype(str).str.zfill(5)\n",
        "\n",
        "print('\\nBEA first rows')\n",
        "print(BEA_clean.head())\n",
        "print('\\nBEA variable info')\n",
        "print(BEA_clean.info())\n",
        "print('\\nBEA descriptive stats')\n",
        "print(BEA_clean.describe())\n",
        "print('\\n BEA nulls')\n",
        "print(BEA_clean.isnull().sum())"
      ],
      "id": "bbda4e6b",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "BEA first rows\n",
            "    FIPS  YEAR  BEA_pci  BEA_gdp     RPP\n",
            "0  01001  2020    45068  1746979  87.517\n",
            "1  01001  2021    49174  1736001  88.497\n",
            "2  01001  2011    34430  1493906  91.098\n",
            "3  01001  2012    35151  1726577  93.269\n",
            "4  01001  2013    35348  1618151  91.394\n",
            "\n",
            "BEA variable info\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 34298 entries, 0 to 34297\n",
            "Data columns (total 5 columns):\n",
            " #   Column   Non-Null Count  Dtype  \n",
            "---  ------   --------------  -----  \n",
            " 0   FIPS     34298 non-null  object \n",
            " 1   YEAR     34298 non-null  int64  \n",
            " 2   BEA_pci  34298 non-null  int64  \n",
            " 3   BEA_gdp  34298 non-null  int64  \n",
            " 4   RPP      34298 non-null  float64\n",
            "dtypes: float64(1), int64(3), object(1)\n",
            "memory usage: 1.3+ MB\n",
            "None\n",
            "\n",
            "BEA descriptive stats\n",
            "               YEAR        BEA_pci       BEA_gdp           RPP\n",
            "count  34298.000000   34298.000000  3.429800e+04  34298.000000\n",
            "mean    2016.000000   42470.220188  6.115655e+06     90.968153\n",
            "std        3.162324   12990.093071  2.665519e+07      6.529244\n",
            "min     2011.000000       0.000000  0.000000e+00      0.000000\n",
            "25%     2013.000000   34387.250000  3.731842e+05     86.128000\n",
            "50%     2016.000000   40145.500000  9.669475e+05     89.021000\n",
            "75%     2019.000000   47569.000000  2.850464e+06     94.674000\n",
            "max     2021.000000  353263.000000  7.735713e+08    122.740000\n",
            "\n",
            " BEA nulls\n",
            "FIPS       0\n",
            "YEAR       0\n",
            "BEA_pci    0\n",
            "BEA_gdp    0\n",
            "RPP        0\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "No initial BEA issues to clean"
      ],
      "metadata": {
        "id": "Oh6vI6INKDAJ"
      },
      "id": "Oh6vI6INKDAJ"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "81010bd6",
        "outputId": "6c1fef82-b92e-47d1-b0e2-40b4d64d353d"
      },
      "source": [
        "BLS_clean = pd.read_csv('BLS_import.csv')\n",
        "\n",
        "# Convert 'FIPS' to 5-digit string\n",
        "BLS_clean['FIPS'] = BLS_clean['FIPS'].astype(str).str.zfill(5)\n",
        "\n",
        "print('\\nBLS first rows')\n",
        "print(BLS_clean.head())\n",
        "print('\\nBLS variable info')\n",
        "print(BLS_clean.info())\n",
        "print('\\nBLS descriptive stats')\n",
        "print(BLS_clean.describe())\n",
        "print('\\nBLS nulls')\n",
        "print(BLS_clean.isnull().sum())"
      ],
      "id": "81010bd6",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "BLS first rows\n",
            "    FIPS  YEAR  unemploy_rate\n",
            "0  01001  2021            2.7\n",
            "1  01001  2020            5.3\n",
            "2  01001  2019            2.9\n",
            "3  01001  2018            3.6\n",
            "4  01001  2017            4.0\n",
            "\n",
            "BLS variable info\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 35323 entries, 0 to 35322\n",
            "Data columns (total 3 columns):\n",
            " #   Column         Non-Null Count  Dtype  \n",
            "---  ------         --------------  -----  \n",
            " 0   FIPS           35323 non-null  object \n",
            " 1   YEAR           35323 non-null  int64  \n",
            " 2   unemploy_rate  35245 non-null  float64\n",
            "dtypes: float64(1), int64(1), object(1)\n",
            "memory usage: 828.0+ KB\n",
            "None\n",
            "\n",
            "BLS descriptive stats\n",
            "               YEAR  unemploy_rate\n",
            "count  35323.000000   35245.000000\n",
            "mean    2016.000255       6.104962\n",
            "std        3.162416       2.960603\n",
            "min     2011.000000       1.100000\n",
            "25%     2013.000000       4.000000\n",
            "50%     2016.000000       5.400000\n",
            "75%     2019.000000       7.500000\n",
            "max     2021.000000      28.900000\n",
            "\n",
            "BLS nulls\n",
            "FIPS              0\n",
            "YEAR              0\n",
            "unemploy_rate    78\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inspect BLS  \n",
        "78 MVs\n",
        "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  \n",
        "Puerto Rico is missing 2020 values\n",
        "Drop Puerto Rico (72000 Series)"
      ],
      "metadata": {
        "id": "WW-ali40KbGR"
      },
      "id": "WW-ali40KbGR"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b6a3550c",
        "outputId": "4078415c-f2a1-48b2-ab2d-a3e16d555842"
      },
      "source": [
        "# Exclude Puerto Rico (FIPS codes starting with '72')\n",
        "BLS_clean = BLS_clean[~BLS_clean['FIPS'].astype(str).str.startswith('72')]\n",
        "\n",
        "print('\\nBLS missing values after cleaning:')\n",
        "print(BLS_clean.isnull().sum())\n",
        "print(BLS_clean.tail())\n",
        "print(BLS_clean.info())"
      ],
      "id": "b6a3550c",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "BLS missing values after cleaning:\n",
            "FIPS             0\n",
            "YEAR             0\n",
            "unemploy_rate    0\n",
            "dtype: int64\n",
            "        FIPS  YEAR  unemploy_rate\n",
            "34460  56045  2015            3.4\n",
            "34461  56045  2014            3.5\n",
            "34462  56045  2013            3.6\n",
            "34463  56045  2012            4.1\n",
            "34464  56045  2011            4.8\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 34465 entries, 0 to 34464\n",
            "Data columns (total 3 columns):\n",
            " #   Column         Non-Null Count  Dtype  \n",
            "---  ------         --------------  -----  \n",
            " 0   FIPS           34465 non-null  object \n",
            " 1   YEAR           34465 non-null  int64  \n",
            " 2   unemploy_rate  34465 non-null  float64\n",
            "dtypes: float64(1), int64(1), object(1)\n",
            "memory usage: 1.1+ MB\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9c1201ab"
      },
      "source": [
        "Census_clean = pd.read_csv('Census_import.csv')\n",
        "\n",
        "# Convert 'FIPS' to 5-digit string\n",
        "Census_clean['FIPS'] = Census_clean['FIPS'].astype(str).str.zfill(5)\n",
        "\n",
        "# Exclude Puerto Rico\n",
        "Census_clean = Census_clean[\n",
        "    ~Census_clean['FIPS'].astype(str).str.startswith('72')]\n",
        "\n",
        "print('\\nCensus first rows:')\n",
        "print(Census_clean.head())\n",
        "pd.set_option('display.max_columns', None) # Display ALL variable info/stats\n",
        "print('\\nCensus variable info:')\n",
        "print(Census_clean.info())\n",
        "print('\\nCensus descriptive stats:')\n",
        "print(Census_clean.describe())\n",
        "print('\\nCensus nulls:')\n",
        "print(Census_clean.isnull().sum())\n",
        "pd.reset_option('display.max_columns') # Reset display to default"
      ],
      "id": "9c1201ab",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inspect Census  \n",
        "inspect 23 vars with 1MV  \n",
        "inspect 1 var with 2 MVs  \n",
        "inspect 1 var with 11 MVs  \n",
        "inspect med_home_value -666666666 values  \n",
        "inspect med_hh_income -666666666 values  \n",
        "inspect med_prop_taxes -666666666 values  \n",
        "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "Kalawao, HI(15005): DROP ALL  \n",
        "total_population averages 105 from 2011-2021.  \n",
        "BEA.gov: \"Kalawao County, Hawaii is combined with Maui County.  \n",
        "Separate estimates for the jurisdictions making up the combination areas are not available\"  \n",
        "(0.002% of Maui population)\n",
        "\n",
        "Rio Arriba, New Mexico(35039):  \n",
        "23 MVs in 2018: Fill with average of 2017 and 2019.  \n",
        "\n",
        "Buffalo, South Dakota(46017) NaN's:  \n",
        "2020 med_hm_value=56680, filled with average 2011-2021.  \n",
        "4 med_prop_taxes=634, filled with average 2011-2021.  \n",
        "\n",
        "Mellette, South Dakota(46095) NaN's:  \n",
        "2020 med_hm_value=41900, 2019 med_prop_taxes=1144, 2020 med_prop_taxes=1369   \n",
        "filled with average of year before and year after.  \n",
        "\n",
        "Jeff Davis, Texas(48243) NaN's:  \n",
        "2020 median_hh_income=50232, filled with average of 2011-2019.  \n",
        "2020 median_property_taxes=2296, filled with average of 2019 and 2021.\n",
        "\n",
        "Kenedy, Texas(48261) NaN's 2015-2021:  \n",
        "2014 med_hm_value appears to be an error [166700],  \n",
        "4 NaN and 2014 replaced with average of 2011-2019=40150.  \n",
        "\n",
        "Loving, TX(48301) --  \n",
        "*2015 median_hh_income=61250, fill with average of 2014-2016.  \n",
        "2021 median_hh_income=66064, fill with average of 2011-2020.\n",
        "*median_home_value: Utilized data from 2 nearest counties Reeves(48389) and Winkler(48495):  \n",
        "imputed values for 2016-2021 = 101750, 119100, 131700, 128300, 169400, 178700.  \n",
        "*median_property_taxes: Utillized data from Reeves and Winkler to impute 2015-2021 values:  \n",
        "(significantly lower than state median) = 1350, 1480, 1568, 1960, 2156, 2105, 2200.  \n"
      ],
      "metadata": {
        "id": "FWBx0AMFLMAZ"
      },
      "id": "FWBx0AMFLMAZ"
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop Kalawao, HI (FIPS code 15005)\n",
        "Census_clean = Census_clean[~Census_clean['FIPS'].astype(str).str.startswith('15005')]\n",
        "\n",
        "for col in Census_import.select_dtypes(include=['float64', 'int64']).columns:\n",
        "    if col != 'YEAR': # Exclude 'YEAR' from imputation as it's a year, not a feature to be averaged\n",
        "        Census_import[col].fillna(Census_import[col].mean(), inplace=True)\n",
        "\n",
        "# Ensure 'YEAR' is an integer type (already int64, but explicitly setting as per instructions)\n",
        "Census_import['YEAR'] = Census_import['YEAR'].astype(int)\n",
        "\n",
        "print('\\nCensus_import after cleaning:')\n",
        "print(Census_import.isnull().sum())\n",
        "print(Census_import.info())\n"
      ],
      "metadata": {
        "id": "jFOGJ6BbA4Ku"
      },
      "id": "jFOGJ6BbA4Ku",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QKedw8UCCW3J"
      },
      "id": "QKedw8UCCW3J",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "USDA_clean.to_csv('USDA_test.csv', index=False)"
      ],
      "metadata": {
        "id": "VQkGOn1hr5Qy"
      },
      "id": "VQkGOn1hr5Qy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "USDA_clean = pd.read_csv('USDA_import.csv')\n",
        "\n",
        "# Convert 'FIPS' to 5-digit string\n",
        "USDA_clean['FIPS'] = USDA_clean['FIPS'].astype(str).str.zfill(5)\n",
        "\n",
        "# Exclude Puerto Rico and territories; FIPS codes greater than '56999'\n",
        "USDA_cleant = USDA_clean[USDA_clean['FIPS'].astype(int) <= 56999]\n",
        "\n",
        "print('\\nUSDA descriptive stats:')\n",
        "print(USDA_clean.describe())\n",
        "print('\\nUSDA nulls:')\n",
        "print(USDA_clean.isnull().sum())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rp5_SFMq20tU",
        "outputId": "07d237ed-7978-4f95-e793-55c62d607afc"
      },
      "id": "rp5_SFMq20tU",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "USDA first rows:\n",
            "    FIPS  YEAR  POP_2010  RUCC_2013  POP_2020  RUCC_2023  Amenity_score  \\\n",
            "0  01001  2011     54571        2.0   58805.0        2.0           0.78   \n",
            "1  01003  2011    182265        3.0  231767.0        3.0           1.82   \n",
            "2  01005  2011     27457        6.0   25223.0        6.0           0.19   \n",
            "3  01007  2011     22915        1.0   22293.0        1.0          -0.15   \n",
            "4  01009  2011     57322        1.0   59134.0        1.0           0.23   \n",
            "\n",
            "   Amenity_rank  Industry_type  Farming  ...  Mfging  Govt  Rec  Nonspec  \\\n",
            "0           4.0            0.0      0.0  ...     0.0   0.0  0.0      1.0   \n",
            "1           4.0            5.0      0.0  ...     0.0   0.0  1.0      0.0   \n",
            "2           4.0            3.0      0.0  ...     1.0   0.0  0.0      0.0   \n",
            "3           3.0            0.0      0.0  ...     0.0   0.0  0.0      1.0   \n",
            "4           4.0            0.0      0.0  ...     0.0   0.0  0.0      1.0   \n",
            "\n",
            "   Low_Ed_cnty  Low_emp_cnty  Pop_Loss_2010  Retire_dest_cnty  \\\n",
            "0          0.0           0.0            0.0               1.0   \n",
            "1          0.0           0.0            0.0               1.0   \n",
            "2          1.0           1.0            0.0               0.0   \n",
            "3          1.0           1.0            0.0               0.0   \n",
            "4          1.0           1.0            0.0               0.0   \n",
            "\n",
            "   Persistent_Pov_cnty  Pers_chld_pov_cnty  \n",
            "0                  0.0                 0.0  \n",
            "1                  0.0                 0.0  \n",
            "2                  1.0                 1.0  \n",
            "3                  0.0                 1.0  \n",
            "4                  0.0                 0.0  \n",
            "\n",
            "[5 rows x 21 columns]\n",
            "\n",
            "USDA variable info:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 35574 entries, 0 to 35573\n",
            "Data columns (total 21 columns):\n",
            " #   Column               Non-Null Count  Dtype  \n",
            "---  ------               --------------  -----  \n",
            " 0   FIPS                 35574 non-null  object \n",
            " 1   YEAR                 35574 non-null  int64  \n",
            " 2   POP_2010             35574 non-null  int64  \n",
            " 3   RUCC_2013            35552 non-null  float64\n",
            " 4   POP_2020             35442 non-null  float64\n",
            " 5   RUCC_2023            35420 non-null  float64\n",
            " 6   Amenity_score        34177 non-null  float64\n",
            " 7   Amenity_rank         34177 non-null  float64\n",
            " 8   Industry_type        34573 non-null  float64\n",
            " 9   Farming              34573 non-null  float64\n",
            " 10  Mining               34573 non-null  float64\n",
            " 11  Mfging               34573 non-null  float64\n",
            " 12  Govt                 34573 non-null  float64\n",
            " 13  Rec                  34573 non-null  float64\n",
            " 14  Nonspec              34573 non-null  float64\n",
            " 15  Low_Ed_cnty          34573 non-null  float64\n",
            " 16  Low_emp_cnty         34573 non-null  float64\n",
            " 17  Pop_Loss_2010        34573 non-null  float64\n",
            " 18  Retire_dest_cnty     34573 non-null  float64\n",
            " 19  Persistent_Pov_cnty  34573 non-null  float64\n",
            " 20  Pers_chld_pov_cnty   34573 non-null  float64\n",
            "dtypes: float64(18), int64(2), object(1)\n",
            "memory usage: 5.7+ MB\n",
            "None\n",
            "\n",
            "USDA descriptive stats:\n",
            "               YEAR      POP_2010     RUCC_2013      POP_2020     RUCC_2023  \\\n",
            "count  35574.000000  3.557400e+04  35552.000000  3.544200e+04  35420.000000   \n",
            "mean    2016.000000  9.673670e+04      4.942141  1.028664e+05      5.186957   \n",
            "std        3.162322  3.086749e+05      2.722685  3.308938e+05      2.941048   \n",
            "min     2011.000000  0.000000e+00      1.000000  0.000000e+00      1.000000   \n",
            "25%     2013.000000  1.126700e+04      2.000000  1.092400e+04      2.000000   \n",
            "50%     2016.000000  2.607400e+04      6.000000  2.575950e+04      6.000000   \n",
            "75%     2019.000000  6.564500e+04      7.000000  6.680400e+04      8.000000   \n",
            "max     2021.000000  9.818605e+06      9.000000  1.001401e+07      9.000000   \n",
            "\n",
            "       Amenity_score  Amenity_rank  Industry_type       Farming        Mining  \\\n",
            "count   34177.000000  34177.000000   34573.000000  34573.000000  34573.000000   \n",
            "mean        0.052359      3.491149       1.807827      0.161311      0.081451   \n",
            "std         2.277218      1.041867       1.819244      0.367823      0.273530   \n",
            "min        -6.400000      1.000000       0.000000      0.000000      0.000000   \n",
            "25%        -1.420000      3.000000       0.000000      0.000000      0.000000   \n",
            "50%        -0.140000      3.000000       1.000000      0.000000      0.000000   \n",
            "75%         1.090000      4.000000       3.000000      0.000000      0.000000   \n",
            "max        11.170000      7.000000       5.000000      1.000000      1.000000   \n",
            "\n",
            "             Mfging          Govt           Rec       Nonspec   Low_Ed_cnty  \\\n",
            "count  34573.000000  34573.000000  34573.000000  34573.000000  34573.000000   \n",
            "mean       0.164174      0.146675      0.136176      0.393573      0.148584   \n",
            "std        0.370439      0.353787      0.342980      0.488549      0.355683   \n",
            "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
            "25%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
            "50%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
            "75%        0.000000      0.000000      0.000000      1.000000      0.000000   \n",
            "max        1.000000      1.000000      1.000000      1.000000      1.000000   \n",
            "\n",
            "       Low_emp_cnty  Pop_Loss_2010  Retire_dest_cnty  Persistent_Pov_cnty  \\\n",
            "count  34573.000000   34573.000000      34573.000000         34573.000000   \n",
            "mean       0.288260       0.168311          0.140630             0.112313   \n",
            "std        0.452959       0.374147          0.347644             0.315756   \n",
            "min        0.000000       0.000000          0.000000             0.000000   \n",
            "25%        0.000000       0.000000          0.000000             0.000000   \n",
            "50%        0.000000       0.000000          0.000000             0.000000   \n",
            "75%        1.000000       0.000000          0.000000             0.000000   \n",
            "max        1.000000       1.000000          1.000000             1.000000   \n",
            "\n",
            "       Pers_chld_pov_cnty  \n",
            "count        34573.000000  \n",
            "mean             0.225262  \n",
            "std              0.417761  \n",
            "min              0.000000  \n",
            "25%              0.000000  \n",
            "50%              0.000000  \n",
            "75%              0.000000  \n",
            "max              1.000000  \n",
            "\n",
            "USDA nulls:\n",
            "FIPS                      0\n",
            "YEAR                      0\n",
            "POP_2010                  0\n",
            "RUCC_2013                22\n",
            "POP_2020                132\n",
            "RUCC_2023               154\n",
            "Amenity_score          1397\n",
            "Amenity_rank           1397\n",
            "Industry_type          1001\n",
            "Farming                1001\n",
            "Mining                 1001\n",
            "Mfging                 1001\n",
            "Govt                   1001\n",
            "Rec                    1001\n",
            "Nonspec                1001\n",
            "Low_Ed_cnty            1001\n",
            "Low_emp_cnty           1001\n",
            "Pop_Loss_2010          1001\n",
            "Retire_dest_cnty       1001\n",
            "Persistent_Pov_cnty    1001\n",
            "Pers_chld_pov_cnty     1001\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inspect USDA  \n",
        "Amenity_score & Amenity_rank  MV=396  \n",
        "POP_2020 & RUCC_2023          MV=132  \n",
        "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "Alaska and Hawaii are not part of USDA Amenities Scale  \n",
        "[fill score=0 (average) and rank=0 (outside 1-7 ranking)]  \n",
        "\n",
        "Not sure what to do with these:  \n",
        "09xxx CT 8 counties became 9 planning regions RUCC_2023 is the only different database with POP=3605944 and codes=1,2,1,2,4,4,2,2,2(almost identical) impute 2010_pop into 2020 * 1.00891, keep RUCC codes, drop(09110-09190)\n",
        "46113 renamed 46102 in 2015 (make 46108?)\n",
        "51515 (Bedford city) folded into 51019 (Bedford County) in 2014, incorporate RUCC_2010 and census values into 51019"
      ],
      "metadata": {
        "id": "wIJMNwxivnWE"
      },
      "id": "wIJMNwxivnWE"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        },
        "id": "179da6f7",
        "outputId": "361f9c7c-a7ff-4825-f320-7e0a33218944"
      },
      "source": [
        "# Mask Alaska and Hawaii\n",
        "alaska_fips_mask = USDA_clean['FIPS'].astype(str).str.startswith('02')\n",
        "hawaii_fips_mask = USDA_clean['FIPS'].astype(str).str.startswith('15')\n",
        "ak_hi_mask = alaska_fips_mask | hawaii_fips_mask\n",
        "\n",
        "# Fill Alaska & Hawaii Amenities with 0's\n",
        "USDA_clean.loc[ak_hi_mask, 'Amenity_score'] = USDA_clean.loc[\n",
        "    ak_hi_mask, 'Amenity_score'].fillna(0)\n",
        "USDA_clean.loc[ak_hi_mask, 'Amenity_rank'] = USDA_clean.loc[\n",
        "    ak_hi_mask, 'Amenity_rank'].fillna(0)\n",
        "\n",
        "print(f\"AK/HI null values:\\n{USDA_clean[ak_hi_mask][[ \\\n",
        "        'FIPS', 'Amenity_score', 'Amenity_rank']].isnull().sum()}\")\n",
        "print(f\"\\n{USDA_clean[['POP_2020', 'RUCC_2023']].isnull().sum()}\")\n",
        "'''\n",
        "# Impute missing 2020s data with 2010s data\n",
        "USDA_clean['POP_2020'] = USDA_clean[\n",
        "    'POP_2020'].fillna(USDA_clean['POP_2010'])\n",
        "USDA_clean['RUCC_2023'] = USDA_clean[\n",
        "    'RUCC_2023'].fillna(USDA_clean['RUCC_2013'])\n",
        "\n",
        "\n",
        "'''"
      ],
      "id": "179da6f7",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AK/HI null values:\n",
            "FIPS             0\n",
            "Amenity_score    0\n",
            "Amenity_rank     0\n",
            "dtype: int64\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n# Impute missing 2020s data with 2010s data\\nUSDA_clean[\\'POP_2020\\'] = USDA_clean[\\n    \\'POP_2020\\'].fillna(USDA_clean[\\'POP_2010\\'])\\nUSDA_clean[\\'RUCC_2023\\'] = USDA_clean[\\n    \\'RUCC_2023\\'].fillna(USDA_clean[\\'RUCC_2013\\'])\\n\\nprint(f\"\\nNull values after imputation:\\n{USDA_clean[[\\'POP_2020\\', \\'RUCC_2023\\']].isnull().sum()}\")\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a5850d5b"
      },
      "source": [
        "print(USDA_clean.info())"
      ],
      "id": "a5850d5b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "02xxx AK has no amenity scale/rank fill with 0/2\n",
        "09xxx CT 8 counties became 9 planning regions\n",
        "46113 renamed 46102 in 2015\n",
        "51515 incorporated into 51019 between 2013-2023"
      ],
      "metadata": {
        "id": "H-MHJePt20aW"
      },
      "id": "H-MHJePt20aW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "IRS_import**  \n",
        "Ensure str zfill (5)  \n",
        "FIPS that are an issue:  \n",
        "xx000 = state totals, drop  \n",
        "15005, drop  \n",
        "\n",
        "origin_fips:  \n",
        "57000-98000, drop"
      ],
      "metadata": {
        "id": "oSe_LpYO4RUy"
      },
      "id": "oSe_LpYO4RUy"
    },
    {
      "cell_type": "code",
      "source": [
        "IRS_trim = pd.read_csv('IRS_import.csv')\n",
        "\n",
        "# Convert to integer for filtering\n",
        "IRS_trim['FIPS_int'] = pd.to_numeric(IRS_trim['in_FIPS'], errors='coerce')\n",
        "IRS_trim['out_FIPS_int'] = pd.to_numeric(IRS_trim['out_FIPS'], errors='coerce')\n",
        "# Drop territories/foreign origins (57000-98000)\n",
        "foreign_origin = (IRS_trim['out_FIPS_int'] >= 57000)\n",
        "# Drop state totals (xx000)\n",
        "state_totals_dest = (IRS_trim['FIPS_int'] % 1000 == 0)\n",
        "state_totals_origin = (IRS_trim['out_FIPS_int'] % 1000 == 0)\n",
        "# Drop non-movers (same origin and destination)\n",
        "non_movers = (IRS_trim['out_FIPS_int'] == IRS_trim['FIPS_int'])\n",
        "# Drop Kalawao, HI (15005)\n",
        "kalawao = (IRS_trim['FIPS_int'] == 15005)\n",
        "# Combined drop mask\n",
        "drop_mask = (state_totals_dest | state_totals_origin |\n",
        "             kalawao | foreign_origin | non_movers)\n",
        "IRS_clean = IRS_trim[~drop_mask].copy()\n",
        "\n",
        "IRS_clean['in_FIPS'] = IRS_clean[\n",
        "    'FIPS_int'].astype(int).astype(str).str.zfill(5)\n",
        "IRS_clean['out_FIPS'] = IRS_clean[\n",
        "    'out_FIPS_int'].astype(int).astype(str).str.zfill(5)\n",
        "IRS_clean = IRS_clean.drop(columns=['FIPS_int', 'out_FIPS_int'])\n",
        "\n",
        "IRS_clean.to_csv('IRS_clean.csv', index=False)\n",
        "\n",
        "print('\\nIRS variable info:')\n",
        "print(IRS_clean.info())\n",
        "print('\\nIRS descriptive stats:')\n",
        "print(IRS_clean.describe())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ce-P4rcUOUk1",
        "outputId": "282d037c-25d1-4839-d65f-dc20e51585d6"
      },
      "id": "ce-P4rcUOUk1",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "IRS variable info:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 627182 entries, 11 to 1048532\n",
            "Data columns (total 5 columns):\n",
            " #   Column    Non-Null Count   Dtype  \n",
            "---  ------    --------------   -----  \n",
            " 0   out_FIPS  627182 non-null  object \n",
            " 1   in_FIPS   627182 non-null  object \n",
            " 2   YEAR      627182 non-null  int64  \n",
            " 3   Movers    627182 non-null  float64\n",
            " 4   agi       627180 non-null  float64\n",
            "dtypes: float64(2), int64(1), object(2)\n",
            "memory usage: 28.7+ MB\n",
            "None\n",
            "\n",
            "IRS descriptive stats:\n",
            "                YEAR         Movers           agi\n",
            "count  627182.000000  627182.000000  6.271800e+05\n",
            "mean     2015.545720     194.794084  7.033679e+03\n",
            "std         3.366377     798.687576  3.465237e+04\n",
            "min      2011.000000      10.000000 -1.283400e+06\n",
            "25%      2012.000000      41.000000  1.070000e+03\n",
            "50%      2016.000000      65.000000  1.976000e+03\n",
            "75%      2019.000000     130.000000  4.263000e+03\n",
            "max      2021.000000   49355.000000  3.015639e+06\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "create 2 IRS datafiles:  \n",
        "one to add to panel with 'net_movers' and keep one county-to-county for deeper trend analysis."
      ],
      "metadata": {
        "id": "pRq-fFj1kZC1"
      },
      "id": "pRq-fFj1kZC1"
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Calculate net migration per county-year\n",
        "inflows = IRS_clean.groupby(['in_FIPS', 'YEAR']).agg({\n",
        "    'Movers': 'sum',\n",
        "    'agi': 'sum'}).rename(columns={'Movers': 'move_in', 'agi': 'agi_in'})\n",
        "outflows = IRS_clean.groupby(['out_FIPS', 'YEAR']).agg({\n",
        "    'Movers': 'sum',\n",
        "    'agi': 'sum'}).rename(columns={'Movers': 'move_out', 'agi': 'agi_out'})\n",
        "outflows.index.names = ['FIPS', 'YEAR']\n",
        "\n",
        "# Merge and calculate net\n",
        "IRS_panel = inflows.join(outflows, how='outer').fillna(0).reset_index()\n",
        "IRS_panel['net_movers'] = IRS_panel['move_in'] - IRS_panel['move_out']\n",
        "IRS_panel['net_agi'] = IRS_panel['agi_in'] - IRS_panel['agi_out']\n",
        "\n",
        "# Reorder columns\n",
        "cols = ['FIPS', 'YEAR', 'move_in', 'move_out', 'net_movers', 'agi_in', 'agi_out', 'net_agi']\n",
        "IRS_panel = IRS_panel[cols]\n",
        "\n",
        "print(f\"  Panel rows: {len(IRS_panel):,}\")\n",
        "print(f\"  Unique counties: {IRS_panel['FIPS'].nunique():,}\")\n",
        "print(f\"  Years: {sorted(IRS_panel['YEAR'].unique())}\")\n",
        "\n",
        "# Step 4: Save outputs\n",
        "print(\"\\n--- Step 4: Save outputs ---\")\n",
        "\n",
        "IRS_panel.to_csv('IRS_panel.csv', index=False)\n",
        "print(f\"  ✓ Saved IRS_panel.csv ({len(IRS_panel):,} rows)\")\n"
      ],
      "metadata": {
        "id": "H9qvbQa0WH4f"
      },
      "id": "H9qvbQa0WH4f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(IRS_clean.info())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "88MGzUUgmQjk",
        "outputId": "37f65233-f815-416c-fcb9-6542997b3f28"
      },
      "id": "88MGzUUgmQjk",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 627182 entries, 0 to 627181\n",
            "Data columns (total 5 columns):\n",
            " #   Column    Non-Null Count   Dtype  \n",
            "---  ------    --------------   -----  \n",
            " 0   out_FIPS  627182 non-null  int64  \n",
            " 1   in_FIPS   627182 non-null  int64  \n",
            " 2   YEAR      627182 non-null  int64  \n",
            " 3   Movers    627182 non-null  float64\n",
            " 4   agi       627180 non-null  float64\n",
            "dtypes: float64(2), int64(3)\n",
            "memory usage: 23.9 MB\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "IRS_clean = pd.read_csv('IRS_clean.csv')\n",
        "IRS_dyadic = IRS_clean[['out_FIPS', 'in_FIPS', 'YEAR', 'Movers', 'agi']].copy()\n",
        "IRS_dyadic = IRS_dyadic.rename(columns={\n",
        "    'out_FIPS': 'origin_fips',\n",
        "    'in_FIPS': 'dest_fips',\n",
        "    'Movers': 'n_migrants'})\n",
        "IRS_dyadic.to_csv('IRS_dyadic.csv', index=False)\n",
        "print(f\"  ✓ Saved IRS_dyadic.csv ({len(IRS_dyadic):,} rows)\")\n",
        "\n",
        "print(f\"\\nIRS Dyadic preview:\\n{IRS_dyadic.head()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WEUA2rDqkSIh",
        "outputId": "08e36d14-b20f-4862-f3ef-f17d9baa3a18"
      },
      "id": "WEUA2rDqkSIh",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  ✓ Saved IRS_dyadic.csv (627,182 rows)\n",
            "\n",
            "IRS Dyadic preview:\n",
            "   origin_fips  dest_fips  YEAR  n_migrants      agi\n",
            "0         1051       1001  2011      1016.0  18398.0\n",
            "1         1101       1001  2011       982.0  15955.0\n",
            "2         1021       1001  2011       192.0   2228.0\n",
            "3         1047       1001  2011       126.0   1638.0\n",
            "4         1073       1001  2011       104.0   1585.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Inspect IRS_import"
      ],
      "metadata": {
        "id": "5N2zgzD931i7"
      },
      "id": "5N2zgzD931i7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "⚛⚛⚛⚛⚛⚛⚛⚛⚛⚛⚛⚛⚛⚛⚛⚛⚛⚛⚛⚛⚛⚛⚛⚛⚛⚛⚛⚛⚛⚛⚛⚛⚛⚛  \n",
        "All Below is from a different project, here for coding reference"
      ],
      "metadata": {
        "id": "TYQWrLl4ALNE"
      },
      "id": "TYQWrLl4ALNE"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Clean DHC data\n",
        "All 322 features will need:  \n",
        "to be renamed (for clarity) or  \n",
        "to be dropped (for redundency)\n",
        "Project will prioritize 'percent' variables, scale is improved over 'count'."
      ],
      "metadata": {
        "id": "3KLiwCkVhkBn"
      },
      "id": "3KLiwCkVhkBn"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d4d05e06-0624-410a-b39f-80c8041ddc9a",
      "metadata": {
        "scrolled": true,
        "id": "d4d05e06-0624-410a-b39f-80c8041ddc9a",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# Create new working dataframe\n",
        "DHC_clean = DHC_import.copy()\n",
        "\n",
        "# Remove Puerto Rico, rows where GEOID is US72000 or greater\n",
        "DHC_clean = DHC_clean[~DHC_clean['Geography'].str.startswith('0500000US72')]\n",
        "\n",
        "# Rename Geography and Geographic Area Name\n",
        "DHC_clean = DHC_clean.rename(columns={\n",
        "    'Geography': 'GEOID',\n",
        "    'Geographic Area Name': 'County'})\n",
        "DHC_clean['GEOID'] = DHC_clean['GEOID'].str[-5:]\n",
        "\n",
        "# Confirm\n",
        "print(DHC_clean.info())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transform DHC data"
      ],
      "metadata": {
        "id": "y-ZwBs1UgqYw"
      },
      "id": "y-ZwBs1UgqYw"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create new 'Under 18' and '18-19' age groups"
      ],
      "metadata": {
        "id": "5Zkp0T5ZS9nj"
      },
      "id": "5Zkp0T5ZS9nj"
    },
    {
      "cell_type": "code",
      "source": [
        "# Create new working dataframe\n",
        "DHC_transform = DHC_clean.copy()\n",
        "\n",
        "# Calculate 'Under 18' by subtracting '18 years and over' from 'totals'\n",
        "DHC_transform['Total_U18'] = DHC_transform[\n",
        "    'Pop_total'] - DHC_transform['Total_18+']\n",
        "DHC_transform['Male_U18'] = DHC_transform[\n",
        "    'Male_total'] - DHC_transform['Male_18+']\n",
        "DHC_transform['Female_U18'] = DHC_transform[\n",
        "    'Female_total'] - DHC_transform['Female_18+']\n",
        "\n",
        "# Calculate 'Total_18-19' by adding all ages 0-19 and subtracting U18\n",
        "DHC_transform['Total_18_19'] = (DHC_transform['Total_U5'] +\n",
        "    DHC_transform['Total_5_9'] + DHC_transform['Total_10_14'] +\n",
        "    DHC_transform['Total_15_19'] - DHC_transform['Total_U18'])\n",
        "\n",
        "# Repeat for Male 18-19\n",
        "DHC_transform['Male_18_19'] = (DHC_transform['Male_U5'] +\n",
        "    DHC_transform['Male_5_9'] + DHC_transform['Male_10_14'] +\n",
        "    DHC_transform['Male_15_19'] - DHC_transform['Male_U18'])\n",
        "\n",
        "# Repeat for Female 18-19\n",
        "DHC_transform['Female_18_19'] = (DHC_transform['Female_U5'] +\n",
        "    DHC_transform['Female_5_9'] + DHC_transform['Female_10_14'] +\n",
        "    DHC_transform['Female_15_19'] - DHC_transform['Female_U18'])\n",
        "\n",
        "# Calculate '%_18-19' by dividing by 'totals'\n",
        "DHC_transform['%TOTAL_18_19'] = (\n",
        "    DHC_transform['Total_18_19'] / DHC_transform['Pop_total']* 100).round(2)\n",
        "DHC_transform['%MALE_18_19'] = (\n",
        "    DHC_transform['Male_18_19'] / DHC_transform['Male_total']* 100).round(2)\n",
        "DHC_transform['%FEMALE_18_19'] = (\n",
        "    DHC_transform['Female_18_19'] / DHC_transform['Female_total']* 100).round(2)\n",
        "\n",
        "# Can now drop these columns\n",
        "columns_tform_drop = [\n",
        "    'Total_U5', 'Male_U5', 'Female_U5',\n",
        "    'Total_5_9', 'Male_5_9', 'Female_5_9',\n",
        "    'Total_10_14', 'Male_10_14', 'Female_10_14',\n",
        "    'Total_15_19', 'Male_15_19', 'Female_15_19',\n",
        "    'Total_18+', 'Male_18+', 'Female_18+',\n",
        "    'Total_U18', 'Male_U18', 'Female_U18',\n",
        "    'Total_18_19', 'Male_18_19', 'Female_18_19']\n",
        "DHC_transform.drop(columns=columns_tform_drop, inplace=True)\n",
        "\n",
        "# Reorder columns to move '18-19' before '20-24'\n",
        "cols = DHC_transform.columns.tolist()\n",
        "cols.insert(8, cols.pop(cols.index('%TOTAL_18_19')))\n",
        "cols.insert(23, cols.pop(cols.index('%MALE_18_19')))\n",
        "cols.insert(38, cols.pop(cols.index('%FEMALE_18_19')))\n",
        "DHC_transform = DHC_transform[cols]\n",
        "\n",
        "#Confirm\n",
        "#pd.set_option('display.max_columns', None)\n",
        "#print(DHC_transform.head())\n",
        "#print(DHC_transform.info())"
      ],
      "metadata": {
        "id": "dAQB-_Ny8iXx",
        "collapsed": true
      },
      "id": "dAQB-_Ny8iXx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save DHC cleaned data"
      ],
      "metadata": {
        "id": "zplfSEYVg3Dh"
      },
      "id": "zplfSEYVg3Dh"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c26101d-493b-4562-b987-f9bdc233820f",
      "metadata": {
        "id": "3c26101d-493b-4562-b987-f9bdc233820f"
      },
      "outputs": [],
      "source": [
        "# Create the tidy dataframe\n",
        "DHC_tidy = DHC_transform.copy()\n",
        "\n",
        "DHC_tidy.to_csv('DHC_tidy.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import P2 data (2 of 3)\n",
        "\n",
        "P2 data is the population living in urban or rural (PUR) areas within each county **[number of households also available (H2)]**.  \n",
        "For the 2020 Census, an urban area will comprise a densely settled core of census blocks that meet minimum population density requirements; which includes adjacent territory containing non-residential urban land uses. To qualify as an urban area, the territory identified according to criteria must have a population of at least 5,000. *(Note on Alaska: See accompanying 'Alaska County' amalgamation file on github for method used to match census area to state senate district. PUR datafile combines 30 census areas into 14 'County_fips' created for this analysis)*. Also dropped Kalawao County, Hawaii: 82 rural residents, none of them voted, dropping will align it with MEDSL file when Kalawao county_fips (15005) is cleaned from MEDSL data.    \n",
        "https://data.census.gov/all?q=urban+and+rural&g=010XX00US$0500000"
      ],
      "metadata": {
        "id": "7b5RnzYIcT3Q"
      },
      "id": "7b5RnzYIcT3Q"
    },
    {
      "cell_type": "code",
      "source": [
        "# Import next dataset\n",
        "PUR_import = pd.read_csv(\n",
        "    'DECENNIALDHC2020.P2-AKfix.csv', header=1)\n",
        "\n",
        "# Inspect\n",
        "print(PUR_import.info())\n",
        "print(PUR_import.head())"
      ],
      "metadata": {
        "id": "97ihi91wgP4B",
        "collapsed": true
      },
      "id": "97ihi91wgP4B",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Clean PUR data"
      ],
      "metadata": {
        "id": "a_e9eDxJhbQe"
      },
      "id": "a_e9eDxJhbQe"
    },
    {
      "cell_type": "code",
      "source": [
        "# Create new working dataframe\n",
        "PUR_clean = PUR_import.copy()\n",
        "\n",
        "# Remove Puerto Rico: rows where GEOID is US72000 or greater\n",
        "PUR_clean = PUR_clean[~PUR_clean['Geography'].str.startswith('0500000US72')]\n",
        "\n",
        "# Rename variables to keep and drop remaining\n",
        "PUR_clean = PUR_clean.rename(columns={\n",
        "    'Geography': 'GEOID',\n",
        "    ' !!Total:': 'Pop_total',\n",
        "    ' !!Total:!!Urban': 'Pop_Urban',\n",
        "    ' !!Total:!!Rural': 'Pop_Rural'})\n",
        "\n",
        "PUR_clean['GEOID'] = PUR_clean['GEOID'].str[-5:]\n",
        "\n",
        "# Calculate Urban percent\n",
        "PUR_clean['%Urban_pop'] = (\n",
        "    (PUR_clean['Pop_Urban'] / PUR_clean['Pop_total']) * 100).round(2)\n",
        "\n",
        "# Drop the specified columns\n",
        "columns_PUR_drop = ['Pop_total',\n",
        "                    'Geographic Area Name',\n",
        "                    ' !!Total:!!Not defined for this file']\n",
        "PUR_clean.drop(columns=columns_PUR_drop, inplace=True)\n",
        "\n",
        "# Confirm\n",
        "print(PUR_clean.info())"
      ],
      "metadata": {
        "collapsed": true,
        "id": "npT_eugNhcIU"
      },
      "id": "npT_eugNhcIU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save PUR data"
      ],
      "metadata": {
        "id": "QGz3_A_Yicwu"
      },
      "id": "QGz3_A_Yicwu"
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the tidy dataframe\n",
        "PUR_tidy = PUR_clean.copy()\n",
        "\n",
        "PUR_tidy.to_csv('PUR_tidy.csv', index=False)"
      ],
      "metadata": {
        "id": "JfAS76fjie58"
      },
      "id": "JfAS76fjie58",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import MEDSL data (3 of 3)\n",
        "2020 general election results for most* (46) of the 50 states and D.C. downloaded from MEDSL (the MIT Election Data and Science Lab) https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/NT66Z3  \n",
        "\n",
        "* ALASKA: voting data is not gathered by county, MEDSL 'county_fips' is empty. Used https://www.elections.alaska.gov/results/20GENR/Map/ Votes aggregated to state senate districts (1 - 40). See accompanying 'Alaska County' amalgamation file on github for method used to match 30 census areas to 40 state senate districts. MEDSL datafile uses 14 County_fips created for this analysis. Datafile only has the 4 variables that will be utilized here.\n",
        "\n",
        "* INDIANA: MEDSL missing multiple county results. Used https://indianavoters.in.gov/ENRHistorical/ElectionResults  Datafile only has the 4 variables that will be utilized here, aggregated to the county level  \n",
        "\n",
        "* NEW MEXICO: To protect the privacy of voters, New Mexico 'masks' vote totals in precinct results for candidates with small vote tallies. Used https://electionstats.sos.nm.gov/contest/13250  Datafile only has the 4 variables that will be utilized here, aggregated to the county level  \n",
        "\n",
        "* NEVADA: To protect the privacy of voters, Nevada 'masks' vote totals in precinct results for candidates with 1-10 vote tallies. Used https://www.nvsos.gov/SOSelectionPages/results/2020StateWideGeneral/ElectionSummary.aspx  Datafile only has the 4 variables that will be utilized here, aggregated to the county level\n",
        "\n",
        "##Pre-import processing Notes:  \n",
        "The below adjustments were made to the MEDSL datafiles to standardize cleaning and processing.   \n",
        "\n",
        "1. HAWAII: Adjusted DHC and PUR data regarding Kalawao County, Hawaii. Both have fips 15005, but there are no official votes cast, removed so all files align  \n",
        "\n",
        "1. MAINE: Uniformed and Overseas Citizens Absentee Voting tallied seperately in 23000 fips, 23000 deleted to match DHC and PUR with votes added to 23005 (most populous county)  \n",
        "\n",
        "1. MICHIGAN: MEDSL precinct data contains precinct '9999', which are 'statistical adjustments' rows. There were minor corrections needed to match official results at https://www.michigan.gov/sos/elections/election-results-and-data/candidate-listings-and-election-results-by-county  \n",
        "\n",
        "1. MINNESOTA: 'DEMOCRATIC FARMER LABOR' party changed to 'DEMOCRAT'  \n",
        "\n",
        "1. MISSOURI: MEDSL tallied Kansas City votes seperately in 36000 fips. Utillized https://www.sos.mo.gov/CMSImages/ElectionResultsStatistics/November3_2020GeneralElection.pdf to aportion some votes to Jackson County with remainder assigned to Clay County (official results not available on https://www.voteclaycountymo.gov/election-results), but totals match State official numbers  \n",
        "\n",
        "1.  NEW YORK: 'CONSERVATIVE' party changed to 'REPUBLICAN'  \n",
        "'WORKING FAMILIES' party changed to 'DEMOCRAT'  \n",
        "\n",
        "1.  NORTH DAKOTA: 'DEMOCRATIC-NPL' party changed to 'DEMOCRAT' and 'county_fips' for OGLALA LAKOTA County changed from 46113 to 46102 to match data from DHC and PUR  \n",
        "\n",
        "1.  OREGON: Sherman County included cadidate 'BALLOTS CAST' which totaled all votes in each precinct: Deleted  \n",
        "\n",
        "1.  PENNSYLVANIA: 1 blank 'party_detailed' vote cast for Trump, party changed to 'REPUBLICAN'  \n",
        "\n",
        "1.  VERMONT: 3 blank 'party_detailed' votes cast for Trump, party changed to 'REPUBLICAN'  \n",
        "6 blank 'party_detailed' votes cast for Biden, party changed to 'DEMOCRAT'  \n",
        "\n",
        "##Post-import cleaning Notes:\n",
        "1.  All blanks in 'party_detailed' have been verified as writein votes cast for 'THIRD' party candidates  \n",
        "\n",
        "2.  In Nov 2020, there were over 50 recognized political parties in the US.  \n",
        "DEM and REP ballots accounted for 96% of total votes. Third parties accounted for 1-4% of the vote in each state. 'THIRD' will combine any vote NOT for Presidents Biden or Trump.  \n",
        "\n"
      ],
      "metadata": {
        "id": "3BLXE3tAg-ZV"
      },
      "id": "3BLXE3tAg-ZV"
    },
    {
      "cell_type": "code",
      "source": [
        "# Define list of all 51 voter CSV files to process (50 states plus D.C.)\n",
        "file_list = glb.glob('2020-*-precinct-general.csv')\n",
        "\n",
        "# Define function to read, select features, and clean a single CSV file\n",
        "def process_file(file_path):\n",
        "\n",
        "    try:\n",
        "# Specify data types, let 'votes' be float during import\n",
        "        dtype_spec = {'office': str, 'county_fips': str,\n",
        "                      'party_detailed': str, 'votes': float}\n",
        "        df = pd.read_csv(file_path, dtype=dtype_spec, low_memory=False)\n",
        "\n",
        "# Filter for President in 'office' to avoid counting multiple votes per person\n",
        "# Only analyze US Presidential race (it has the most voter participation)\n",
        "        df = df[df['office'] == 'US PRESIDENT'].copy()\n",
        "        df = df[['office', 'county_fips', 'party_detailed', 'votes']]\n",
        "        df = df.rename(columns={\n",
        "            'county_fips': 'GEOID',\n",
        "            'party_detailed': 'PARTY',\n",
        "            'votes': 'VOTES'})\n",
        "        return df\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f'Error processing {file_path}: {e}')\n",
        "        return None"
      ],
      "metadata": {
        "id": "1XNttjNMPRry"
      },
      "id": "1XNttjNMPRry",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Iterate through the file list, apply function, and store dataframes\n",
        "all_processed_dataframes = [\n",
        "    process_file(file_path) for file_path in file_list]\n",
        "\n",
        "# Filter out any None values if errors occurred during processing\n",
        "all_processed_dataframes = [\n",
        "    df for df in all_processed_dataframes if df is not None]\n",
        "\n",
        "# Concatenate all processed files into single dataframe\n",
        "US_combined = pd.concat(all_processed_dataframes, ignore_index=True)\n",
        "\n",
        "# Confirm\n",
        "#print(US_combined.info())\n",
        "#print(US_combined.head())"
      ],
      "metadata": {
        "id": "LN4oYN6UiPiT"
      },
      "id": "LN4oYN6UiPiT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Clean MEDSL data"
      ],
      "metadata": {
        "id": "8szuOb6tuQy_"
      },
      "id": "8szuOb6tuQy_"
    },
    {
      "cell_type": "code",
      "source": [
        "# Fill missing values with 'THIRD'\n",
        "US_combined.loc[:, 'PARTY'] = US_combined['PARTY'].fillna('THIRD')\n",
        "\n",
        "# Create list of parties to rename\n",
        "print(sorted(US_combined['PARTY'].unique()))"
      ],
      "metadata": {
        "id": "C17J4CPBTpTj"
      },
      "id": "C17J4CPBTpTj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6f5ae6da-c471-4027-8073-665b6ea95549",
      "metadata": {
        "id": "6f5ae6da-c471-4027-8073-665b6ea95549",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# Define other parties to replace with 'THIRD' (remove DEMOCRAT and REPUBLICAN from 'US_combined' output)\n",
        "other_parties = [\n",
        "    'ALLIANCE', 'ALLIANCE PARTY', 'AMERICAN', 'AMERICAN CONSTITUTION', 'AMERICAN SHOPPING', 'AMERICAN SOLIDARITY', 'APPROVAL VOTING', 'BECOMING ONE NATION', 'BIRTHDAY', 'BLANK', 'BOILING FROG', 'BREAD AND ROSES', 'BULL MOOSE', 'C.U.P', 'CONSTITUTION', 'CONSTITUTION PARTY', 'CUP', 'FREEDOM AND PROSPERITY', 'GENEALOGY KNOW YOUR FAMILY HISTORY', 'GREEN', 'GREEN INDEPENDENT', 'GREEN-RAINBOW', 'GRUMPY OLD PATRIOTS', 'INDEPENDENCE', 'INDEPENDENCE ALLIANCE', 'INDEPENDENT', 'INDEPENDENT AMERICAN', 'LIBERTARIAN', 'LIBERTY UNION', 'LIFE', 'LIFE LIBERTY CONSTITUTION', 'NATURAL LAW PARTY', 'NONE', 'NONPARTISAN', 'OREGON PROGRESSIVE', 'OTHER', 'PACIFIC GREEN', 'PARTY FOR SOCIALISM AND LIBERATION', 'PROGRESSIVE', 'PROHIBITION', 'REFORM', 'SOCIALISM', 'SOCIALISM AND LIBERATION', 'SOCIALIST', 'SOCIALIST EQUALITY', 'SOCIALIST WORKERS', 'STATEWIDE GREEN', 'UNAFFILIATED', 'UNITY', 'UNITY AMERICA', 'UNITY OF COLORADO', 'US TAXPAYERS PARTY']\n",
        "\n",
        "# Replace these other parties with 'THIRD'\n",
        "US_combined['PARTY'] = US_combined['PARTY'].replace(other_parties, 'THIRD')\n",
        "\n",
        "# Tally Presidential votes\n",
        "PRES_votes = (US_combined.groupby('PARTY', as_index=False)['VOTES']\n",
        "    .sum().sort_values(by='VOTES', ascending=False))\n",
        "\n",
        "# Confirm\n",
        "print(US_combined.info())\n",
        "print(US_combined['PARTY'].unique())\n",
        "print(US_combined['PARTY'].value_counts(dropna=False))\n",
        "print(PRES_votes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2f3bb4e7-fbae-403b-af0a-1dc74dc75f8a",
      "metadata": {
        "scrolled": true,
        "id": "2f3bb4e7-fbae-403b-af0a-1dc74dc75f8a",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# Pivot to get vote counts by Party\n",
        "US_transform = (US_combined.groupby(['GEOID', 'PARTY'])['VOTES']\n",
        "    .sum()\n",
        "    .unstack(fill_value=0)\n",
        "    .reset_index())\n",
        "\n",
        "# Rename columns that were the party names after unstacking\n",
        "US_transform = US_transform.rename(columns={\n",
        "    'DEMOCRAT': 'DEM_VOTES',\n",
        "    'REPUBLICAN': 'REP_VOTES',\n",
        "    'THIRD': 'THRD_VOTES'})\n",
        "\n",
        "# Change vote columns to int32\n",
        "vote_cols = ['DEM_VOTES', 'REP_VOTES', 'THRD_VOTES']\n",
        "US_transform[vote_cols] = US_transform[vote_cols].astype('int32')\n",
        "\n",
        "# Confirm\n",
        "#print(US_transform.info())\n",
        "#print(US_transform.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create share of vote feature"
      ],
      "metadata": {
        "id": "pfkaBMaAHa-V"
      },
      "id": "pfkaBMaAHa-V"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6adb69cf-c68d-4197-86a0-4d0cf787fb99",
      "metadata": {
        "scrolled": true,
        "id": "6adb69cf-c68d-4197-86a0-4d0cf787fb99"
      },
      "outputs": [],
      "source": [
        "# Create new working dataframe\n",
        "US_tranfm2 = US_transform.copy()\n",
        "\n",
        "# Compute TOTAL_VOTES, drop any where the sum of all votes = 0\n",
        "US_tranfm2['TOTAL_VOTES'] = US_tranfm2[vote_cols].sum(axis=1).astype('int32')\n",
        "US_tranfm2 = US_tranfm2[US_tranfm2['TOTAL_VOTES'] != 0]\n",
        "\n",
        "# Compute shares of votes\n",
        "US_tranfm2['DEM_SHARE'] = (\n",
        "    (US_tranfm2['DEM_VOTES'] / US_tranfm2['TOTAL_VOTES'])* 100).round(2)\n",
        "US_tranfm2['REP_SHARE'] = (\n",
        "    (US_tranfm2['REP_VOTES'] / US_tranfm2['TOTAL_VOTES'])* 100).round(2)\n",
        "US_tranfm2['THRD_SHARE'] = (\n",
        "    (US_tranfm2['THRD_VOTES'] / US_tranfm2['TOTAL_VOTES'])* 100).round(2)\n",
        "\n",
        "# Confirm\n",
        "#print(US_tranfm2.info())\n",
        "#print(US_tranfm2.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create political leaning feature"
      ],
      "metadata": {
        "id": "tV4-3lQkHnV0"
      },
      "id": "tV4-3lQkHnV0"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4a29ead3"
      },
      "source": [
        "# View values of DEM_SHARE, ensure all >0\n",
        "print(sorted(US_tranfm2['DEM_SHARE'].unique()))"
      ],
      "id": "4a29ead3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "861238c1-9836-410e-af43-2b6a918f9788",
      "metadata": {
        "scrolled": true,
        "id": "861238c1-9836-410e-af43-2b6a918f9788",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# Create new working dataframe\n",
        "US_tranfm3 = US_tranfm2.copy()\n",
        "\n",
        "# Define the political leaning function\n",
        "def determine_win(row):\n",
        "# Only DEM or REP win, only consider their shares for determining lead\n",
        "    shares = {\n",
        "        'DEM': row['DEM_SHARE'],\n",
        "        'REP': row['REP_SHARE']}\n",
        "\n",
        "    # Determine the winning party between DEM and REP\n",
        "    if shares['DEM'] > shares['REP']:\n",
        "        party_win = 1 # Democrat wins = Positive lead for DEM\n",
        "        party_lead = (shares['DEM'] - shares['REP']) / 100\n",
        "    elif shares['REP'] > shares['DEM']: # Corrected from else\n",
        "        party_win = 0 # Republican wins = Negative lead for REP\n",
        "        party_lead = (shares['DEM'] - shares['REP']) / 100\n",
        "    else: # Tie (very unlikely)\n",
        "        party_win = 2\n",
        "        party_lead = 0.0\n",
        "\n",
        "    return party_win, round(party_lead, 2)\n",
        "\n",
        "# Apply function and create two new variables\n",
        "US_tranfm3[['PARTY_WIN', 'PARTY_LEAD']] = US_tranfm3.apply(\n",
        "    determine_win, axis=1).apply(pd.Series)\n",
        "\n",
        "# Convert 'PARTY_WIN' to int\n",
        "US_tranfm3['PARTY_WIN'] = US_tranfm3['PARTY_WIN'].astype('int32')\n",
        "\n",
        "# Confirm\n",
        "print(US_tranfm3.head(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save MEDSL data"
      ],
      "metadata": {
        "id": "u62yAvnWHu4i"
      },
      "id": "u62yAvnWHu4i"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "83768481-ad9d-44d3-b4f4-46a4b3768ab1",
      "metadata": {
        "scrolled": true,
        "id": "83768481-ad9d-44d3-b4f4-46a4b3768ab1",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# Order variables\n",
        "final_cols = ['GEOID', 'TOTAL_VOTES',\n",
        "              'DEM_VOTES', 'DEM_SHARE',\n",
        "              'REP_VOTES', 'REP_SHARE',\n",
        "              'THRD_VOTES', 'THRD_SHARE',\n",
        "              'PARTY_WIN', 'PARTY_LEAD']\n",
        "\n",
        "# Create the tidy dataframe\n",
        "MEDSL_tidy = US_tranfm3[final_cols]\n",
        "\n",
        "MEDSL_tidy.to_csv('MEDSL_tidy.csv', index=False)\n",
        "\n",
        "# Confirm\n",
        "print(MEDSL_tidy.info())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Merge data files"
      ],
      "metadata": {
        "id": "2vanEG68IBcU"
      },
      "id": "2vanEG68IBcU"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import _tidy files here if you do not want to clean the data"
      ],
      "metadata": {
        "id": "i5-7tTinDzML"
      },
      "id": "i5-7tTinDzML"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "# Import here if utilizing the _tidy files\n",
        "DHC_tidy = pd.read_csv('DHC_tidy.csv')\n",
        "PUR_tidy = pd.read_csv('PUR_tidy.csv')\n",
        "MEDSL_tidy = pd.read_csv('MEDSL_tidy.csv')\n",
        "\n",
        "# Confirm\n",
        "print(DHC_tidy.info())\n",
        "print(PUR_tidy.info())\n",
        "print(MEDSL_tidy.info())"
      ],
      "metadata": {
        "id": "O0jpcAOQDylG",
        "collapsed": true
      },
      "id": "O0jpcAOQDylG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "M0qZvjDvCTUB",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# Merge first two files\n",
        "TWO_join = pd.merge(DHC_tidy, PUR_tidy, on='GEOID', how='outer')\n",
        "\n",
        "# Confirm\n",
        "#print(TWO_join.info())"
      ],
      "id": "M0qZvjDvCTUB"
    },
    {
      "cell_type": "code",
      "source": [
        "# Merge with third dataset\n",
        "FULL_DF = pd.merge(TWO_join, MEDSL_tidy, on='GEOID', how='outer')\n",
        "\n",
        "# change GEOID type\n",
        "FULL_DF['GEOID'] = FULL_DF['GEOID'].astype(str)\n",
        "\n",
        "# Confirm\n",
        "#print(FULL_DF.info())"
      ],
      "metadata": {
        "id": "r3_SMWlhcTpi",
        "collapsed": true
      },
      "id": "r3_SMWlhcTpi",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create new working dataframe\n",
        "FULL_transform = FULL_DF.copy()\n",
        "\n",
        "# Split 'Name' into 'County' and 'State'\n",
        "FULL_transform[['County', 'State']] = FULL_transform[\n",
        "    'County'].str.split(', ', expand=True)\n",
        "\n",
        "# Reorder columns to move 'State' to index 1\n",
        "cols = FULL_transform.columns.tolist()\n",
        "cols.insert(1, cols.pop(cols.index('State')))\n",
        "MERGED_DF = FULL_transform[cols]"
      ],
      "metadata": {
        "id": "jQLuxhTtiFzG"
      },
      "id": "jQLuxhTtiFzG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save MERGED datafile"
      ],
      "metadata": {
        "id": "tcE3avVMUVmZ"
      },
      "id": "tcE3avVMUVmZ"
    },
    {
      "cell_type": "code",
      "source": [
        "MERGED_DF.to_csv('MERGED_DF.csv', index=False)"
      ],
      "metadata": {
        "id": "c9t1xr4nUaO4"
      },
      "id": "c9t1xr4nUaO4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exploratory Data Analysis (with MERGED_DF)"
      ],
      "metadata": {
        "id": "Db35tTfshpfU"
      },
      "id": "Db35tTfshpfU"
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup exploration environment\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "%matplotlib inline\n",
        "print('Environment Ready')"
      ],
      "metadata": {
        "id": "vCHwEDBjtOhJ"
      },
      "id": "vCHwEDBjtOhJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d6416223-79d2-4eea-8253-104ed250f502",
      "metadata": {
        "id": "d6416223-79d2-4eea-8253-104ed250f502",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "MERGED_DF = pd.read_csv('MERGED_DF.csv')\n",
        "# ensure GEOID is an object\n",
        "MERGED_DF['GEOID'] = MERGED_DF['GEOID'].astype(str)\n",
        "\n",
        "# Confirm\n",
        "print(MERGED_DF.info())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Basic Information"
      ],
      "metadata": {
        "id": "digFEi-Pf4-E"
      },
      "id": "digFEi-Pf4-E"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a0b7a653"
      },
      "source": [
        "# Display descriptive statistics for numerical columns\n",
        "print('Number of rows:', MERGED_DF.shape[0], '(Number of counties)')\n",
        "print('Number of columns:', MERGED_DF.shape[1])\n",
        "print('\\nMissing Values: None')\n",
        "print(MERGED_DF.isna().sum().sort_values(ascending=False))\n",
        "\n",
        "print('\\nDescriptive Statistics for Numerical Columns:')\n",
        "display(MERGED_DF.describe())\n",
        "\n",
        "# Display value counts for categorical column (PARTY_WIN)\n",
        "print('\\nValue Counts for 'PARTY_WIN' \\n0: Republican Win\\n1: Democrat Win:')\n",
        "display(MERGED_DF['PARTY_WIN'].value_counts())"
      ],
      "id": "a0b7a653",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualizations"
      ],
      "metadata": {
        "id": "6U447_NdnbvO"
      },
      "id": "6U447_NdnbvO"
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize the distribution of the target variable 'PARTY_WIN'\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.countplot(x='PARTY_WIN', data=MERGED_DF)\n",
        "plt.title('Distribution of PARTY_WIN (0: Republican Win, 1: Democrat Win)')\n",
        "plt.xlabel('Party Win')\n",
        "plt.ylabel('Count')\n",
        "plt.xticks([0, 1], ['Republican Win', 'Democrat Win'])\n",
        "plt.show()\n",
        "\n",
        "print('')\n",
        "# Visualize the distribution of 'PARTY_LEAD'\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15,5))\n",
        "sns.histplot(MERGED_DF['DEM_SHARE'], bins=30, kde=True, ax=axes[0], color='blue')\n",
        "axes[0].set_title('Democratic Vote Share')\n",
        "sns.histplot(MERGED_DF['REP_SHARE'], bins=30, kde=True, ax=axes[1], color='red')\n",
        "axes[1].set_title('Republican Vote Share')\n",
        "sns.histplot(MERGED_DF['PARTY_LEAD'], bins=30, kde=True, ax=axes[2], color='purple')\n",
        "axes[2].set_title('Margin of Victory (Party Lead)')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print('')\n",
        "sns.histplot(MERGED_DF['Pop_total'], bins=50, kde=True)\n",
        "plt.title('County Population Distribution')\n",
        "plt.xlabel('Population')\n",
        "plt.ylabel('Number of Counties')\n",
        "plt.show()\n",
        "\n",
        "print('')\n",
        "sns.histplot(MERGED_DF['%Urban_pop'], bins=30, kde=True)\n",
        "plt.title('Urban Population Share by County')\n",
        "plt.xlabel('% Urban Population')\n",
        "plt.ylabel('Number of Counties')\n",
        "plt.show()\n",
        "\n",
        "print('')\n",
        "sns.histplot(MERGED_DF['MED_AGE'], bins=30, kde=True)\n",
        "plt.title('Median Age Distribution')\n",
        "plt.xlabel('Median Age')\n",
        "plt.ylabel('Number of Counties')\n",
        "plt.show()\n",
        "\n",
        "print('')\n",
        "race_cols = ['%RACE_White', '%RACE_Black', '%RACE_Latino', '%RACE_Asian']\n",
        "MERGED_DF[race_cols].plot(kind='box', figsize=(8,6))\n",
        "plt.title('Distribution of Racial Composition by County')\n",
        "plt.ylabel('Percentage')\n",
        "plt.show()\n",
        "\n",
        "print('')\n",
        "sns.scatterplot(x='%OWN_HOME', y='%RENT_HOME', data=MERGED_DF)\n",
        "plt.title('Own vs Rent in Counties')\n",
        "plt.xlabel('% Own Home')\n",
        "plt.ylabel('% Rent Home')\n",
        "plt.show()\n",
        "\n",
        "print('')\n",
        "sns.scatterplot(x='%Urban_pop', y='DEM_SHARE', data=MERGED_DF, alpha=0.6)\n",
        "plt.title('Urban Population vs Democratic Vote Share')\n",
        "plt.show()\n",
        "\n",
        "print('')\n",
        "sns.scatterplot(x='%RACE_White', y='REP_SHARE', data=MERGED_DF, alpha=0.6, color='red')\n",
        "plt.title('% White Population vs Republican Vote Share')\n",
        "plt.show()\n",
        "\n",
        "print('')\n",
        "sns.scatterplot(x='MED_AGE', y='REP_SHARE', data=MERGED_DF, alpha=0.6, color='green')\n",
        "plt.title('Median Age vs Republican Vote Share')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "71v2mSxuncdD"
      },
      "id": "71v2mSxuncdD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Correlation checks on separated groups of features"
      ],
      "metadata": {
        "id": "bu3scl36eF51"
      },
      "id": "bu3scl36eF51"
    },
    {
      "cell_type": "code",
      "source": [
        "#corr_vars = ['Pop_total', 'MED_AGE', '%Urban_pop',\n",
        "#             '%RACE_White', '%RACE_Black', '%RACE_Latino',\n",
        "#             '%OWN_HOME', '%RENT_HOME',\n",
        "#             'DEM_SHARE', 'REP_SHARE', 'PARTY_LEAD']\n",
        "\n",
        "#corr = MERGED_DF[corr_vars].corr()\n",
        "MERGED_num = MERGED_DF.select_dtypes(include=np.number)\n",
        "\n",
        "corr = MERGED_num.corr()\n",
        "\n",
        "plt.figure(figsize=(10,8))\n",
        "sns.heatmap(corr, annot=False, fmt='.2f', cmap='coolwarm', center=0)\n",
        "plt.title('Correlation Heatmap')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "LmiEfl2TP3ZD"
      },
      "id": "LmiEfl2TP3ZD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Analyze Age data"
      ],
      "metadata": {
        "id": "T6sApGow-rrc"
      },
      "id": "T6sApGow-rrc"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d0cb92f9"
      },
      "source": [
        "# Create new working dataframe\n",
        "MERGED_trform = MERGED_DF.copy()\n",
        "\n",
        "# Define column groups for total, male, and female age percentages\n",
        "age_total_cols = [\n",
        "    col for col in MERGED_trform.columns if col.startswith('%TOTAL_')]\n",
        "age_male_cols  = [\n",
        "    col for col in MERGED_trform.columns if col.startswith('%MALE_')]\n",
        "age_female_cols = [\n",
        "    col for col in MERGED_trform.columns if col.startswith('%FEMALE_')]\n",
        "\n",
        "# Combine all percentage age columns and the target variables\n",
        "features_for_age = age_total_cols + age_male_cols + age_female_cols + [\n",
        "    'PARTY_WIN', 'PARTY_LEAD']\n",
        "\n",
        "# Calculate the correlation matrix for the selected features\n",
        "corr_age = MERGED_trform[features_for_age].corr()\n",
        "\n",
        "# Select and display only the correlations with PARTY_WIN and PARTY_LEAD\n",
        "corr_age_subset = corr_age[['PARTY_WIN', 'PARTY_LEAD']].loc[\n",
        "    age_total_cols + age_male_cols + age_female_cols]\n",
        "\n",
        "# Plot heatmap for better visualization of correlations\n",
        "plt.figure(figsize=(10, 15)) # Adjust figure size as needed\n",
        "sns.heatmap(corr_age_subset,\n",
        "            cmap='seismic_r',\n",
        "            annot=True, fmt='.2f',\n",
        "            vmin=-1, vmax=1)\n",
        "plt.title('Correlation Heatmap: Percentage Age Groups vs PARTY_WIN and PARTY_LEAD')\n",
        "plt.yticks(rotation=0)\n",
        "plt.show()"
      ],
      "id": "d0cb92f9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### With a clear divergence around age 55, compare 2 vs 3 age groupings  \n",
        "- yng, mid, old: Looks to break groups into pos, neutral (between -0.1 and 0.1), neg  \n",
        "- young, older: Looks to break age groups into positive and negative only  "
      ],
      "metadata": {
        "id": "StFYlTn8-4pI"
      },
      "id": "StFYlTn8-4pI"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4506464d",
        "collapsed": true
      },
      "source": [
        "# Define lists of age columns for young, middle (cutoff is |0.1|), and old\n",
        "age_male_yng = [col for col in MERGED_trform.columns if col.startswith('%MALE_') and any(age in col for age in ['18_19', '20_24', '25_29', '30_34', '35_39'])]\n",
        "age_male_mid = [col for col in MERGED_trform.columns if col.startswith('%MALE_') and any(age in col for age in ['40_44', '45_49', '50_54'])]\n",
        "age_male_old = [col for col in MERGED_trform.columns if col.startswith('%MALE_') and any(age in col for age in ['55_59', '60_64', '65_69', '70_74', '75_79', '80_84', '85+'])]\n",
        "age_female_yng = [col for col in MERGED_trform.columns if col.startswith('%FEMALE_') and any(age in col for age in ['18_19', '20_24', '25_29', '30_34', '35_39', '40_44'])]\n",
        "age_female_mid = [col for col in MERGED_trform.columns if col.startswith('%FEMALE_') and any(age in col for age in ['45_49', '50_54'])]\n",
        "age_female_old = [col for col in MERGED_trform.columns if col.startswith('%FEMALE_') and any(age in col for age in ['55_59', '60_64', '65_69', '70_74', '75_79', '80_84', '85+'])]\n",
        "\n",
        "# Define lists of age columns for young (cutoff is 0) and older\n",
        "age_male_young = [col for col in MERGED_trform.columns if col.startswith('%MALE_') and any(age in col for age in ['18_19', '20_24', '25_29', '30_34', '35_39', '40_44', '45_49'])]\n",
        "age_male_older = [col for col in MERGED_trform.columns if col.startswith('%MALE_') and any(age in col for age in ['50_54', '55_59', '60_64', '65_69', '70_74', '75_79', '80_84', '85+'])]\n",
        "age_female_young = [col for col in MERGED_trform.columns if col.startswith('%FEMALE_') and any(age in col for age in ['18_19', '20_24', '25_29', '30_34', '35_39', '40_44', '45_49', '50_54'])]\n",
        "age_female_older = [col for col in MERGED_trform.columns if col.startswith('%FEMALE_') and any(age in col for age in ['55_59', '60_64', '65_69', '70_74', '75_79', '80_84', '85+'])]\n",
        "\n",
        "# Calculate the new aggregated percentage age groups\n",
        "MERGED_trform['%AGE_MALE_YNG'] = MERGED_trform[age_male_yng].sum(axis=1).round(2)\n",
        "MERGED_trform['%AGE_MALE_MID'] = MERGED_trform[age_male_mid].sum(axis=1).round(2)\n",
        "MERGED_trform['%AGE_MALE_OLD'] = MERGED_trform[age_male_old].sum(axis=1).round(2)\n",
        "MERGED_trform['%AGE_MALE_YOUNG'] = MERGED_trform[age_male_young].sum(axis=1).round(2)\n",
        "MERGED_trform['%AGE_MALE_OLDER'] = MERGED_trform[age_male_older].sum(axis=1).round(2)\n",
        "\n",
        "MERGED_trform['%AGE_FEMALE_YNG'] = MERGED_trform[age_female_yng].sum(axis=1).round(2)\n",
        "MERGED_trform['%AGE_FEMALE_MID'] = MERGED_trform[age_female_mid].sum(axis=1).round(2)\n",
        "MERGED_trform['%AGE_FEMALE_OLD'] = MERGED_trform[age_female_old].sum(axis=1).round(2)\n",
        "MERGED_trform['%AGE_FEMALE_YOUNG'] = MERGED_trform[age_female_young].sum(axis=1).round(2)\n",
        "MERGED_trform['%AGE_FEMALE_OLDER'] = MERGED_trform[age_female_older].sum(axis=1).round(2)\n",
        "\n",
        "# Confirm\n",
        "print(MERGED_trform.info())\n",
        "print(MERGED_trform.head())"
      ],
      "id": "4506464d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Analyze race groups"
      ],
      "metadata": {
        "id": "7iSBoE5IFpZ2"
      },
      "id": "7iSBoE5IFpZ2"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "61a03ee5"
      },
      "source": [
        "# Define the list of race percentage columns\n",
        "race_cols = [col for col in MERGED_trform.columns if col.startswith('%RACE_')]\n",
        "\n",
        "# Combine race percentage columns and the target variables\n",
        "features_for_race = race_cols + ['PARTY_WIN', 'PARTY_LEAD']\n",
        "\n",
        "# Calculate the correlation matrix\n",
        "corr_race = MERGED_trform[features_for_race].corr()\n",
        "\n",
        "# Select and display only the correlations with PARTY_WIN and PARTY_LEAD\n",
        "corr_race_subset = corr_race[['PARTY_WIN', 'PARTY_LEAD']].loc[race_cols]\n",
        "\n",
        "# Display the correlations\n",
        "print('Correlation of Race/Ethnic Group Percentages with PARTY_WIN and PARTY_LEAD:')\n",
        "display(corr_race_subset.sort_values(by='PARTY_LEAD', key=abs, ascending=False))\n",
        "\n",
        "# Plot heatmap for better visualization\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(corr_race_subset, cmap='seismic_r', annot=True, fmt='.2f', vmin=-1, vmax=1)\n",
        "plt.title('Correlation Heatmap: Race/Ethnic Group Percentages vs PARTY_WIN and PARTY_LEAD')\n",
        "plt.yticks(rotation=0)\n",
        "plt.show()"
      ],
      "id": "61a03ee5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "With clear racial differences, I will try two variations of race groups  \n",
        "- White and Non-White  \n",
        "- White, strong political lean, more neutral lean"
      ],
      "metadata": {
        "id": "emS9onA2FBET"
      },
      "id": "emS9onA2FBET"
    },
    {
      "cell_type": "code",
      "source": [
        "# Define lists to compare 2 groups: non-whites or with a cutoff of |0.2|\n",
        "RACE_NonWhite = [col for col in MERGED_trform.columns if col.startswith('%RACE_') and any(race in col for race in ['Asian', 'Black', 'Other', 'Latino', 'Native', 'HI_PI', 'Mixed'])]\n",
        "RACE_BAO = [col for col in MERGED_trform.columns if col.startswith('%RACE_') and any(race in col for race in ['Black', 'Asian', 'Other'])]\n",
        "RACE_LNHM = [col for col in MERGED_trform.columns if col.startswith('%RACE_') and any(race in col for race in ['Latino', 'Native', 'HI_PI', 'Mixed'])]\n",
        "\n",
        "# Calculate the new aggregated percentage race groups\n",
        "MERGED_trform['%RACE_NonWhite'] = MERGED_trform[RACE_NonWhite].sum(axis=1).round(2)\n",
        "MERGED_trform['%RACE_BAO'] = MERGED_trform[RACE_BAO].sum(axis=1).round(2)\n",
        "MERGED_trform['%RACE_LNHM'] = MERGED_trform[RACE_LNHM].sum(axis=1).round(2)"
      ],
      "metadata": {
        "id": "6UnXrijUFOiU"
      },
      "id": "6UnXrijUFOiU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Analyze relationship groups"
      ],
      "metadata": {
        "id": "lFf97gr-gqR_"
      },
      "id": "lFf97gr-gqR_"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dc56dff3"
      },
      "source": [
        "# Filter for columns starting with '%REL_'\n",
        "rel_cols = [col for col in MERGED_trform.columns if col.startswith('%REL_')]\n",
        "\n",
        "# Calculate the correlation of these columns with PARTY_WIN and PARTY_LEAD\n",
        "corr_rel = MERGED_trform[rel_cols + ['PARTY_WIN', 'PARTY_LEAD']].corr()\n",
        "\n",
        "# Select and display only the correlations with PARTY_WIN and PARTY_LEAD\n",
        "corr_rel_subset = corr_rel[['PARTY_WIN', 'PARTY_LEAD']].loc[rel_cols]\n",
        "\n",
        "# Display the correlations\n",
        "print('Correlation of Relationship Variables with PARTY_WIN and PARTY_LEAD:')\n",
        "display(corr_rel_subset.sort_values(by='PARTY_LEAD', key=abs, ascending=False))\n",
        "\n",
        "# Optional: Visualize correlations as a heatmap\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(corr_rel_subset,\n",
        "            cmap='seismic_r',\n",
        "            annot=True, fmt='.2f',\n",
        "            vmin=-1, vmax=1)\n",
        "plt.title('Correlation Heatmap: Relationship Variables vs PARTY_WIN and PARTY_LEAD')\n",
        "plt.yticks(rotation=0)\n",
        "plt.show()"
      ],
      "id": "dc56dff3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Analyze household groups"
      ],
      "metadata": {
        "id": "Yodje20Rg4cx"
      },
      "id": "Yodje20Rg4cx"
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter for columns starting with '%HH_'\n",
        "hh_cols = [col for col in MERGED_trform.columns if col.startswith('%HH_')]\n",
        "\n",
        "# Add own and urban columns\n",
        "hh_cols.extend(['%OWN_HOME', '%Urban_pop'])\n",
        "\n",
        "# Correlate these columns with PARTY_WIN and PARTY_LEAD\n",
        "corr_hh = MERGED_trform[hh_cols + ['PARTY_WIN', 'PARTY_LEAD']].corr()\n",
        "\n",
        "# Select and display only the correlations with PARTY_WIN and PARTY_LEAD\n",
        "corr_hh_subset = corr_hh[['PARTY_WIN', 'PARTY_LEAD']].loc[hh_cols]\n",
        "\n",
        "# Display the correlations\n",
        "print('Correlation of Household, Ownership, and Urban Variables with PARTY_WIN and PARTY_LEAD:')\n",
        "display(corr_hh_subset.sort_values(by='PARTY_LEAD', key=abs, ascending=False))\n",
        "\n",
        "# Optional: Visualize correlations as a heatmap\n",
        "plt.figure(figsize=(8, 10)) # Adjusted figure size\n",
        "sns.heatmap(corr_hh_subset,\n",
        "            cmap='seismic_r',\n",
        "            annot=True, fmt='.2f',\n",
        "            vmin=-1, vmax=1)\n",
        "plt.title('Correlation Heatmap: Household, Ownership, and Urban Variables vs PARTY_WIN and PARTY_LEAD')\n",
        "plt.yticks(rotation=0)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "OpxhE7HlN6D3"
      },
      "id": "OpxhE7HlN6D3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save VOTE_DF"
      ],
      "metadata": {
        "id": "XQzEWtOiy6hj"
      },
      "id": "XQzEWtOiy6hj"
    },
    {
      "cell_type": "code",
      "source": [
        "# List columns to keep (drop HH_totals, only keep M_total and F_total as ref)\n",
        "columns_to_keep = [\n",
        "    'GEOID', 'Male_total', 'Female_total', '%AGE_MALE_YNG', '%AGE_MALE_MID', '%AGE_MALE_OLD', '%AGE_MALE_YOUNG', '%AGE_MALE_OLDER', '%AGE_FEMALE_YNG', '%AGE_FEMALE_MID', '%AGE_FEMALE_OLD', '%AGE_FEMALE_YOUNG', '%AGE_FEMALE_OLDER', '%RACE_White', '%RACE_Black', '%RACE_Latino', '%RACE_Native', '%RACE_Asian', '%RACE_HI_PI', '%RACE_Other', '%RACE_Mixed', '%RACE_NonWhite', '%RACE_BAO', '%RACE_LNHM', '%REL_OP_SEX_MAR', '%REL_OP_SEX_UNMAR', '%REL_S_SEX_MAR', '%REL_S_SEX_UNMAR', '%REL_W_RELATIVES', '%REL_NON_REL', '%REL_MALE_JAILED', '%REL_FEMALE_JAILED', '%REL_MALE_GRP_DORM', '%REL_FEMALE_GRP_DORM', '%HH_MARRIED', '%HH_MAR_W_KIDS','%HH_NOT_MAR',  '%HH_NOT_MAR_W_KIDS', '%HH_MALE_ALONE', '%HH_MALE_65+', '%HH_MALE_W_KIDS', '%HH_FEMALE_ALONE', '%HH_FEMALE_65+', '%HH_FEMALE_W_KIDS', '%OWN_HOME', '%Urban_pop', 'PARTY_WIN', 'PARTY_LEAD']\n",
        "\n",
        "# Create VOTE dataframe\n",
        "VOTE_DF = MERGED_trform[columns_to_keep].copy()\n",
        "\n",
        "VOTE_DF.to_csv('VOTE_DF.csv', index=False)"
      ],
      "metadata": {
        "id": "GcrFAQVraqCb",
        "collapsed": true
      },
      "id": "GcrFAQVraqCb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## EDA complete; dataframe cleaned, merged, transformed, partially reduced, and ready for analysis"
      ],
      "metadata": {
        "id": "DVfNdSAXw94R"
      },
      "id": "DVfNdSAXw94R"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature analysis (with VOTE_DF)"
      ],
      "metadata": {
        "id": "02vUwlYly7Bl"
      },
      "id": "02vUwlYly7Bl"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import numpy.typing as npt\n",
        "import statsmodels.api as sm\n",
        "from typing import Literal, Tuple, Union\n",
        "from scipy.stats import shapiro, mannwhitneyu, rankdata, norm\n",
        "from sklearn.linear_model import LogisticRegression, LassoCV\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, mean_squared_error, r2_score, mean_absolute_error, precision_score, recall_score, f1_score, roc_auc_score\n",
        "from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.feature_selection import RFECV\n",
        "from sklearn.inspection import permutation_importance, PartialDependenceDisplay\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "from statsmodels.multivariate.manova import MANOVA\n",
        "from collections import Counter\n",
        "%matplotlib inline\n",
        "print('Environment Ready')"
      ],
      "metadata": {
        "id": "SkOYUXVGtj34"
      },
      "id": "SkOYUXVGtj34",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import VOTE_DF file here for analysis of features\n",
        "(Looked at feature interactions as well, but opted to keep simple as few improved modeling)"
      ],
      "metadata": {
        "id": "AiJlPNEASeb_"
      },
      "id": "AiJlPNEASeb_"
    },
    {
      "cell_type": "code",
      "source": [
        "VOTE_DF = pd.read_csv('VOTE_DF.csv')\n",
        "# ensure GEOID is an object\n",
        "VOTE_DF['GEOID'] = VOTE_DF['GEOID'].astype(str)\n",
        "\n",
        "# Inspect\n",
        "#print(VOTE_DF.info())"
      ],
      "metadata": {
        "id": "-3Mwukkuy-ex"
      },
      "id": "-3Mwukkuy-ex",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Variance check"
      ],
      "metadata": {
        "id": "fzazFZcGgG69"
      },
      "id": "fzazFZcGgG69"
    },
    {
      "cell_type": "code",
      "source": [
        "# Select numerical columns\n",
        "VOTE_num = VOTE_DF.select_dtypes(include=np.number)\n",
        "\n",
        "variances = VOTE_num.var()\n",
        "\n",
        "# Sort variances in descending order\n",
        "var_sorted = variances.sort_values(ascending=True)\n",
        "\n",
        "# Set pandas display option to show float format\n",
        "pd.set_option('display.float_format', '{:.2f}'.format)\n",
        "\n",
        "# Confirm (Consider dropping features with low variance >0.05)\n",
        "print('\\nFeature Variances (sorted):')\n",
        "print(var_sorted.head(20))"
      ],
      "metadata": {
        "collapsed": true,
        "id": "-MYZcsF3-A2l"
      },
      "execution_count": null,
      "outputs": [],
      "id": "-MYZcsF3-A2l"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0ddeafb"
      },
      "source": [
        "## Compute VIF for VOTE_DF"
      ],
      "id": "b0ddeafb"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "98e045b4"
      },
      "source": [
        "# Remove independent variables\n",
        "VOTE_features = VOTE_num.drop(columns=['PARTY_WIN', 'PARTY_LEAD'])\n",
        "\n",
        "# Add required constant\n",
        "VOTE_features = sm.add_constant(VOTE_features)\n",
        "\n",
        "# Compute Variance Inflation Factor for each feature\n",
        "VOTE_VIF = pd.DataFrame()\n",
        "VOTE_VIF['Feature'] = VOTE_features.columns\n",
        "# Compute VIF, handling potential inf values which occur with perfect multicollinearity\n",
        "VOTE_VIF['VIF'] = [variance_inflation_factor(VOTE_features.values, i) for i in range(VOTE_features.shape[1])]\n",
        "\n",
        "# Sort by VIF in descending order for easier analysis\n",
        "VOTE_VIF = VOTE_VIF.sort_values(by='VIF', ascending=False)\n",
        "\n",
        "# Set display to show float format\n",
        "pd.set_option('display.float_format', '{:.2f}'.format)\n",
        "\n",
        "print('VIF for VOTE_DF:')\n",
        "display(VOTE_VIF)"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "98e045b4"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Correlations"
      ],
      "metadata": {
        "id": "x0ZNe24WqY2G"
      },
      "id": "x0ZNe24WqY2G"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "946999ad"
      },
      "source": [
        "## Pearson Correlation Matrix"
      ],
      "id": "946999ad"
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "20233d7d"
      },
      "source": [
        "# Compute Pearson correlation matrix\n",
        "pearson_corr_matrix = VOTE_DF.corr(method='pearson')\n",
        "\n",
        "# Display the correlations\n",
        "print('Pearson Correlation Matrix:')\n",
        "display(pearson_corr_matrix)\n",
        "\n",
        "# Sort Pearson correlations with PARTY_WIN and PARTY_LEAD\n",
        "pearson_corr_win = pearson_corr_matrix['PARTY_WIN'].sort_values(ascending=False)\n",
        "pearson_corr_lead = pearson_corr_matrix['PARTY_LEAD'].sort_values(ascending=False)"
      ],
      "id": "20233d7d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2bf3b66a"
      },
      "source": [
        "## Spearman Correlation Matrix"
      ],
      "id": "2bf3b66a"
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "a6cc88d6"
      },
      "source": [
        "# Compute Spearman correlation matrix\n",
        "spearman_corr_matrix = VOTE_DF.corr(method='spearman')\n",
        "\n",
        "# Display the correlations\n",
        "print('\\nSpearman Correlation Matrix:')\n",
        "display(spearman_corr_matrix)\n",
        "\n",
        "# Sort and store Spearman correlation results\n",
        "spearman_corr_win = spearman_corr_matrix['PARTY_WIN'].sort_values(ascending=False)\n",
        "spearman_corr_lead = spearman_corr_matrix['PARTY_LEAD'].sort_values(ascending=False)"
      ],
      "id": "a6cc88d6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chatterjee's Correlation  \n",
        "In 2020, a paper titled 'A New Coefficient of Correlation' introduced a new coefficient measure ξ (“Xi”) which measures how much the dependent variable is a function of the independent. The result equals 0 if the two variables are independent and will be closer to 1 as the relationship strengthens. Also includes some theoretical properties that allow for hypothesis testing prior to making assumptions about the data.  \n",
        "\n",
        "Along with the article, the R package 'XICOR' was released which contains the function xicor() which calculates ξ when X and Y vectors or matrices are provided (provides p-values for hypothesis testing).\n",
        "\n",
        "S. Chatterjee, *A New Coefficient of Correlation* (2020), Journal of the American Statistical Association.\n",
        "https://doi.org/10.48550/arXiv.1909.10140\n",
        "\n",
        "The below code is a python xicor function based on one written by Tim Sumner https://medium.com/data-science/a-new-coefficient-of-correlation-64ae4f260310"
      ],
      "metadata": {
        "id": "UyZcNq2_fayo"
      },
      "id": "UyZcNq2_fayo"
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute Chatterjee's Correlation\n",
        "def xicor(X, Y, ties='auto', return_p=True):\n",
        "    np.random.seed(1)\n",
        "    X = np.asarray(X)\n",
        "    Y = np.asarray(Y)\n",
        "    Y_sorted = Y[np.argsort(X)]\n",
        "    n = len(X)\n",
        "\n",
        "    if ties == 'auto':\n",
        "        ties = len(np.unique(Y)) < n\n",
        "\n",
        "    if ties:\n",
        "        r = rankdata(Y_sorted, method='ordinal')\n",
        "        l = rankdata(Y_sorted, method='max')\n",
        "        xi = 1 - n * np.sum(np.abs(np.diff(r))) / (2 * np.sum(l * (n - l)))\n",
        "    else:\n",
        "        r = rankdata(Y_sorted, method='ordinal')\n",
        "        xi = 1 - 3 * np.sum(np.abs(np.diff(r))) / (n**2 - 1)\n",
        "\n",
        "# p-value approximation\n",
        "    p_value = norm.sf(xi, scale=2/5/np.sqrt(n))\n",
        "\n",
        "    if return_p:\n",
        "        return xi, p_value\n",
        "    else:\n",
        "        return xi\n",
        "\n",
        "# Define the independent and dependent variables\n",
        "features = [col for col in VOTE_DF.columns if col not in [\n",
        "    'PARTY_WIN', 'PARTY_LEAD',\n",
        "    'Male_total', 'Female_total']]\n",
        "\n",
        "target_win = VOTE_DF['PARTY_WIN']\n",
        "target_lead = VOTE_DF['PARTY_LEAD']\n",
        "\n",
        "# Store xicor results\n",
        "xicor_results_win = {}\n",
        "xicor_results_lead = {}\n",
        "\n",
        "# Compute xicor for each feature against PARTY_WIN\n",
        "for feature in features:\n",
        "    x_data = VOTE_DF[feature]\n",
        "    xi_stat, xi_p_value = xicor(x_data, target_win)\n",
        "    xicor_results_win[feature] = {'statistic': xi_stat, 'p_value': xi_p_value}\n",
        "    #print(f'{feature}: Statistic={xi_stat:.2f}, P-value={xi_p_value:.2f}')\n",
        "\n",
        "# Compute xicor for each feature against PARTY_LEAD\n",
        "for feature in features:\n",
        "    x_data = VOTE_DF[feature]\n",
        "    xi_stat, xi_p_value = xicor(x_data, target_lead)\n",
        "    xicor_results_lead[feature] = {'statistic': xi_stat, 'p_value': xi_p_value}\n",
        "    #print(f'{feature}: Statistic={xi_stat:.2f}, P-value={xi_p_value:.2f}')\n",
        "\n",
        "# Store Chatterjee correlation results\n",
        "xi_corr_win = pd.DataFrame.from_dict(xicor_results_win, orient='index')\n",
        "xi_corr_lead = pd.DataFrame.from_dict(xicor_results_lead, orient='index')"
      ],
      "metadata": {
        "id": "jN_NUj06zQc1"
      },
      "id": "jN_NUj06zQc1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Compare Correlation Coefficients"
      ],
      "metadata": {
        "id": "GWZ-52yYuVVY"
      },
      "id": "GWZ-52yYuVVY"
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine all correlation results into a single DataFrame\n",
        "correlation_comparison = pd.concat([\n",
        "    xi_corr_lead['statistic'].rename('Xi_Corr_LEAD'),\n",
        "    xi_corr_win['statistic'].rename('Xi_Corr_WIN'),\n",
        "    pearson_corr_lead,\n",
        "    pearson_corr_win,\n",
        "    spearman_corr_lead,\n",
        "    spearman_corr_win,\n",
        "], axis=1)\n",
        "\n",
        "# Remove the target variables  if included\n",
        "correlation_comparison.drop(['PARTY_WIN', 'PARTY_LEAD'], errors='ignore', inplace=True)\n",
        "\n",
        "# Rename features\n",
        "Correlation_Table = correlation_comparison.rename(columns={\n",
        "    'Pearson_Corr_PARTY_LEAD': 'Pearson_LEAD',\n",
        "    'Pearson_Corr_PARTY_WIN': 'Pearson_WIN',\n",
        "    'Spearman_Corr_PARTY_LEAD': 'Spearman_LEAD',\n",
        "    'Spearman_Corr_PARTY_WIN': 'Spearman_WIN'})\n",
        "\n",
        "# Display all correlations\n",
        "print('Comparison of Xi, Pearson, and Spearman Correlations:')\n",
        "display(Correlation_Table.round(4).sort_values(by='Xi_Corr_LEAD', ascending=False))"
      ],
      "metadata": {
        "id": "TeAT4snMuSuQ"
      },
      "id": "TeAT4snMuSuQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6d525c65"
      },
      "source": [
        "# Statistical test (Test for normality first)  \n"
      ],
      "id": "6d525c65"
    },
    {
      "cell_type": "code",
      "source": [
        "# Separate the dataframe into two groups based on PARTY_WIN\n",
        "group_Republican = VOTE_num[VOTE_num['PARTY_WIN'] == 0]\n",
        "group_Democrat = VOTE_num[VOTE_num['PARTY_WIN'] == 1]\n",
        "\n",
        "features_for_norm = VOTE_num.columns.tolist()\n",
        "features_for_norm.remove('PARTY_WIN')\n",
        "\n",
        "normality_results = {}\n",
        "\n",
        "for feature in features_for_norm:\n",
        "    data1 = group_Republican[feature]\n",
        "    data2 = group_Democrat[feature]\n",
        "\n",
        "    if len(data1) > 2 and len(data2) > 2:\n",
        "        stat1, p_norm1 = shapiro(data1)\n",
        "        stat2, p_norm2 = shapiro(data2)\n",
        "\n",
        "        normality_results[feature] = {\n",
        "            'Rep_p': f'{p_norm1:.2f}',\n",
        "            'Dem_p': f'{p_norm2:.2f}'}\n",
        "    else:\n",
        "        normality_results[feature] = {\n",
        "            'Rep_p': None,\n",
        "            'Dem_p': None}\n",
        "\n",
        "# Convert to DataFrame\n",
        "normality_df = pd.DataFrame(normality_results).T\n",
        "\n",
        "# Confirm (Normality will be defined as above a threshhold of 0.05)\n",
        "print(normality_df)"
      ],
      "metadata": {
        "id": "PB0iFHm4Gbl4"
      },
      "id": "PB0iFHm4Gbl4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Almost every feature is way below 0.05 in both groups: normality is violated with one exception: Will not use T-test.\n",
        "\n",
        "## Run Mann-Whitney U Test"
      ],
      "metadata": {
        "id": "hxMHyq_zIdAs"
      },
      "id": "hxMHyq_zIdAs"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0988c1d4"
      },
      "source": [
        "mannwhit_results = []\n",
        "\n",
        "for feature in features_for_norm:\n",
        "    if feature == 'PARTY_LEAD':\n",
        "        continue\n",
        "\n",
        "    data1 = group_Republican[feature]\n",
        "    data2 = group_Democrat[feature]\n",
        "\n",
        "    if len(data1) < 2 or len(data2) < 2:\n",
        "        continue\n",
        "\n",
        "    U_stat, p_value = mannwhitneyu(data1, data2, alternative='two-sided')\n",
        "\n",
        "    if p_value < 0.05:\n",
        "        mannwhit_results.append({\n",
        "            'Feature': feature,\n",
        "            'DEM_median': data2.median(),\n",
        "            'REP_median': data1.median(),\n",
        "            'U_stat': U_stat,\n",
        "            'p_value': p_value,\n",
        "            'n_dem': len(data2),\n",
        "            'n_rep': len(data1)})\n",
        "\n",
        "mannwhit_df = pd.DataFrame(mannwhit_results)\n",
        "\n",
        "# Derive additional stats\n",
        "mannwhit_df['diff_median'] = mannwhit_df['DEM_median'] - mannwhit_df['REP_median']\n",
        "\n",
        "mannwhit_df['R_biserial'] = 1 - (2 * mannwhit_df['U_stat'] / (\n",
        "                            mannwhit_df['n_dem'] * mannwhit_df['n_rep']))\n",
        "\n",
        "mannwhit_df['Cohens_d'] = (2 * mannwhit_df['R_biserial']\n",
        "                          ) / np.sqrt(1 - mannwhit_df['R_biserial']**2)\n",
        "\n",
        "# Add qualitative labels\n",
        "def label_effect_size(d):\n",
        "    d = abs(d)\n",
        "    if d < 0.2:\n",
        "        return 'Negligible'\n",
        "    elif d < 0.5:\n",
        "        return 'Small'\n",
        "    elif d < 0.8:\n",
        "        return 'Medium'\n",
        "    else:\n",
        "        return 'Large'\n",
        "\n",
        "mannwhit_df['Effect_size'] = mannwhit_df['Cohens_d'].astype(float).apply(label_effect_size)\n",
        "\n",
        "# Reorder columns for priority in table (consider dropping n_ features)\n",
        "cols = mannwhit_df.columns.tolist()\n",
        "cols.insert(3, cols.pop(cols.index('diff_median')))\n",
        "cols.insert(5, cols.pop(cols.index('Cohens_d')))\n",
        "cols.insert(6, cols.pop(cols.index('Effect_size')))\n",
        "cols.insert(7, cols.pop(cols.index('R_biserial')))\n",
        "mannwhit_df = mannwhit_df[cols]\n",
        "\n",
        "# Format after sorting\n",
        "mannwhit_df['DEM_median'] = mannwhit_df['DEM_median'].map(lambda x: f'{x:.2f}')\n",
        "mannwhit_df['REP_median'] = mannwhit_df['REP_median'].map(lambda x: f'{x:.2f}')\n",
        "mannwhit_df['diff_median'] = mannwhit_df['diff_median'].map(lambda x: f'{x:.2f}')\n",
        "mannwhit_df['Cohens_d'] = mannwhit_df['Cohens_d'].map(lambda x: f'{x:.2f}')\n",
        "mannwhit_df['R_biserial'] = mannwhit_df['R_biserial'].map(lambda x: f'{x:.2f}')\n",
        "mannwhit_df['p_value'] = mannwhit_df['p_value'].map(lambda x: f'{x:.2f}')\n",
        "\n",
        "# Confirm\n",
        "display(mannwhit_df.sort_values(by='Cohens_d', ascending=False))"
      ],
      "id": "0988c1d4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature importance"
      ],
      "metadata": {
        "id": "s0z6-6WJqrFL"
      },
      "id": "s0z6-6WJqrFL"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4a180b4"
      },
      "source": [
        "## Feature Importance for PARTY_WIN from Logistic Regression"
      ],
      "id": "e4a180b4"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "072737b3",
        "collapsed": true
      },
      "source": [
        "# Define the features to exclude based on p-values\n",
        "# Could drop Same_Sex features, but not ready to drop yet\n",
        "features_to_exclude = ['']\n",
        "\n",
        "# Select features for logistic regression, excluding the specified ones\n",
        "features_for_logit = [col for col in VOTE_DF.columns if col not in features_to_exclude + ['GEOID', 'Male_total', 'Female_total', 'PARTY_WIN', 'PARTY_LEAD']]\n",
        "\n",
        "X0 = VOTE_DF[features_for_logit]\n",
        "y0 = VOTE_DF['PARTY_WIN']\n",
        "\n",
        "# Split data into training and testing sets (recommended for model evaluation)\n",
        "X0_train, X0_test, y0_train, y0_test = train_test_split(\n",
        "    X0, y0, test_size=0.2, random_state=1,\n",
        "    stratify=y0) # To maintain class distribution\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X0_train_scaled = scaler.fit_transform(X0_train)\n",
        "X0_test_scaled = scaler.transform(X0_test)\n",
        "\n",
        "# Initialize and train the Logistic Regression model with regularization\n",
        "# Using default L2 penalty and balanced class weight\n",
        "logit_model_sklearn = LogisticRegression(\n",
        "    random_state=1,\n",
        "    class_weight='balanced',\n",
        "    max_iter=1000) # Increased max_iter for convergence\n",
        "logit_model_sklearn.fit(X0_train_scaled, y0_train)\n",
        "\n",
        "# Confirm feature importances from the trained model (coefficients)\n",
        "print('Feature Importance (Coefficients from Regularized Logistic Regression):')\n",
        "logit_feature_importance = pd.Series(\n",
        "    logit_model_sklearn.coef_[0], index=features_for_logit)\n",
        "print(logit_feature_importance.sort_values(ascending=False))\n",
        "\n",
        "# Plot feature importances\n",
        "plt.figure(figsize=(10, 8)) # Adjusted figure size for better readability\n",
        "logit_feature_importance.sort_values().plot(kind='barh')\n",
        "plt.title('Feature Importance from Logistic Regression(WIN)')\n",
        "plt.xlabel('Coefficient Value')\n",
        "plt.ylabel('Feature')\n",
        "plt.show()\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "y0_pred_logit = logit_model_sklearn.predict(X0_test_scaled)\n",
        "\n",
        "print('\\nLogistic Regression Model Evaluation (on test set):')\n",
        "print(f'Accuracy: {accuracy_score(y0_test, y0_pred_logit):.2f}')\n",
        "print('Classification Report:')\n",
        "print(classification_report(y0_test, y0_pred_logit))\n",
        "print('Confusion Matrix:')\n",
        "print(confusion_matrix(y0_test, y0_pred_logit))"
      ],
      "id": "072737b3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature Importance for PARTY_WIN from Decision Tree Classifier"
      ],
      "metadata": {
        "id": "vay-KdHNsueQ"
      },
      "id": "vay-KdHNsueQ"
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the features (X) and the target variable (y)\n",
        "# Exclude the target variables themselves from the features\n",
        "features_for_dtc = [col for col in VOTE_DF.columns if col not in [\n",
        "    'GEOID', 'Male_total', 'Female_total', 'PARTY_WIN', 'PARTY_LEAD']]\n",
        "\n",
        "# Define the features (X) and the target variable (y)\n",
        "X1 = VOTE_DF[features_for_dtc]\n",
        "y1 = VOTE_DF['PARTY_WIN']\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X1_train, X1_test, y1_train, y1_test = train_test_split(\n",
        "    X1, y1, test_size=0.2, random_state=1)\n",
        "\n",
        "# Initialize and train the Decision Tree Regressor model\n",
        "dtc_model = DecisionTreeClassifier(\n",
        "    random_state=1,\n",
        "    max_depth=5,\n",
        "    min_samples_split=20,\n",
        "    min_samples_leaf=10)\n",
        "\n",
        "dtc_model.fit(X1_train, y1_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y1_pred_dtc = dtc_model.predict(X1_test)\n",
        "\n",
        "# Evaluate the model\n",
        "mse = mean_squared_error(y1_test, y1_pred_dtc)\n",
        "rmse = np.sqrt(mse) # Calculate RMSE manually\n",
        "mae = mean_absolute_error(y1_test, y1_pred_dtc)\n",
        "r2 = r2_score(y1_test, y1_pred_dtc)\n",
        "\n",
        "print('Decision Tree Regressor Model Evaluation (on test set):')\n",
        "print(f'Mean Squared Error (MSE): {mse:.2f}')\n",
        "print(f'Root Mean Squared Error (RMSE): {rmse:.2f}')\n",
        "print(f'Mean Absolute Error (MAE): {mae:.2f}')\n",
        "print(f'R-squared (R2): {r2:.2f}')\n",
        "\n",
        "# Plot feature importances from the trained model\n",
        "print('\\nFeature Importance from Decision Tree Regressor:')\n",
        "dtc_feature_importance = pd.Series(dtc_model.feature_importances_, index=features_for_dtc)\n",
        "\n",
        "# Sort and print feature importances\n",
        "print(dtc_feature_importance.sort_values(ascending=False).head(15))\n",
        "\n",
        "# Plot feature importances\n",
        "plt.figure(figsize=(10, 8)) # Adjusted figure size\n",
        "dtc_feature_importance.sort_values().plot(kind='barh')\n",
        "plt.title('Feature Importance from Decision Tree Regressor(WIN)')\n",
        "plt.xlabel('Importance Score')\n",
        "plt.ylabel('Feature')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "JmMVTcvCXKfp",
        "collapsed": true
      },
      "id": "JmMVTcvCXKfp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature Importance for PARTY_LEAD from Decision Tree Regressor"
      ],
      "metadata": {
        "id": "mrMYzNbQXDr7"
      },
      "id": "mrMYzNbQXDr7"
    },
    {
      "cell_type": "code",
      "source": [
        "X2 = VOTE_DF[features_for_dtc]\n",
        "y2 = VOTE_DF['PARTY_LEAD']\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X2_train, X2_test, y2_train, y2_test = train_test_split(\n",
        "    X2, y2, test_size=0.2, random_state=1)\n",
        "\n",
        "# Initialize and train the Decision Tree Regressor model\n",
        "dtr_model = DecisionTreeRegressor(\n",
        "    random_state=1,\n",
        "    max_depth=5,\n",
        "    min_samples_split=20,\n",
        "    min_samples_leaf=10)\n",
        "\n",
        "dtr_model.fit(X2_train, y2_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y2_pred_dtr = dtr_model.predict(X2_test)\n",
        "\n",
        "# Evaluate the model\n",
        "mse = mean_squared_error(y2_test, y2_pred_dtr)\n",
        "rmse = np.sqrt(mse) # Calculate RMSE manually\n",
        "mae = mean_absolute_error(y2_test, y2_pred_dtr)\n",
        "r2 = r2_score(y2_test, y2_pred_dtr)\n",
        "\n",
        "print('Decision Tree Regressor Model Evaluation (on test set):')\n",
        "print(f'Mean Squared Error (MSE): {mse:.2f}')\n",
        "print(f'Root Mean Squared Error (RMSE): {rmse:.2f}')\n",
        "print(f'Mean Absolute Error (MAE): {mae:.2f}')\n",
        "print(f'R-squared (R2): {r2:.2f}')\n",
        "\n",
        "# Get and plot feature importances from the trained model\n",
        "print('\\nFeature Importance from Decision Tree Regressor:')\n",
        "dtr_feature_importance = pd.Series(dtr_model.feature_importances_, index=features_for_dtc)\n",
        "\n",
        "# Sort and print feature importances\n",
        "print(dtr_feature_importance.sort_values(ascending=False).head(20))\n",
        "\n",
        "# Plot feature importances\n",
        "plt.figure(figsize=(10, 8)) # Adjusted figure size\n",
        "dtr_feature_importance.sort_values().plot(kind='barh')\n",
        "plt.title('Feature Importance from Decision Tree Regressor(LEAD)')\n",
        "plt.xlabel('Importance Score')\n",
        "plt.ylabel('Feature')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "UB8RDvm8s4KO",
        "collapsed": true
      },
      "id": "UB8RDvm8s4KO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2c30e6de"
      },
      "source": [
        "## Feature Importance for PARTY_WIN from Random Forest"
      ],
      "id": "2c30e6de"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4469fad4"
      },
      "source": [
        "# Select features for the Random Forest model\n",
        "# We can use the same set of features that worked for the logistic regression.\n",
        "features_for_rf = features_for_logit\n",
        "\n",
        "X3 = VOTE_DF[features_for_rf]\n",
        "y3 = VOTE_DF['PARTY_WIN']\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X3_train, X3_test, y3_train, y3_test = train_test_split(\n",
        "    X3, y3, test_size=0.2, random_state=1, stratify=y3)\n",
        "\n",
        "# Initialize and train the Random Forest Classifier\n",
        "# Use a reasonable number of estimators (n_estimators) and a random state for reproducibility\n",
        "rf_model = RandomForestClassifier(\n",
        "    n_estimators=300,\n",
        "    max_depth=None,        # control/avoid overfitting\n",
        "    min_samples_split=10,  # avoid tiny splits\n",
        "    min_samples_leaf=5,    # smoother trees\n",
        "    random_state=1,\n",
        "    class_weight='balanced')\n",
        "rf_model.fit(X3_train, y3_train)\n",
        "\n",
        "# Get feature importances from the trained model\n",
        "rf_feature_importance = pd.Series(\n",
        "    rf_model.feature_importances_, index=features_for_rf)\n",
        "\n",
        "# Sort and print feature importances\n",
        "print('Feature Importance from Random Forest:')\n",
        "print(rf_feature_importance.sort_values(ascending=False))\n",
        "\n",
        "# Plot feature importances\n",
        "plt.figure(figsize=(10, 6))\n",
        "rf_feature_importance.sort_values().plot(kind='barh')\n",
        "plt.title('Feature Importance from Random Forest(WIN)')\n",
        "plt.xlabel('Importance Score (Mean Decrease in Impurity)')\n",
        "plt.ylabel('Feature')\n",
        "plt.show()\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "y3_pred_rf = rf_model.predict(X3_test)\n",
        "\n",
        "print('\\nRandom Forest Model Evaluation (on test set):')\n",
        "print(f'Accuracy: {accuracy_score(y3_test, y3_pred_rf):.2f}')\n",
        "print('Classification Report:')\n",
        "print(classification_report(y3_test, y3_pred_rf))\n",
        "print('Confusion Matrix:')\n",
        "print(confusion_matrix(y3_test, y3_pred_rf))"
      ],
      "id": "4469fad4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define permutation importance function (with optional cross-validation)"
      ],
      "metadata": {
        "id": "n7YXZMIne-zS"
      },
      "id": "n7YXZMIne-zS"
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute permutation importance (PI) or cross-validated PI (CV-PI)\n",
        "def get_PI(model, X3, y3, cv=False, n_splits=5, n_repeats=10, random_state=1):\n",
        "    '''\n",
        "    Parameters\n",
        "    ----------\n",
        "    model : estimator\n",
        "        Trained model (must support predict).\n",
        "    X : DataFrame\n",
        "        Features used for prediction.\n",
        "    y : Series or array-like\n",
        "        Target values.\n",
        "    cv : bool, default=False\n",
        "        If True, performs cross-validated permutation importance.\n",
        "    n_splits : int, default=5\n",
        "        Number of CV folds (only used if cv=True).\n",
        "    n_repeats : int, default=10\n",
        "        Number of shuffles for permutation importance.\n",
        "    random_state : int, default=1\n",
        "        Random seed for reproducibility.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    importance_df : DataFrame\n",
        "        Feature importances sorted by mean decrease in score.\n",
        "    '''\n",
        "\n",
        "    if not cv:\n",
        "# Standard PI on a single fitted model\n",
        "        result = permutation_importance(model, X3, y3,\n",
        "                                        n_repeats=n_repeats,\n",
        "                                        random_state=random_state,\n",
        "                                        n_jobs=-1)\n",
        "        importance_df = pd.DataFrame({\n",
        "            'Feature': X3.columns,\n",
        "            'Importance Mean': result.importances_mean,\n",
        "            'Importance Std': result.importances_std\n",
        "        }).sort_values(by='Importance Mean', ascending=False)\n",
        "\n",
        "    else:\n",
        "# Cross-validated PI\n",
        "        skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
        "        importances = []\n",
        "\n",
        "        for train_idx, test_idx in skf.split(X3, y3):\n",
        "            X3_train, X3_test = X3.iloc[train_idx], X3.iloc[test_idx]\n",
        "            y3_train, y3_test = y3.iloc[train_idx], y3.iloc[test_idx]\n",
        "\n",
        "            model.fit(X3_train, y3_train)\n",
        "            result = permutation_importance(model, X3_test, y3_test,\n",
        "                                            n_repeats=n_repeats,\n",
        "                                            random_state=random_state,\n",
        "                                            n_jobs=-1)\n",
        "            importances.append(result.importances_mean)\n",
        "\n",
        "        mean_importances = np.mean(importances, axis=0)\n",
        "        std_importances = np.std(importances, axis=0)\n",
        "\n",
        "        importance_df = pd.DataFrame({\n",
        "            'Feature': X3.columns,\n",
        "            'Importance Mean': mean_importances,\n",
        "            'Importance Std': std_importances\n",
        "        }).sort_values(by='Importance Mean', ascending=False)\n",
        "    return importance_df\n",
        "\n",
        "# Confirm\n",
        "pd.set_option('display.max_rows', None)\n",
        "pd.reset_option('display.float_format')\n",
        "\n",
        "RF_PI = get_PI(rf_model, X3_test, y3_test, cv=False)\n",
        "print(RF_PI)\n",
        "\n",
        "# Plot permutation importances\n",
        "plt.figure(figsize=(10, 6))\n",
        "# Convert 'Importance Mean' to numeric before plotting\n",
        "RF_PI['Importance Mean'] = pd.to_numeric(RF_PI['Importance Mean'])\n",
        "RF_PI.sort_values(by='Importance Mean', ascending=True).plot(kind='barh')\n",
        "plt.title('Permutation Importance from Random Forest(WIN)')\n",
        "plt.xlabel('Importance Score (Importance decrease in Mean)')\n",
        "plt.ylabel('Feature')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "tX3KCh5yfLWn"
      },
      "id": "tX3KCh5yfLWn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Run RFECV with Random Forest to confirm best features"
      ],
      "metadata": {
        "id": "Wd1SSWaExjpc"
      },
      "id": "Wd1SSWaExjpc"
    },
    {
      "cell_type": "code",
      "source": [
        "# Utillize X, y, train, test from Logit (X0, y0)\n",
        "# RFECV with Random Forest\n",
        "rf = RandomForestClassifier(n_estimators=500, random_state=1, class_weight='balanced')\n",
        "\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n",
        "selector = RFECV(estimator=rf, step=1, cv=cv, scoring='accuracy', n_jobs=-1)\n",
        "selector.fit(X0_train, y0_train)\n",
        "\n",
        "# Best features\n",
        "best_features = X0.columns[selector.support_].tolist()\n",
        "print('Best feature subset:')\n",
        "print(best_features)\n",
        "\n",
        "# Retrain final model with best features\n",
        "rf_ECV = RandomForestClassifier(n_estimators=500, random_state=1, class_weight='balanced')\n",
        "rf_ECV.fit(X0_train[best_features], y0_train)\n",
        "y_pred_ECV = rf_ECV.predict(X0_test[best_features])\n",
        "\n",
        "print('\\nFinal Model Evaluation with Best Features:')\n",
        "print(f'Accuracy: {accuracy_score(y0_test, y_pred_ECV):.4f}')\n",
        "print('Classification Report:')\n",
        "print(classification_report(y0_test, y_pred_ECV))\n",
        "print('Confusion Matrix:')\n",
        "print(confusion_matrix(y0_test, y_pred_ECV))"
      ],
      "metadata": {
        "id": "ROv9Mv8C7d2_"
      },
      "id": "ROv9Mv8C7d2_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature importance for PARTY_LEAD from Lasso Regression after Cross-validate Alpha"
      ],
      "metadata": {
        "id": "wKZL1KAiwy4W"
      },
      "id": "wKZL1KAiwy4W"
    },
    {
      "cell_type": "code",
      "source": [
        "# Define features and exclude target variables\n",
        "features_for_lasso_cv = [col for col in VOTE_DF.columns if col not in ['GEOID', 'PARTY_WIN', 'PARTY_LEAD']]\n",
        "X4 = VOTE_DF[features_for_lasso_cv]\n",
        "y4 = VOTE_DF['PARTY_LEAD']\n",
        "\n",
        "# Split into train and test sets\n",
        "X4_train, X4_test, y4_train, y4_test = train_test_split(\n",
        "    X4, y4, test_size=0.2, random_state=1)\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X4_train_scaled = scaler.fit_transform(X4_train)\n",
        "X4_test_scaled = scaler.transform(X4_test)\n",
        "\n",
        "# Let LassoCV automatically generates an alpha grid to test\n",
        "lasso_cv_model = LassoCV(cv=5, random_state=1, max_iter=10000)\n",
        "lasso_cv_model.fit(X4_train_scaled, y4_train)\n",
        "\n",
        "# Confirm the optimal alpha found by LassoCV\n",
        "optimal_alpha = lasso_cv_model.alpha_\n",
        "print(f'Optimal alpha found by LassoCV: {optimal_alpha:.4f}')\n",
        "print('')\n",
        "# Plot the MSE as a function of alpha\n",
        "mse_path = lasso_cv_model.mse_path_\n",
        "alphas = lasso_cv_model.alphas_\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(alphas, mse_path, linestyle='-', marker='o')\n",
        "plt.xscale('log') # Often useful to plot alpha on a log scale\n",
        "plt.xlabel('Alpha')\n",
        "plt.ylabel('Mean Squared Error (across folds)')\n",
        "plt.title('Mean Squared Error vs. Alpha during Cross-validation')\n",
        "plt.axvline(optimal_alpha, color='red', linestyle='--', label=f'Optimal Alpha = {optimal_alpha:.4f}')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# List the coefficients with the optimal alpha\n",
        "print('\\nFeature Importance from LassoCV (CV = 0.0002):')\n",
        "feature_importance_lasso_cv = pd.Series(lasso_cv_model.coef_, index=features_for_lasso_cv)\n",
        "print(feature_importance_lasso_cv.sort_values(ascending=False))\n",
        "\n",
        "# Plot feature importances with optimal alpha\n",
        "plt.figure(figsize=(10, 10))\n",
        "feature_importance_lasso_cv.sort_values().plot(kind='barh')\n",
        "plt.title(f'Feature Importance from Lasso Regression(LEAD) with Optimal Alpha = {optimal_alpha:.4f}')\n",
        "plt.xlabel('Coefficient Value')\n",
        "plt.ylabel('Feature')\n",
        "plt.show()\n",
        "\n",
        "# Evaluate the final Lasso model with the optimal alpha on the test set\n",
        "y4_pred_lasso_cv = lasso_cv_model.predict(X4_test_scaled)\n",
        "\n",
        "print('\\nLasso Regression Model Evaluation (with Optimal Alpha):')\n",
        "mse_test = mean_squared_error(y4_test, y4_pred_lasso_cv)\n",
        "rmse_test = np.sqrt(mse_test)\n",
        "r2_test = r2_score(y4_test, y4_pred_lasso_cv)\n",
        "\n",
        "print(f'Mean Squared Error (MSE) on test set: {mse_test:.4f}')\n",
        "print(f'Root Mean Squared Error (RMSE) on test set: {rmse_test:.4f}')\n",
        "print(f'R-squared (R2) on test set: {r2_test:.4f}')"
      ],
      "metadata": {
        "id": "is2-5D_DQUxz",
        "collapsed": true
      },
      "id": "is2-5D_DQUxz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Compare feature importance"
      ],
      "metadata": {
        "id": "zP3rnW8z6VkD"
      },
      "id": "zP3rnW8z6VkD"
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_series(s, keep_sign=True):\n",
        "    '''Normalize feature importance to 0–1 scale, optionally keeping sign.'''\n",
        "    s = s.fillna(0)\n",
        "    if keep_sign:\n",
        "        return s / s.abs().max()  # scale to -1..1, preserving sign\n",
        "    else:\n",
        "        scaler = MinMaxScaler()\n",
        "        return pd.Series(scaler.fit_transform(s.values.reshape(-1, 1)).flatten(), index=s.index)\n",
        "\n",
        "# Collect raw importances into a DataFrame\n",
        "feature_importances = pd.DataFrame({\n",
        "    'LogReg': logit_feature_importance,\n",
        "    'DecTreeClass': dtc_feature_importance,\n",
        "    'DecTreeReg': dtr_feature_importance,\n",
        "    'RandomForest': rf_feature_importance,\n",
        "    'RF_PI': RF_PI.set_index('Feature')['Importance Mean'],  # permutation importance\n",
        "    'Lasso_LogReg': feature_importance_lasso_cv})\n",
        "\n",
        "# Normalize each column (preserving signs)\n",
        "for col in feature_importances.columns:\n",
        "    if col in ['LogReg', 'Lasso_LogReg']:  # signed coefficients\n",
        "        feature_importances[col] = normalize_series(feature_importances[col], keep_sign=True)\n",
        "    else:  # tree-based importances are ≥ 0\n",
        "        feature_importances[col] = normalize_series(feature_importances[col], keep_sign=False)\n",
        "\n",
        "# Compute mean rank or average importance across models\n",
        "feature_importances['Avg_Importance'] = feature_importances.abs().mean(axis=1)\n",
        "\n",
        "# Sort by average importance\n",
        "feature_importances = feature_importances.sort_values(by='Avg_Importance', ascending=False)\n",
        "\n",
        "# Print the top features\n",
        "print('\\nTop 25 Features Across Models (normalized):')\n",
        "display(feature_importances.round(4))"
      ],
      "metadata": {
        "id": "TPFzdRAd-SB7"
      },
      "id": "TPFzdRAd-SB7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RFECV Feature Selection for final model"
      ],
      "metadata": {
        "id": "eBwGovenQL60"
      },
      "id": "eBwGovenQL60"
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare Data, drop reference and overlap features\n",
        "drop_features = [\n",
        "    'Male_total', 'Female_total', # reference only\n",
        "    '%AGE_MALE_YNG', '%AGE_MALE_MID', '%AGE_MALE_OLD', # overlap\n",
        "    '%AGE_FEMALE_YNG', '%AGE_FEMALE_MID', '%AGE_FEMALE_OLD', # overlap\n",
        "    '%RACE_NonWhite', '%RACE_BAO', '%RACE_LNHM'] # overlap\n",
        "\n",
        "VOTE_FULL = VOTE_DF.drop(\n",
        "    columns=drop_features,\n",
        "    errors='ignore')\n",
        "\n",
        "X = VOTE_FULL.drop(\n",
        "    columns=['GEOID', 'PARTY_WIN', 'PARTY_LEAD'])\n",
        "y = VOTE_FULL['PARTY_WIN']\n",
        "\n",
        "# Train/test split\n",
        "XF_train, XF_test, yF_train, yF_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=1, stratify=y)\n",
        "\n",
        "# Recursive Feature Elimination with Cross-Validation\n",
        "rf = RandomForestClassifier(\n",
        "    n_estimators=400, random_state=1, class_weight='balanced')\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n",
        "selector = RFECV(\n",
        "    estimator=rf, step=1, cv=cv, scoring='accuracy', n_jobs=-1)\n",
        "selector.fit(XF_train, yF_train)\n",
        "\n",
        "# Plot accuracy vs. number of features\n",
        "n_features = np.arange(\n",
        "    1, len(selector.cv_results_['mean_test_score']) + 1)\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.plot(n_features, selector.cv_results_['mean_test_score'], marker='o')\n",
        "plt.axhline(0.93, color='red', linestyle='--', label='93% threshold')\n",
        "plt.axhline(max(selector.cv_results_['mean_test_score']), color='green', linestyle='--', label='Best Acc')\n",
        "plt.xlabel('Number of Features Selected')\n",
        "plt.ylabel('Cross-Validated Accuracy')\n",
        "plt.title('Accuracy vs. Number of Features (RFECV)')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Retrain with Best Features\n",
        "best_features = X.columns[\n",
        "    selector.support_].tolist()\n",
        "print('\\nBest Feature Subset:')\n",
        "print(best_features)\n",
        "\n",
        "rf_final = RandomForestClassifier(\n",
        "    n_estimators=400, random_state=1, class_weight='balanced')\n",
        "rf_final.fit(\n",
        "    XF_train[best_features], yF_train)\n",
        "yF_pred = rf_final.predict(\n",
        "    XF_test[best_features])\n",
        "\n",
        "print('\\nFinal Model Evaluation with Best Features:')\n",
        "print(f'Accuracy: {accuracy_score(yF_test, yF_pred):.4f}')\n",
        "print('Classification Report:')\n",
        "print(classification_report(yF_test, yF_pred))\n",
        "print('Confusion Matrix:')\n",
        "print(confusion_matrix(yF_test, yF_pred))\n",
        "\n",
        "# Plot Feature Importance\n",
        "importances = rf_final.feature_importances_\n",
        "feat_imp = pd.DataFrame({\n",
        "    'Feature': best_features,\n",
        "    'Importance': importances\n",
        "}).sort_values(by='Importance', ascending=False)\n",
        "\n",
        "print('\\nTop Features Driving Model Accuracy:')\n",
        "display(feat_imp)\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.barh(feat_imp['Feature'], feat_imp['Importance'], color='steelblue')\n",
        "plt.gca().invert_yaxis()\n",
        "plt.xlabel('Feature Importance (RF)')\n",
        "plt.title('Key Demographic Predictors of Voting Patterns')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "K-T03hP9CCVo"
      },
      "id": "K-T03hP9CCVo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train FINAL MODEL"
      ],
      "metadata": {
        "id": "oeQkkBXBNWyn"
      },
      "id": "oeQkkBXBNWyn"
    },
    {
      "cell_type": "code",
      "source": [
        "# Select final features: From 'accuracy vs. number of features' plot, ideal number of features start at 8, 9, or 10, compare 8-10 features for accuracy and MANOVA scores (dropped HH_MARRIED due to correlation with REL_xxx_MAR)\n",
        "\n",
        "final_features = [ # Drop '%AGE_FEMALE_YOUNG', '%REL_W_RELATIVES' for best results\n",
        "    'GEOID', 'PARTY_WIN', 'PARTY_LEAD',\n",
        "    '%RACE_White', '%RACE_Asian', '%Urban_pop', '%REL_S_SEX_MAR',\n",
        "    '%REL_OP_SEX_MAR', '%OWN_HOME', '%REL_NON_REL', '%RACE_Black']\n",
        "VOTE_FINAL = VOTE_DF[final_features]\n",
        "\n",
        "# Set features for final model\n",
        "X_final = [col for col in VOTE_FINAL.columns if col not in [\n",
        "    'GEOID', 'PARTY_WIN', 'PARTY_LEAD']]\n",
        "y_final = VOTE_FINAL['PARTY_WIN']\n",
        "\n",
        "# Train/test split\n",
        "XF_train, XF_test, yF_train, yF_test = train_test_split(\n",
        "    VOTE_FINAL[X_final], y_final,\n",
        "    test_size=0.2, random_state=1,\n",
        "    stratify=y_final)\n",
        "\n",
        "# Train Random Forest\n",
        "rf_final = RandomForestClassifier(\n",
        "    n_estimators=500,\n",
        "    random_state=1,\n",
        "    class_weight='balanced')\n",
        "rf_final.fit(XF_train, yF_train)\n",
        "\n",
        "# Evaluate\n",
        "yF_pred = rf_final.predict(XF_test)\n",
        "\n",
        "print('\\nFinal Model Evaluation:')\n",
        "print(f'Accuracy: {accuracy_score(yF_test, yF_pred):.4f}')\n",
        "print('Classification Report:')\n",
        "print(classification_report(yF_test, yF_pred))\n",
        "print('Confusion Matrix:')\n",
        "print(confusion_matrix(yF_test, yF_pred))"
      ],
      "metadata": {
        "id": "isQiPuMBQVry"
      },
      "id": "isQiPuMBQVry",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Use MANOVA to assess whether multiple features jointly differ between Democrats and Republicans"
      ],
      "metadata": {
        "id": "ei1Fc3phWpsy"
      },
      "id": "ei1Fc3phWpsy"
    },
    {
      "cell_type": "code",
      "source": [
        "maov = MANOVA(endog=VOTE_FINAL[X_final], exog=VOTE_FINAL[[y_final.name]])\n",
        "print(maov.mv_test())"
      ],
      "metadata": {
        "id": "ubqRzPOShnav"
      },
      "id": "ubqRzPOShnav",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run CV PI"
      ],
      "metadata": {
        "id": "wVVFLvVyNfIo"
      },
      "id": "wVVFLvVyNfIo"
    },
    {
      "cell_type": "code",
      "source": [
        "# Cross-validate Permutation Importance\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n",
        "importances = []\n",
        "\n",
        "# Pass feature data to skf.split\n",
        "for train_fcv, test_fcv in skf.split(VOTE_FINAL[X_final], y_final):\n",
        "    XF_train, XF_test = VOTE_FINAL[X_final].iloc[train_fcv], VOTE_FINAL[X_final].iloc[test_fcv]\n",
        "    yF_train, yF_test = y_final.iloc[train_fcv], y_final.iloc[test_fcv]\n",
        "\n",
        "    rf_final.fit(XF_train, yF_train)\n",
        "    result = permutation_importance(\n",
        "        rf_final, XF_test, yF_test,\n",
        "        n_repeats=10, random_state=1, n_jobs=-1)\n",
        "    importances.append(result.importances_mean)\n",
        "\n",
        "mean_importances = np.mean(importances, axis=0)\n",
        "std_importances = np.std(importances, axis=0)\n",
        "\n",
        "# Build PI DF\n",
        "pi_df = pd.DataFrame({\n",
        "    'Feature': X_final, # Use X_final for feature names\n",
        "    'Importance Mean': mean_importances.round(4),\n",
        "    'Importance Std': std_importances.round(4)\n",
        "}).sort_values(by='Importance Mean', ascending=False)\n",
        "\n",
        "# Confirm\n",
        "print('\\nCross-validated Permutation Importance:')\n",
        "print(pi_df)\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "pi_df.set_index('Feature')['Importance Mean'].sort_values().plot(kind='barh')\n",
        "plt.title('Final Model - CV Permutation Importance')\n",
        "plt.xlabel('Mean Importance (± CV variation)')\n",
        "plt.ylabel('Feature')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "OzKUEo_CXvz-"
      },
      "id": "OzKUEo_CXvz-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Profile counties with **extremely high PARTY_LEAD** (-30 > LEAD > +30)  \n",
        "\n",
        "Do the demographics of partisan counties match final feature importance?"
      ],
      "metadata": {
        "id": "UW637VgsYaVd"
      },
      "id": "UW637VgsYaVd"
    },
    {
      "cell_type": "code",
      "source": [
        "VOTE_FINAL.to_csv('VOTE_FINAL.csv', index=False)"
      ],
      "metadata": {
        "id": "KcGlrFaJY5nN"
      },
      "id": "KcGlrFaJY5nN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Equal Cutoff Strongholds (±0.5)"
      ],
      "metadata": {
        "id": "OCqURRbheo9y"
      },
      "id": "OCqURRbheo9y"
    },
    {
      "cell_type": "code",
      "source": [
        "def profile_group(dataframe, name, features):\n",
        "    '''Calculates the mean of specified features for a group and returns a Series.'''\n",
        "# Ensure only numeric columns in features are selected for mean calculation\n",
        "    numeric_features = dataframe[features].select_dtypes(include=np.number).columns.tolist()\n",
        "    profile = dataframe[numeric_features].mean()\n",
        "    profile.name = name\n",
        "    return profile\n",
        "\n",
        "# Set cutoff value to 0.50\n",
        "cutoff_val = 0.50\n",
        "\n",
        "# Use _FIN to allow R and D access to all variables\n",
        "extreme_counties = VOTE_FULL[np.abs(VOTE_FULL['PARTY_LEAD']) > cutoff_val]\n",
        "\n",
        "# Republican strongholds\n",
        "extreme_R = extreme_counties[extreme_counties['PARTY_LEAD'] < -cutoff_val]\n",
        "\n",
        "# Democratic strongholds\n",
        "extreme_D = extreme_counties[extreme_counties['PARTY_LEAD'] > cutoff_val]\n",
        "\n",
        "print(f'Republican strongholds (cutoff -{cutoff_val}):', extreme_R.shape[0])\n",
        "print(f'Democratic strongholds (cutoff +{cutoff_val}):', extreme_D.shape[0])\n",
        "\n",
        "# Select demographic features only (drop outcomes and GEOID)\n",
        "demo_features = [col for col in VOTE_FULL.columns if col not in ['PARTY_WIN', 'PARTY_LEAD', 'GEOID']]\n",
        "\n",
        "# Profiles for cutoff-based groups\n",
        "cutoff_profiles_combined = pd.concat([\n",
        "    profile_group(extreme_R, 'R_characteristics', demo_features),\n",
        "    profile_group(extreme_D, 'D_characteristics', demo_features),\n",
        "], axis=1)\n",
        "\n",
        "# Add absolute difference column\n",
        "cutoff_profiles_combined['Abs_Diff'] = np.abs(cutoff_profiles_combined['R_characteristics'] - cutoff_profiles_combined['D_characteristics'])\n",
        "\n",
        "print(f'\\n=== Cutoff-based Stronghold Profiles (>{cutoff_val} Party Lead) ===')\n",
        "print(cutoff_profiles_combined.sort_values(by='Abs_Diff', ascending=False))"
      ],
      "metadata": {
        "id": "DG6o7sPmuJzq"
      },
      "id": "DG6o7sPmuJzq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Balanced Strongholds (Top/Bottom 10% quantiles)"
      ],
      "metadata": {
        "id": "Ez_MMt7k1BmF"
      },
      "id": "Ez_MMt7k1BmF"
    },
    {
      "cell_type": "code",
      "source": [
        "# Set cutoff value\n",
        "lower_10 = VOTE_FULL['PARTY_LEAD'].quantile(0.10)   # bottom 10% cutoff\n",
        "upper_10 = VOTE_FULL['PARTY_LEAD'].quantile(0.90)   # top 10% cutoff\n",
        "\n",
        "R_stnghd_bal = VOTE_FULL[VOTE_FULL['PARTY_LEAD'] <= lower_10].copy()\n",
        "D_stnghd_bal = VOTE_FULL[VOTE_FULL['PARTY_LEAD'] >= upper_10].copy()\n",
        "\n",
        "print(f'Republican strongholds (quantile-based): {len(R_stnghd_bal)} counties (<= {lower_10:.2f})')\n",
        "print(f'Democratic strongholds (quantile-based): {len(D_stnghd_bal)} counties (>= {upper_10:.2f})')\n",
        "\n",
        "# Select demographic features only (drop outcomes and GEOID)\n",
        "demo_features = [col for col in VOTE_FULL.columns if col not in [\n",
        "    'GEOID', 'PARTY_WIN', 'PARTY_LEAD']]\n",
        "\n",
        "# Profiles for quantile-based groups\n",
        "balanced_profiles = pd.concat([\n",
        "    profile_group(R_stnghd_bal, 'R_characteristics', demo_features),\n",
        "    profile_group(D_stnghd_bal, 'D_characteristics', demo_features),\n",
        "], axis=1)\n",
        "\n",
        "# Add absolute difference column\n",
        "balanced_profiles['Abs_Diff'] = np.abs(balanced_profiles['R_characteristics'] - balanced_profiles['D_characteristics'])\n",
        "\n",
        "print('\\n=== Quantile-based Stronghold Profiles (Top/Bottom 10%) ===')\n",
        "print(balanced_profiles.sort_values(by='Abs_Diff', ascending=False))"
      ],
      "metadata": {
        "id": "CYEfEDEYqsmy"
      },
      "id": "CYEfEDEYqsmy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Comparison Table"
      ],
      "metadata": {
        "id": "qPQOzyr65O31"
      },
      "id": "qPQOzyr65O31"
    },
    {
      "cell_type": "code",
      "source": [
        "# Build combined comparison table\n",
        "\n",
        "# Profiles for cutoff-based groups\n",
        "# Select demographic features only (drop outcomes and GEOID)\n",
        "demo_features = [col for col in VOTE_FULL.columns if col not in ['PARTY_WIN', 'PARTY_LEAD', 'GEOID']]\n",
        "\n",
        "cutoff_profiles = pd.concat([\n",
        "    profile_group(extreme_R, 'R_cutoff', demo_features),\n",
        "    profile_group(extreme_D, 'D_cutoff', demo_features)\n",
        "], axis=1)\n",
        "\n",
        "# Profiles for quantile-based groups\n",
        "# Select demographic features only (drop outcomes and GEOID)\n",
        "demo_features = [col for col in VOTE_FULL.columns if col not in ['PARTY_WIN', 'PARTY_LEAD', 'GEOID']]\n",
        "\n",
        "balanced_profiles = pd.concat([\n",
        "    profile_group(R_stnghd_bal, 'R_quantile', demo_features),\n",
        "    profile_group(D_stnghd_bal, 'D_quantile', demo_features)\n",
        "], axis=1)\n",
        "\n",
        "# Combine both into one big table\n",
        "comparison_table = pd.concat([cutoff_profiles, balanced_profiles], axis=1)\n",
        "\n",
        "# Add difference columns (D – R) for clarity of spread\n",
        "comparison_table['Diff_cutoff'] = comparison_table['D_cutoff'] - comparison_table['R_cutoff']\n",
        "comparison_table['Diff_quantile'] = comparison_table['D_quantile'] - comparison_table['R_quantile']\n",
        "\n",
        "comparison_table = comparison_table.round(2)\n",
        "\n",
        "# Confirm\n",
        "print('\\n=== Combined Stronghold Profiles (Cutoff vs Quantile) ===')\n",
        "display(comparison_table.sort_values(by='R_cutoff', ascending=False))"
      ],
      "metadata": {
        "id": "vaz2G7a_5XT_"
      },
      "id": "vaz2G7a_5XT_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Locations of Extremes"
      ],
      "metadata": {
        "id": "HhW9Hq7yevwv"
      },
      "id": "HhW9Hq7yevwv"
    },
    {
      "cell_type": "code",
      "source": [
        "# Pad GEOIDs with a leading zero if length is less than 5 and extract state FIPS\n",
        "R_geoids = [f'{int(geo):05d}' if len(geo) < 5 else geo for geo in extreme_R['GEOID'].unique()]\n",
        "D_geoids = [f'{int(geo):05d}' if len(geo) < 5 else geo for geo in extreme_D['GEOID'].unique()]\n",
        "\n",
        "R_fips = [geo[:2] for geo in R_geoids]\n",
        "D_fips = [geo[:2] for geo in D_geoids]\n",
        "\n",
        "R_counts = Counter(R_fips)\n",
        "D_counts = Counter(D_fips)\n",
        "\n",
        "print('\\nCounts for counties above 50% party lead (Min 75-25% split):')\n",
        "# Sort state_counts by 2-digit state FIPS)\n",
        "sorted_R_counts = dict(sorted(R_counts.items()))\n",
        "sorted_D_counts = dict(sorted(D_counts.items()))\n",
        "print(sorted_R_counts)\n",
        "print(sorted_D_counts)"
      ],
      "metadata": {
        "id": "S282Z_AkaweE"
      },
      "id": "S282Z_AkaweE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Results"
      ],
      "metadata": {
        "id": "kLLzTeuG7I89"
      },
      "id": "kLLzTeuG7I89"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Model Performance & Prediction Quality"
      ],
      "metadata": {
        "id": "grgTS-Nv7eAW"
      },
      "id": "grgTS-Nv7eAW"
    },
    {
      "cell_type": "code",
      "source": [
        "# Predictions: Use the subset of features that the model was trained on\n",
        "yF_pred = rf_final.predict(XF_test)\n",
        "y_pred_proba = rf_final.predict_proba(XF_test)[:, 1] if hasattr(\n",
        "               rf_final, 'predict_proba') else None\n",
        "\n",
        "# Collect metrics\n",
        "performance = {\n",
        "    'Accuracy': accuracy_score(yF_test, yF_pred),\n",
        "    'Precision': precision_score(yF_test, yF_pred, zero_division=0),\n",
        "    'Recall': recall_score(yF_test, yF_pred, zero_division=0),\n",
        "    'F1 Score': f1_score(yF_test, yF_pred, zero_division=0)}\n",
        "\n",
        "if y_pred_proba is not None:\n",
        "    performance['ROC-AUC'] = roc_auc_score(yF_test, y_pred_proba)\n",
        "\n",
        "# Create performance DataFrame\n",
        "perf_df = pd.DataFrame(performance, index=['Final Model']).T\n",
        "display(perf_df)"
      ],
      "metadata": {
        "id": "tqn0PurM7LXr"
      },
      "id": "tqn0PurM7LXr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Feature Importance"
      ],
      "metadata": {
        "id": "k7DxriQ87ahq"
      },
      "id": "k7DxriQ87ahq"
    },
    {
      "cell_type": "code",
      "source": [
        "print('\\nCross-validated Permutation Importance:')\n",
        "print(pi_df)\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "pi_df.set_index('Feature')['Importance Mean'].sort_values().plot(kind='barh')\n",
        "plt.title('Final Model - CV Permutation Importance')\n",
        "plt.xlabel('Mean Importance (± CV variation)')\n",
        "plt.ylabel('Feature')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1ldgozQLupRO"
      },
      "id": "1ldgozQLupRO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "END"
      ],
      "metadata": {
        "id": "Ec1EelkQ_mYs"
      },
      "id": "Ec1EelkQ_mYs"
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Read Minimum Wage Data  \n",
        "Datafile manually compiled from  \n",
        "state law https://www.dol.gov/agencies/whd/state/minimum-wage/history  \n",
        "and local ordinances https://laborcenter.berkeley.edu/inventory-of-us-city-and-county-minimum-wage-ordinances/  \n",
        "Many exceptions exist for number of employees, annual receipts, type of work, tip workers, etc. Highest wage selected for study."
      ],
      "metadata": {
        "id": "_c46d1vowQlv"
      },
      "id": "_c46d1vowQlv"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3456af60"
      },
      "source": [],
      "id": "3456af60",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "cell_execution_strategy": "setup",
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}